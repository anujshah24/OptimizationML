{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of features of the input.\n",
    "            output_size (int): Dimension of output.\n",
    "            hidden_layer_size (int): Number of neurons in the input layer.\n",
    "            activations (list): List of strings giving the activations for each layer.\n",
    "            loss (str): Loss function for the model.\n",
    "            optimizer (str): Optimization algorithm for the model.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X, w1=None, wn=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (tensor): Input for the model. \n",
    "            w1 (tensor): Weights to be used for the first layer. (Optional Argument)\n",
    "            wn (tensor): Weights to be used for the final layer. (Optional Argument)\n",
    "            \n",
    "        Returns:\n",
    "            z (list): List of outputs from linear function at each layer.\n",
    "            a (list): List of activation outputs from each layer.\n",
    "        \"\"\"\n",
    "        if w1 is None:\n",
    "            w1 = self.w1\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a, wn=None):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Input Data\n",
    "            y (Tensor): Output Data\n",
    "            z (list): List of outputs from linear layers.\n",
    "            a (list): List of actiation outputs.\n",
    "            wn (Tensor): Weights from final layer. (Optional Argument)\n",
    "        \"\"\"\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False, decay_rate=0.999, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        ypred = None\n",
    "        if self.optimizer == 'SGD':\n",
    "            if momentum_param != 0:\n",
    "                if nesterov:\n",
    "                    funVals ,ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param, True)\n",
    "                else:\n",
    "                    funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param)\n",
    "            else:\n",
    "                funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'Adagrad':\n",
    "            funVals, ypred = self.Adagrad(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            funVals, ypred = self.RMSProp(X, y, batch_size, iterations, alpha, decay_rate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            funVals, ypred = self.Adam(X, y, batch_size, iterations, alpha, beta1, beta2)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def SGD(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Gradient Descent Algorithm\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        vn = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                if nesterov:\n",
    "                    z, a = self.forward(X[i:i+batch_size], self.w1+momentum_param*v1, self.wn+momentum_param*vn)\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a, self.wn+momentum_param*vn)\n",
    "                else:\n",
    "                    z, a = self.forward(X[i:i+batch_size])\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                v1 = momentum_param * v1 - alpha * dW[0]\n",
    "                vn = momentum_param * vn - alpha * dW[1]\n",
    "                self.w1 = self.w1 + v1\n",
    "                self.wn = self.wn + vn\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adagrad(self, X, y, batch_size=100, iterations=500, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        AdaGrad Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 += dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1)+smoothing_param)) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2)+smoothing_param)) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def RMSProp(self, X, y, batch_size=100, iterations=500, alpha=1e-04, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        RMSProp Optimizer.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 = decay_rate*cache1 + (1 - decay_rate) * dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1+smoothing_param))) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2+smoothing_param))) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adam(self, X, y, batch_size=100, iterations=500, alpha=1e-04, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        m1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        m2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        v2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                m1 = beta1 * m1 + (1-beta1) * dW[0]\n",
    "                v1 = beta2 * v1 + (1-beta2) * dW[0]**2\n",
    "                m2 = beta1 * m2 + (1-beta1) * dW[1]\n",
    "                v2 = beta2 * v2 + (1-beta2) * dW[1]**2\n",
    "                self.w1 += -alpha*(m1/(1-beta1**n_iter))/(torch.sqrt((v1)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                self.wn += -alpha*(m2/(1-beta1**n_iter))/(torch.sqrt((v2)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "        \"\"\"\n",
    "        _, a = self.forward(X)\n",
    "        return a[-1]\n",
    "    \n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset Weights\n",
    "        \"\"\"\n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            def sigmoid(z):\n",
    "                s = torch.exp(-z)\n",
    "                return 1/(1+s)\n",
    "            return sigmoid\n",
    "#             return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        elif activation == 'tanh':\n",
    "            return lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : 1/(1 + torch.exp(-z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>=0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            tanh = lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "            return lambda z : 1 - tanh(z)**2\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((ypred - y).T, (ypred - y))/(2*len(y))\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLoss(ypred, y):\n",
    "                loss = torch.sum(torch.log(ypred[y==1])) + torch.sum(torch.log(1 - ypred[y==0]))\n",
    "                return -loss/y.shape[0]\n",
    "            return binaryCrossEntropyLoss\n",
    "#             return lambda ypred, y : (-1/len(y))*(torch.matmul(y.T, torch.log(ypred)) + torch.matmul((1-y).T, torch.log(1-ypred)))\n",
    "            return binaryCrossEntropyLoss\n",
    "        elif self.loss == \"CELoss\":\n",
    "            def crossEntropyLoss(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                prob = self.softmax(ypred)\n",
    "                log_likelihood = -torch.log(prob[range(m), y.long()])\n",
    "                loss = torch.sum(log_likelihood)\n",
    "                return loss/m\n",
    "            return crossEntropyLoss\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y: (ypred - y)/len(y)\n",
    "        elif self.loss == 'BCELoss':\n",
    "            return lambda ypred, y: (-1/len(y)) * ((y/ypred) - ((1-y)/(1-ypred)))\n",
    "        elif self.loss == 'CELoss':\n",
    "            def crossEntropyLossGradient(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                grad = self.softmax(ypred)\n",
    "                grad[range(m), y.long()] -= 1\n",
    "                return grad/m\n",
    "            return crossEntropyLossGradient\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exps = torch.exp(z - (torch.max(z, dim=1).values.reshape(-1,1)))\n",
    "        return exps/torch.sum(exps, dim=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(312, 20, dtype=torch.double)\n",
    "# y = torch.randint(0, 2,(312, 1)).double()\n",
    "# y = torch.randn(312, 1, dtype=torch.double)\n",
    "# y = torch.randint(0,3,(312, 1)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_temp = NeuralNetwork(X.shape[1], 1, 32, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funVals, ypred = model_temp.train(X, y, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((torch.sum(ypred.argmax(dim=1).reshape(-1,1) == y.long()).float()*100.0)/(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# def plotLoss(funVals, filePath, title):\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "# plt.xlabel(\"Number of Iterations\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plotLoss(funVals, filePath=None, title=\"\", plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    if not plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"./dataset/\"+filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAutoMPGDataset():\n",
    "    import pandas as pd\n",
    "    auto_mpg_dataset = pd.read_csv(\"./dataset/auto-mpg/auto-mpg.data\", header=-1, comment='\\t', skipinitialspace=True, na_values='?', sep=' ')\n",
    "    auto_mpg_dataset = auto_mpg_dataset.dropna()\n",
    "    origin = auto_mpg_dataset.pop(7)\n",
    "    auto_mpg_dataset[7] = (origin==1)*1.0\n",
    "    auto_mpg_dataset[8] = (origin==2)*1.0\n",
    "    auto_mpg_dataset[9] = (origin==3)*1.0\n",
    "    auto_dataset = torch.tensor(auto_mpg_dataset.values, dtype=torch.double)\n",
    "    return auto_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dataset = loadAutoMPGDataset()\n",
    "auto_dataset = auto_dataset[torch.randperm(auto_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = auto_dataset[:int(0.8 * auto_dataset.shape[0])]\n",
    "test = auto_dataset[int(0.8 * auto_dataset.shape[0]):]\n",
    "\n",
    "Xtrain = train[:, 1:]\n",
    "Xtrain = (Xtrain - Xtrain.mean(dim=0))/Xtrain.std(dim=0)\n",
    "ytrain = train[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest = test[:, 1:]\n",
    "Xtest = (Xtest - Xtest.mean(dim=0))/Xtest.std(dim=0)\n",
    "ytest = test[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model = NeuralNetwork(Xtrain.shape[1], ytrain.shape[1], 64, ['relu', 'relu'], 'MSE', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_mpg_model.reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funVals, ypred = auto_mpg_model.train(Xtrain, ytrain, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIrisDataset():\n",
    "    iris_dataset = pd.read_csv(\"./dataset/iris/iris.data\", header=-1)\n",
    "    iris_dataset[4] = iris_dataset[4].astype('category').cat.codes\n",
    "    iris_dataset = torch.tensor(iris_dataset.values, dtype=torch.double)\n",
    "    return iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = loadIrisDataset()\n",
    "iris_dataset = iris_dataset[torch.randperm(iris_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris = iris_dataset[:int(0.8 * iris_dataset.shape[0])]\n",
    "test_iris = iris_dataset[int(0.8 * iris_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_iris = train_iris[:, :-1]\n",
    "Xtrain_iris = (Xtrain_iris - Xtrain_iris.mean(dim=0))/Xtrain_iris.std(dim=0)\n",
    "ytrain_iris = train_iris[:, -1].reshape(-1, 1)\n",
    "\n",
    "Xtest_iris = test_iris[:, :-1]\n",
    "Xtest_iris = (Xtest_iris - Xtest_iris.mean(dim=0))/Xtest_iris.std(dim=0)\n",
    "ytest_iris = test_iris[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = NeuralNetwork(Xtrain_iris.shape[1], 3, 64, ['relu', 'linear'], 'CELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funVals, ypred = iris_model.train(Xtrain_iris, ytrain_iris, batch_size=100, iterations=500, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.argmax(ypred, dim=1).reshape(-1,1) == ytrain_iris.long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(Xtrain_iris.shape[1], 32), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval = []\n",
    "for epoch in range(1000):\n",
    "    for i in range(Xtrain_iris.shape[0]//100):\n",
    "        optimizer.zero_grad()\n",
    "        yp = model(Xtrain_iris[i:i+100].float())\n",
    "        loss = lossFn(yp, ytrain_iris[i:i+100].long()[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fval.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(fval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBreastCancerDataset():\n",
    "    cancer_dataset = pd.read_csv(\"./dataset/breast-cancer/data.csv\")\n",
    "    cancer_dataset.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "    cancer_dataset['diagnosis'] = cancer_dataset['diagnosis'].astype('category').cat.codes\n",
    "    cancer_dataset = torch.tensor(cancer_dataset.values, dtype=torch.double)\n",
    "    return cancer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = loadBreastCancerDataset()\n",
    "cancer_dataset = cancer_dataset[torch.randperm(cancer_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cancer = cancer_dataset[:int(0.8*cancer_dataset.shape[0])]\n",
    "test_cancer = cancer_dataset[int(0.8*cancer_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_cancer = train_cancer[:, 1:]\n",
    "Xtrain_cancer = (Xtrain_cancer-Xtrain_cancer.mean(dim=0))/Xtrain_cancer.std(dim=0)\n",
    "ytrain_cancer = train_cancer[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_cancer = test_cancer[:, 1:]\n",
    "Xtest_cancer = (Xtest_cancer-Xtest_cancer.mean(dim=0))/Xtest_cancer.std(dim=0)\n",
    "ytest_cancer = test_cancer[:, 0].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_model = NeuralNetwork(Xtrain_cancer.shape[1], 1, 16, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.1721434012905105\n",
      "8 1.1178754404790323\n",
      "12 1.0662013436637188\n",
      "16 1.0168211871912642\n",
      "20 0.9696337439568992\n",
      "24 0.9245751857149923\n",
      "28 0.8814975635373248\n",
      "32 0.8403259386395664\n",
      "36 0.8010763855887729\n",
      "40 0.7637546927970412\n",
      "44 0.7284250540449979\n",
      "48 0.6950577953538213\n",
      "52 0.6635239382882988\n",
      "56 0.6335779355162863\n",
      "60 0.6050241003985053\n",
      "64 0.5775496081566854\n",
      "68 0.5512745342179106\n",
      "72 0.526096018222431\n",
      "76 0.5021602283175242\n",
      "80 0.47965223719668076\n",
      "84 0.458584680274452\n",
      "88 0.43905153909234\n",
      "92 0.4209687280227108\n",
      "96 0.40431992297347386\n",
      "100 0.3890098470422621\n",
      "100 0.3890098470422621\n",
      "104 0.37503220164519463\n",
      "108 0.36222580726188325\n",
      "112 0.3504515762107168\n",
      "116 0.3394795814711981\n",
      "120 0.32947307930069014\n",
      "124 0.3203146872834864\n",
      "128 0.3119139892918546\n",
      "132 0.3041941815493446\n",
      "136 0.29711313706637626\n",
      "140 0.29056634857273267\n",
      "144 0.2845123987230874\n",
      "148 0.27889132741932793\n",
      "152 0.2736878280507816\n",
      "156 0.2688256398394953\n",
      "160 0.26426133365519106\n",
      "164 0.2599923696880881\n",
      "168 0.2560352270559661\n",
      "172 0.2523241849041017\n",
      "176 0.24884426254528513\n",
      "180 0.24551441672451713\n",
      "184 0.24238689182835438\n",
      "188 0.23943860945032933\n",
      "192 0.23666333892674674\n",
      "196 0.23404441890526687\n",
      "200 0.23158930444805756\n",
      "200 0.23158930444805756\n",
      "204 0.22925632728150447\n",
      "208 0.22704359632593704\n",
      "212 0.22492001921174565\n",
      "216 0.22293119022192104\n",
      "220 0.22105450875169294\n",
      "224 0.21930670613936842\n",
      "228 0.2176677085590474\n",
      "232 0.2161557682532534\n",
      "236 0.21470388055414213\n",
      "240 0.21331396391358046\n",
      "244 0.21202784480411946\n",
      "248 0.21083603872751094\n",
      "252 0.20974620450862783\n",
      "256 0.20872642366530872\n",
      "260 0.20777288992672474\n",
      "264 0.2069003301579295\n",
      "268 0.2060867985481416\n",
      "272 0.20535488078979133\n",
      "276 0.2046679928526036\n",
      "280 0.20407748261915212\n",
      "284 0.20354776207004915\n",
      "288 0.20305252163196227\n",
      "292 0.20261591094908254\n",
      "296 0.20220960823771453\n",
      "300 0.20184709530260797\n",
      "300 0.20184709530260797\n",
      "304 0.20151913739745703\n",
      "308 0.20121115776003604\n",
      "312 0.2009392270966267\n",
      "316 0.20066881076226428\n",
      "320 0.20039897306039783\n",
      "324 0.2001351719222271\n",
      "328 0.1999076811508312\n",
      "332 0.19967942897177351\n",
      "336 0.19948017321241154\n",
      "340 0.19929437999547264\n",
      "344 0.19909899492674907\n",
      "348 0.19892979862620075\n",
      "352 0.19877680995210997\n",
      "356 0.19861710606187147\n",
      "360 0.19847608648051077\n",
      "364 0.1983462836364035\n",
      "368 0.198222083785696\n",
      "372 0.19811918431250944\n",
      "376 0.1980130573055246\n",
      "380 0.19792711000226065\n",
      "384 0.19783970384414717\n",
      "388 0.1977695416800211\n",
      "392 0.19770638086289344\n",
      "396 0.19764616914671296\n",
      "400 0.197596683197313\n",
      "400 0.197596683197313\n",
      "404 0.19755972600564042\n",
      "408 0.19752400327204708\n",
      "412 0.19750090775390827\n",
      "416 0.19748173593510995\n",
      "420 0.19746444982158118\n",
      "424 0.19745218902912146\n",
      "428 0.19744928539119677\n",
      "432 0.19744485259506828\n",
      "436 0.19744896722974564\n",
      "440 0.19745555937176082\n",
      "444 0.1974610165134736\n",
      "448 0.19747845695077862\n",
      "452 0.19749190807230466\n",
      "456 0.19751650738596926\n",
      "460 0.19753843807308388\n",
      "464 0.1975639973618397\n",
      "468 0.19759566622931116\n",
      "472 0.19762416240273414\n",
      "476 0.19766191677176956\n",
      "480 0.19770005437329136\n",
      "484 0.19774429506361438\n",
      "488 0.19778944036122775\n",
      "492 0.19781535167783823\n",
      "496 0.19781473291115925\n",
      "500 0.1977939370324486\n",
      "500 0.1977939370324486\n",
      "504 0.1977659949278951\n",
      "508 0.19773296623756761\n",
      "512 0.19773446286220653\n",
      "516 0.19775502635606254\n",
      "520 0.19776272075971238\n",
      "524 0.1977491438810863\n",
      "528 0.1977345244823218\n",
      "532 0.19774827476255616\n",
      "536 0.19776033362493037\n",
      "540 0.197753042861107\n",
      "544 0.19776690785304163\n",
      "548 0.1977770919508182\n",
      "552 0.19778688190933982\n",
      "556 0.19780802774954753\n",
      "560 0.19782297854724198\n",
      "564 0.19784580062279616\n",
      "568 0.19786388910852282\n",
      "572 0.197890881484504\n",
      "576 0.19791463722334815\n",
      "580 0.1979472871552501\n",
      "584 0.197973579999445\n",
      "588 0.19800562450300646\n",
      "592 0.198039995661802\n",
      "596 0.19807182143970525\n",
      "600 0.19811036751370645\n",
      "600 0.19811036751370645\n",
      "604 0.19814696299933565\n",
      "608 0.19818638350528156\n",
      "612 0.19822527935179368\n",
      "616 0.19826752656385155\n",
      "620 0.1983057915117032\n",
      "624 0.19835161267725127\n",
      "628 0.19839706740791285\n",
      "632 0.19844048377889678\n",
      "636 0.19849351258133419\n",
      "640 0.19853576431449074\n",
      "644 0.19859132635418247\n",
      "648 0.19864033857037897\n",
      "652 0.19869630876276861\n",
      "656 0.1987459331378926\n",
      "660 0.19880541643092478\n",
      "664 0.1988597905494258\n",
      "668 0.1989203427363418\n",
      "672 0.198981235366788\n",
      "676 0.19904487662893722\n",
      "680 0.19910511609331996\n",
      "684 0.1991698425156665\n",
      "688 0.19923115209340117\n",
      "692 0.19928556617151597\n",
      "696 0.1993521304647062\n",
      "700 0.1994102939418872\n",
      "700 0.1994102939418872\n",
      "704 0.19947212981140397\n",
      "708 0.19953616964536008\n",
      "712 0.19959555631186301\n",
      "716 0.19966294376008456\n",
      "720 0.19972694018159332\n",
      "724 0.19979439163303142\n",
      "728 0.19986344512682389\n",
      "732 0.19992807478992802\n",
      "736 0.20000363788050457\n",
      "740 0.20006965913850527\n",
      "744 0.2001422603134722\n",
      "748 0.2002168956766227\n",
      "752 0.20028598235178025\n",
      "756 0.2003603432939038\n",
      "760 0.20043155849291588\n",
      "764 0.2005050466287568\n",
      "768 0.2005774664738211\n",
      "772 0.20065441361272515\n",
      "776 0.20073241174983958\n",
      "780 0.2008167623125312\n",
      "784 0.20090631955364605\n",
      "788 0.20098832967566366\n",
      "792 0.2010736700523472\n",
      "796 0.2011587654066233\n",
      "800 0.20124264067607026\n",
      "800 0.20124264067607026\n",
      "804 0.20133265607582296\n",
      "808 0.20142043600121506\n",
      "812 0.20150436542117478\n",
      "816 0.20159088962153152\n",
      "820 0.20167872958311\n",
      "824 0.20176830572277887\n",
      "828 0.2018566517251546\n",
      "832 0.20194645987807025\n",
      "836 0.20203310823423082\n",
      "840 0.20212144087080058\n",
      "844 0.20221244376973466\n",
      "848 0.20230676971725214\n",
      "852 0.2023953193820587\n",
      "856 0.20248464247284567\n",
      "860 0.20257535182395034\n",
      "864 0.20266466764297347\n",
      "868 0.20275736310625284\n",
      "872 0.20284990615048212\n",
      "876 0.20294418732041727\n",
      "880 0.20303242590189066\n",
      "884 0.20312357720984603\n",
      "888 0.20321627696335362\n",
      "892 0.20330545075723908\n",
      "896 0.20340570878261277\n",
      "900 0.20349641727150644\n",
      "900 0.20349641727150644\n",
      "904 0.20359211242092556\n",
      "908 0.20368406473276993\n",
      "912 0.20377544938877787\n",
      "916 0.20386918303248444\n",
      "920 0.20396196454578527\n",
      "924 0.20405668958614762\n",
      "928 0.20414923851370187\n",
      "932 0.2042410054839767\n",
      "936 0.20433870857639674\n",
      "940 0.20443270235972685\n",
      "944 0.20452605097625456\n",
      "948 0.20461654193019102\n",
      "952 0.20471859586997238\n",
      "956 0.20480853341446592\n",
      "960 0.20490131174306356\n",
      "964 0.2049977246984562\n",
      "968 0.20509269274026037\n",
      "972 0.205185403977954\n",
      "976 0.20528145995011415\n",
      "980 0.20537496817316298\n",
      "984 0.20547239219519606\n",
      "988 0.2055694693239479\n",
      "992 0.20566199297075952\n",
      "996 0.20576023917804895\n",
      "1000 0.20585565060207467\n",
      "1000 0.20585565060207467\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = cancer_model.train(Xtrain_cancer, ytrain_cancer, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals, filePath=\"breast-cancer/results/Adam\", title=\"Adam\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_model.w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
