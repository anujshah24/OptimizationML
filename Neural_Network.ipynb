{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of features of the input.\n",
    "            output_size (int): Dimension of output.\n",
    "            hidden_layer_size (int): Number of neurons in the input layer.\n",
    "            activations (list): List of strings giving the activations for each layer.\n",
    "            loss (str): Loss function for the model.\n",
    "            optimizer (str): Optimization algorithm for the model.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X, w1=None, wn=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (tensor): Input for the model. \n",
    "            w1 (tensor): Weights to be used for the first layer. (Optional Argument)\n",
    "            wn (tensor): Weights to be used for the final layer. (Optional Argument)\n",
    "            \n",
    "        Returns:\n",
    "            z (list): List of outputs from linear function at each layer.\n",
    "            a (list): List of activation outputs from each layer.\n",
    "        \"\"\"\n",
    "        if w1 is None:\n",
    "            w1 = self.w1\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a, wn=None):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Input Data\n",
    "            y (Tensor): Output Data\n",
    "            z (list): List of outputs from linear layers.\n",
    "            a (list): List of actiation outputs.\n",
    "            wn (Tensor): Weights from final layer. (Optional Argument)\n",
    "        \"\"\"\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False, decay_rate=0.999, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        ypred = None\n",
    "        if self.optimizer == 'SGD':\n",
    "            if momentum_param != 0:\n",
    "                if nesterov:\n",
    "                    funVals ,ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param, True)\n",
    "                else:\n",
    "                    funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param)\n",
    "            else:\n",
    "                funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'Adagrad':\n",
    "            funVals, ypred = self.Adagrad(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            funVals, ypred = self.RMSProp(X, y, batch_size, iterations, alpha, decay_rate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            funVals, ypred = self.Adam(X, y, batch_size, iterations, alpha, beta1, beta2)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def SGD(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Gradient Descent Algorithm\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        vn = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                if nesterov:\n",
    "                    z, a = self.forward(X[i:i+batch_size], self.w1+momentum_param*v1, self.wn+momentum_param*vn)\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a, self.wn+momentum_param*vn)\n",
    "                else:\n",
    "                    z, a = self.forward(X[i:i+batch_size])\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                v1 = momentum_param * v1 - alpha * dW[0]\n",
    "                vn = momentum_param * vn - alpha * dW[1]\n",
    "                self.w1 = self.w1 + v1\n",
    "                self.wn = self.wn + vn\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adagrad(self, X, y, batch_size=100, iterations=500, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        AdaGrad Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 += dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1)+smoothing_param)) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2)+smoothing_param)) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def RMSProp(self, X, y, batch_size=100, iterations=500, alpha=1e-04, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        RMSProp Optimizer.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 = decay_rate*cache1 + (1 - decay_rate) * dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1+smoothing_param))) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2+smoothing_param))) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adam(self, X, y, batch_size=100, iterations=500, alpha=1e-04, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        m1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        m2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        v2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                m1 = beta1 * m1 + (1-beta1) * dW[0]\n",
    "                v1 = beta2 * v1 + (1-beta2) * dW[0]**2\n",
    "                m2 = beta1 * m2 + (1-beta1) * dW[1]\n",
    "                v2 = beta2 * v2 + (1-beta2) * dW[1]**2\n",
    "                self.w1 += -alpha*(m1/(1-beta1**n_iter))/(torch.sqrt((v1)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                self.wn += -alpha*(m2/(1-beta1**n_iter))/(torch.sqrt((v2)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "        \"\"\"\n",
    "        _, a = self.forward(X)\n",
    "        return a[-1]\n",
    "    \n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset Weights\n",
    "        \"\"\"\n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            def sigmoid(z):\n",
    "                s = torch.exp(z)\n",
    "                return s/(1+s)\n",
    "            return sigmoid\n",
    "#             return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        elif activation == 'tanh':\n",
    "            return lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>=0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            tanh = lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "            return lambda z : 1 - tanh(z)**2\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((ypred - y).T, (ypred - y))/(2*len(y))\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLoss(ypred, y):\n",
    "                ypredy1 = ypred[y==1]\n",
    "                ypredy1[ypredy1==0] = 1\n",
    "                loss = torch.sum(torch.log(ypred[y==1]+1e-05)) + torch.sum(torch.log(1 - ypred[y==0]+1e-05))\n",
    "                return -loss/y.shape[0]\n",
    "            return binaryCrossEntropyLoss\n",
    "        elif self.loss == \"CELoss\":\n",
    "            def crossEntropyLoss(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                prob = self.softmax(ypred)\n",
    "                log_likelihood = -torch.log(prob[range(m), y.long()[:, 0]])\n",
    "                loss = torch.sum(log_likelihood)\n",
    "                return loss/m\n",
    "            return crossEntropyLoss\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y: (ypred - y)/len(y)\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLossGradient(ypred, y):\n",
    "                gradient = 1/(ypred+1e-05) - 1/(1-ypred+1e-05)\n",
    "                return -gradient/y.shape[0]\n",
    "            return binaryCrossEntropyLossGradient\n",
    "        elif self.loss == 'CELoss':\n",
    "            def crossEntropyLossGradient(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                grad = self.softmax(ypred)\n",
    "                grad[range(m), y.long()[:, 0]] -= 1\n",
    "                return grad/m\n",
    "            return crossEntropyLossGradient\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        '''\n",
    "        SoftMax Activation Function\n",
    "        '''\n",
    "        exps = torch.exp(z - (torch.max(z, dim=1).values.reshape(-1,1)))\n",
    "        return exps/torch.sum(exps, dim=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(312, 20, dtype=torch.double)\n",
    "# # y = torch.randint(0, 2,(312, 1)).double()\n",
    "# # y = torch.randn(312, 1, dtype=torch.double)\n",
    "# y = torch.randint(0,3,(312, 1)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_temp = NeuralNetwork(X.shape[1], 3, 32, ['relu', 'linear'], 'CELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funVals, ypred = model_temp.train(X, y, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((torch.sum(ypred.argmax(dim=1).reshape(-1,1) == y.long()).float()*100.0)/(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # def plotLoss(funVals, filePath, title):\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "# plt.xlabel(\"Number of Iterations\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plotLoss(funVals, filePath=None, title=\"\", plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    if not plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"./dataset/\"+filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAutoMPGDataset():\n",
    "    import pandas as pd\n",
    "    auto_mpg_dataset = pd.read_csv(\"./dataset/auto-mpg/auto-mpg.data\", header=-1, comment='\\t', skipinitialspace=True, na_values='?', sep=' ')\n",
    "    auto_mpg_dataset = auto_mpg_dataset.dropna()\n",
    "    origin = auto_mpg_dataset.pop(7)\n",
    "    auto_mpg_dataset[7] = (origin==1)*1.0\n",
    "    auto_mpg_dataset[8] = (origin==2)*1.0\n",
    "    auto_mpg_dataset[9] = (origin==3)*1.0\n",
    "    auto_dataset = torch.tensor(auto_mpg_dataset.values, dtype=torch.double)\n",
    "    return auto_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dataset = loadAutoMPGDataset()\n",
    "auto_dataset = auto_dataset[torch.randperm(auto_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto = auto_dataset[:int(0.8 * auto_dataset.shape[0])]\n",
    "test_auto = auto_dataset[int(0.8 * auto_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_auto = train_auto[:, 1:]\n",
    "Xtrain_auto = (Xtrain_auto - Xtrain_auto.mean(dim=0))/Xtrain_auto.std(dim=0)\n",
    "ytrain_auto = train_auto[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_auto = test_auto[:, 1:]\n",
    "Xtest_auto = (Xtest_auto - Xtest_auto.mean(dim=0))/Xtest_auto.std(dim=0)\n",
    "ytest_auto = test_auto[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model = NeuralNetwork(Xtrain_auto.shape[1], ytrain_auto.shape[1], 32, ['relu', 'relu'], 'MSE', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 295.32306169439914\n",
      "6 295.0904167791533\n",
      "9 294.86437581747725\n",
      "12 294.6364043915564\n",
      "15 294.39620247888627\n",
      "18 294.1225660263929\n",
      "21 293.82579711565876\n",
      "24 293.51002609179614\n",
      "27 293.12260846059905\n",
      "30 292.72369213875976\n",
      "33 292.28479396952395\n",
      "36 291.79380653711604\n",
      "39 291.23346322541596\n",
      "42 290.6221734016954\n",
      "45 289.89355848658215\n",
      "48 289.10394811403853\n",
      "51 288.2816290634707\n",
      "54 287.4154130582775\n",
      "57 286.43510567286734\n",
      "60 285.36679585472575\n",
      "63 284.202038785603\n",
      "66 282.8922617537978\n",
      "69 281.398003350431\n",
      "72 279.71378874770534\n",
      "75 277.74251706200494\n",
      "78 275.33782277821336\n",
      "81 272.73604126059075\n",
      "84 270.01616717554106\n",
      "87 266.94414786894424\n",
      "90 263.7076923775541\n",
      "93 260.32556171840025\n",
      "96 256.9818231116216\n",
      "99 253.65479077018028\n",
      "100 252.50646233521368\n",
      "102 250.2220066032867\n",
      "105 246.6920458511998\n",
      "108 243.21490248411024\n",
      "111 239.59202509106368\n",
      "114 235.87618712629634\n",
      "117 232.02857788362672\n",
      "120 228.04478235630933\n",
      "123 223.9549524253475\n",
      "126 219.8874585594768\n",
      "129 215.85548378995284\n",
      "132 211.69697548178002\n",
      "135 207.40116149980088\n",
      "138 203.16968685438445\n",
      "141 199.15432951513017\n",
      "144 195.23917869278094\n",
      "147 191.4613771505743\n",
      "150 187.91394398691702\n",
      "153 184.51280604596516\n",
      "156 181.31186334780065\n",
      "159 178.24557895245044\n",
      "162 175.32072978205127\n",
      "165 172.54297780256047\n",
      "168 169.9052032279358\n",
      "171 167.37346208004462\n",
      "174 164.9503542360442\n",
      "177 162.6444615294992\n",
      "180 160.4595125824158\n",
      "183 158.37462041177787\n",
      "186 156.38397760573756\n",
      "189 154.45393344559278\n",
      "192 152.60107418905275\n",
      "195 150.81585840228286\n",
      "198 149.1053902250241\n",
      "200 147.99082651007745\n",
      "201 147.4379560968161\n",
      "204 145.80198436158182\n",
      "207 144.2089734901299\n",
      "210 142.67034637935504\n",
      "213 141.15789580277337\n",
      "216 139.67853772972745\n",
      "219 138.25840244916924\n",
      "222 136.8871983958055\n",
      "225 135.57207737839698\n",
      "228 134.28804933501064\n",
      "231 133.05131031180855\n",
      "234 131.83711366621927\n",
      "237 130.64003581222232\n",
      "240 129.47538336135045\n",
      "243 128.31216562782475\n",
      "246 127.1053257830758\n",
      "249 125.90223236656519\n",
      "252 124.7106155868835\n",
      "255 123.51507099010665\n",
      "258 122.26945734394873\n",
      "261 121.01189792473122\n",
      "264 119.70999311721503\n",
      "267 118.37558584825369\n",
      "270 117.04618894767749\n",
      "273 115.70032858141451\n",
      "276 114.35172643811617\n",
      "279 113.00411867567749\n",
      "282 111.65312917504255\n",
      "285 110.28900804948829\n",
      "288 108.92785298713207\n",
      "291 107.57779487560622\n",
      "294 106.21225367634929\n",
      "297 104.86619299057945\n",
      "300 103.4710674754603\n",
      "300 103.4710674754603\n",
      "303 102.06757451059677\n",
      "306 100.67934411115729\n",
      "309 99.30172955764499\n",
      "312 97.93360394628301\n",
      "315 96.56195548252259\n",
      "318 95.16072791892095\n",
      "321 93.72490660145154\n",
      "324 92.27459911466018\n",
      "327 90.81593782716052\n",
      "330 89.34685466587182\n",
      "333 87.87771721366894\n",
      "336 86.42742026984328\n",
      "339 84.9992628536689\n",
      "342 83.56503909290682\n",
      "345 82.13763544709053\n",
      "348 80.71482315464256\n",
      "351 79.29690652948534\n",
      "354 77.8880135873971\n",
      "357 76.51119802350082\n",
      "360 75.16654156970786\n",
      "363 73.8531825339881\n",
      "366 72.56538716897724\n",
      "369 71.27745543848295\n",
      "372 70.01641961340326\n",
      "375 68.78388426330818\n",
      "378 67.5660961997613\n",
      "381 66.37473229635059\n",
      "384 65.20737354871112\n",
      "387 64.064371268523\n",
      "390 62.94733967757081\n",
      "393 61.855098639966506\n",
      "396 60.78598980215883\n",
      "399 59.73933373017964\n",
      "400 59.39523469804194\n",
      "402 58.71434212745565\n",
      "405 57.71089245268287\n",
      "408 56.72436492873517\n",
      "411 55.75521020303538\n",
      "414 54.80155429826569\n",
      "417 53.862601672237105\n",
      "420 52.938186193181345\n",
      "423 52.027227744679706\n",
      "426 51.13207659385187\n",
      "429 50.25371787770595\n",
      "432 49.39204842256699\n",
      "435 48.545986289466654\n",
      "438 47.71262371083358\n",
      "441 46.893977406832605\n",
      "444 46.09052604797486\n",
      "447 45.30261335513253\n",
      "450 44.529085361351065\n",
      "453 43.77049409211754\n",
      "456 43.02712540731177\n",
      "459 42.29844235241068\n",
      "462 41.58462523837291\n",
      "465 40.88430706556639\n",
      "468 40.19672457667457\n",
      "471 39.52233189802538\n",
      "474 38.86007767870562\n",
      "477 38.20907824224763\n",
      "480 37.56978571060431\n",
      "483 36.94210872916672\n",
      "486 36.32612202348225\n",
      "489 35.72152417015415\n",
      "492 35.1288935929813\n",
      "495 34.54772492503171\n",
      "498 33.97791670226234\n",
      "500 33.604147700410806\n",
      "501 33.41940295223767\n",
      "504 32.87206928087334\n",
      "507 32.33536399777944\n",
      "510 31.80905145050072\n",
      "513 31.29296430847099\n",
      "516 30.78773801950318\n",
      "519 30.292630654275936\n",
      "522 29.80770681449607\n",
      "525 29.332733947516324\n",
      "528 28.866219452070787\n",
      "531 28.407928245113183\n",
      "534 27.95895881314583\n",
      "537 27.51959381210716\n",
      "540 27.09024080398241\n",
      "543 26.669270138934827\n",
      "546 26.25729783124639\n",
      "549 25.854820897396\n",
      "552 25.46108156451636\n",
      "555 25.07644232898317\n",
      "558 24.700978093321492\n",
      "561 24.334535842740046\n",
      "564 23.976120247304344\n",
      "567 23.625938371321165\n",
      "570 23.283832504064428\n",
      "573 22.949075519080775\n",
      "576 22.621025119773314\n",
      "579 22.299943958241155\n",
      "582 21.9858823793432\n",
      "585 21.678785215106082\n",
      "588 21.37835049832271\n",
      "591 21.084530617242315\n",
      "594 20.797800176483108\n",
      "597 20.517958470497554\n",
      "600 20.245144049115847\n",
      "600 20.245144049115847\n",
      "603 19.97885985481234\n",
      "606 19.718620742428126\n",
      "609 19.46443540818609\n",
      "612 19.21672666648155\n",
      "615 18.975161486740582\n",
      "618 18.739739378724835\n",
      "621 18.5101395644084\n",
      "624 18.286012791290435\n",
      "627 18.067326968510802\n",
      "630 17.853997715975837\n",
      "633 17.6458237536205\n",
      "636 17.44265300549991\n",
      "639 17.244259277990473\n",
      "642 17.05060396643251\n",
      "645 16.861743850209837\n",
      "648 16.67789487403965\n",
      "651 16.498573178999028\n",
      "654 16.32357036454826\n",
      "657 16.152820326478647\n",
      "660 15.986281457946163\n",
      "663 15.82404026332269\n",
      "666 15.666079914239633\n",
      "669 15.512140425203059\n",
      "672 15.361841708804194\n",
      "675 15.215217816384005\n",
      "678 15.072467493967268\n",
      "681 14.93332941236292\n",
      "684 14.797687586907438\n",
      "687 14.665384172612525\n",
      "690 14.536400314006366\n",
      "693 14.410887351884965\n",
      "696 14.288415609602488\n",
      "699 14.16893605235524\n",
      "700 14.129790764971084\n",
      "702 14.052262800415772\n",
      "705 13.938470971230286\n",
      "708 13.82767645999767\n",
      "711 13.719675493766205\n",
      "714 13.614390179116928\n",
      "717 13.511596574965543\n",
      "720 13.411090290187015\n",
      "723 13.3127249457559\n",
      "726 13.216631145866453\n",
      "729 13.122637562373304\n",
      "732 13.030584808273955\n",
      "735 12.940493797086857\n",
      "738 12.852549294036907\n",
      "741 12.766759531306027\n",
      "744 12.68293304089745\n",
      "747 12.600962970316052\n",
      "750 12.520829813544811\n",
      "753 12.442540669861279\n",
      "756 12.365736453709292\n",
      "759 12.2905090632185\n",
      "762 12.216734772998585\n",
      "765 12.14427297749957\n",
      "768 12.073205006042695\n",
      "771 12.003621563159667\n",
      "774 11.935089932362647\n",
      "777 11.867600956947692\n",
      "780 11.801234916730253\n",
      "783 11.735951434719606\n",
      "786 11.671850727200558\n",
      "789 11.609010829244378\n",
      "792 11.547161613632486\n",
      "795 11.486130128035215\n",
      "798 11.426073671940717\n",
      "800 11.386629566950173\n",
      "801 11.367106618476909\n",
      "804 11.309292672327219\n",
      "807 11.252595401096803\n",
      "810 11.197047370718728\n",
      "813 11.142446975263745\n",
      "816 11.088730240180032\n",
      "819 11.035949420945734\n",
      "822 10.98403324527937\n",
      "825 10.933082715154393\n",
      "828 10.882956810282831\n",
      "831 10.833634116881937\n",
      "834 10.785135142230306\n",
      "837 10.7374338896611\n",
      "840 10.69048940012489\n",
      "843 10.644285467159355\n",
      "846 10.598784216027022\n",
      "849 10.553912775299489\n",
      "852 10.509798302834549\n",
      "855 10.46643754202813\n",
      "858 10.42380633250503\n",
      "861 10.38184581223038\n",
      "864 10.340630003659234\n",
      "867 10.300077367450724\n",
      "870 10.260004533544679\n",
      "873 10.220485354491789\n",
      "876 10.181613375473054\n",
      "879 10.143452194991232\n",
      "882 10.105958792807066\n",
      "885 10.06914442212578\n",
      "888 10.032846615957492\n",
      "891 9.997103104900258\n",
      "894 9.962159326190022\n",
      "897 9.927990858434393\n",
      "900 9.894375293967643\n",
      "900 9.894375293967643\n",
      "903 9.86139986026751\n",
      "906 9.828942517779822\n",
      "909 9.796983486221324\n",
      "912 9.76545708798792\n",
      "915 9.734398979367242\n",
      "918 9.703720521075516\n",
      "921 9.673539277708224\n",
      "924 9.64371083154057\n",
      "927 9.614244207981482\n",
      "930 9.585193335423268\n",
      "933 9.556435972927362\n",
      "936 9.528141557580572\n",
      "939 9.499976702351818\n",
      "942 9.472165147005054\n",
      "945 9.444528305275988\n",
      "948 9.417044651788885\n",
      "951 9.389758495248051\n",
      "954 9.362624847173613\n",
      "957 9.335485202299184\n",
      "960 9.3085111168177\n",
      "963 9.281686791383294\n",
      "966 9.25500131203611\n",
      "969 9.228564418500387\n",
      "972 9.202347804954812\n",
      "975 9.176476497496347\n",
      "978 9.150940148877764\n",
      "981 9.125675130983911\n",
      "984 9.100633672832153\n",
      "987 9.075879395315011\n",
      "990 9.051396245643204\n",
      "993 9.027170075952643\n",
      "996 9.003116943759126\n",
      "999 8.979239500148221\n",
      "1000 8.971322380367834\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = auto_mpg_model.train(Xtrain_auto, ytrain_auto, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_auto_mpg = nn.Sequential(nn.Linear(Xtrain_auto.shape[1], 32),\n",
    "#                               nn.Tanh(),\n",
    "#                               nn.Linear(32, 1),\n",
    "#                               nn.ReLU())\n",
    "# lossFunc_auto_mpg = nn.MSELoss()\n",
    "# optimizer_auto_mpg = torch.optim.Adagrad(model_auto_mpg.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fval_auto_mpg = []\n",
    "# for _ in range(100):\n",
    "#     for i in range(Xtrain_auto.shape[0]//100):\n",
    "#         optimizer_auto_mpg.zero_grad()\n",
    "#         ypred_auto_mpg = model_auto_mpg(Xtrain_auto[i:i+100].float())\n",
    "#         loss = lossFunc_auto_mpg(ypred_auto_mpg, ytrain_auto[i:i+100].float())\n",
    "#         loss.backward()\n",
    "#         optimizer_auto_mpg.step()\n",
    "#         fval_auto_mpg.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotLoss(fval_auto_mpg[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIrisDataset():\n",
    "    iris_dataset = pd.read_csv(\"./dataset/iris/iris.data\", header=-1)\n",
    "    iris_dataset[4] = iris_dataset[4].astype('category').cat.codes\n",
    "    iris_dataset = torch.tensor(iris_dataset.values, dtype=torch.double)\n",
    "    return iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = loadIrisDataset()\n",
    "iris_dataset = iris_dataset[torch.randperm(iris_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris = iris_dataset[:int(0.8 * iris_dataset.shape[0])]\n",
    "test_iris = iris_dataset[int(0.8 * iris_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_iris = train_iris[:, :-1]\n",
    "Xtrain_iris = (Xtrain_iris - Xtrain_iris.mean(dim=0))/Xtrain_iris.std(dim=0)\n",
    "ytrain_iris = train_iris[:, -1].reshape(-1, 1)\n",
    "\n",
    "Xtest_iris = test_iris[:, :-1]\n",
    "Xtest_iris = (Xtest_iris - Xtest_iris.mean(dim=0))/Xtest_iris.std(dim=0)\n",
    "ytest_iris = test_iris[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = NeuralNetwork(Xtrain_iris.shape[1], 3, 64, ['relu', 'linear'], 'CELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8.314807454633494\n",
      "2 8.206486675069984\n",
      "3 8.099706163840137\n",
      "4 7.994564122227178\n",
      "5 7.891001202284693\n",
      "6 7.7890551451033385\n",
      "7 7.688648100142979\n",
      "8 7.589723852627087\n",
      "9 7.492252052934092\n",
      "10 7.396094415533881\n",
      "11 7.301193334611289\n",
      "12 7.207440942870722\n",
      "13 7.114792877051132\n",
      "14 7.02315663523415\n",
      "15 6.932467165685364\n",
      "16 6.842632964735234\n",
      "17 6.753617973494497\n",
      "18 6.665403796528525\n",
      "19 6.5779475842044866\n",
      "20 6.491192027006312\n",
      "21 6.405126487391411\n",
      "22 6.319730169083349\n",
      "23 6.234961019465201\n",
      "24 6.150839277997982\n",
      "25 6.067354470410786\n",
      "26 5.9844639877662855\n",
      "27 5.902200870774971\n",
      "28 5.820658053073322\n",
      "29 5.739781715616001\n",
      "30 5.659540730827637\n",
      "31 5.5799812854399935\n",
      "32 5.501012689803721\n",
      "33 5.422795731064263\n",
      "34 5.345325801314389\n",
      "35 5.2686532031633515\n",
      "36 5.192754204738509\n",
      "37 5.117645290748017\n",
      "38 5.043366417659939\n",
      "39 4.969961147323084\n",
      "40 4.897512372187171\n",
      "41 4.826003887964048\n",
      "42 4.755445312766147\n",
      "43 4.685858736840694\n",
      "44 4.6172591099595826\n",
      "45 4.549703412796116\n",
      "46 4.483216288667526\n",
      "47 4.417797854457857\n",
      "48 4.35345461241891\n",
      "49 4.290200272117244\n",
      "50 4.228038853000463\n",
      "51 4.166965343233091\n",
      "52 4.106966900457299\n",
      "53 4.048031230413879\n",
      "54 3.9901428324515678\n",
      "55 3.93326743728773\n",
      "56 3.8773913534050806\n",
      "57 3.822484466399588\n",
      "58 3.7685232263117685\n",
      "59 3.715480940294016\n",
      "60 3.663340582967656\n",
      "61 3.6120826905376364\n",
      "62 3.5616843765816695\n",
      "63 3.512125163333\n",
      "64 3.4634127143025975\n",
      "65 3.4155165355449615\n",
      "66 3.368409750426475\n",
      "67 3.322094918919009\n",
      "68 3.2765268639368568\n",
      "69 3.23168839770034\n",
      "70 3.1875602026349585\n",
      "71 3.1441208634601945\n",
      "72 3.101346430777686\n",
      "73 3.059222920751827\n",
      "74 3.0177255248061523\n",
      "75 2.9768313142156497\n",
      "76 2.936533505725313\n",
      "77 2.896827093444465\n",
      "78 2.857682593114631\n",
      "79 2.819081187244683\n",
      "80 2.7810052235665315\n",
      "81 2.7434393070435754\n",
      "82 2.7063620691261843\n",
      "83 2.6697614190054177\n",
      "84 2.6336173052270477\n",
      "85 2.597917365320437\n",
      "86 2.5626471111184\n",
      "87 2.5277900506848994\n",
      "88 2.49331880244807\n",
      "89 2.4592307070175323\n",
      "90 2.425502972216909\n",
      "91 2.3921258062351383\n",
      "92 2.359091721184664\n",
      "93 2.32639261036416\n",
      "94 2.294018215877476\n",
      "95 2.261958258919147\n",
      "96 2.230181222698187\n",
      "97 2.1987010981770845\n",
      "98 2.1675102453516795\n",
      "99 2.1366016052861747\n",
      "100 2.105970647932259\n",
      "100 2.105970647932259\n",
      "101 2.075617640356809\n",
      "102 2.0455279470402377\n",
      "103 2.0156920726999745\n",
      "104 1.986113609761383\n",
      "105 1.9567849799197232\n",
      "106 1.927688607764386\n",
      "107 1.8988329908841928\n",
      "108 1.8702122388311342\n",
      "109 1.8418187516297984\n",
      "110 1.813646044512837\n",
      "111 1.7857140701687897\n",
      "112 1.7579928651241883\n",
      "113 1.7305070889188783\n",
      "114 1.7032547977211718\n",
      "115 1.6762361918881645\n",
      "116 1.6494645758859616\n",
      "117 1.622975466216842\n",
      "118 1.5967689605816189\n",
      "119 1.5708106416754408\n",
      "120 1.54510515934247\n",
      "121 1.5196577189518294\n",
      "122 1.494476058166938\n",
      "123 1.4695637697737909\n",
      "124 1.4449283963876458\n",
      "125 1.4205672778311134\n",
      "126 1.396464200442129\n",
      "127 1.372649596848783\n",
      "128 1.3491251395641841\n",
      "129 1.3259016524573133\n",
      "130 1.3029863413609657\n",
      "131 1.2803632568970396\n",
      "132 1.2580562944650597\n",
      "133 1.2360733978130014\n",
      "134 1.2144202991751554\n",
      "135 1.1930997694478118\n",
      "136 1.1721148629105491\n",
      "137 1.1514714619270343\n",
      "138 1.1311459247390812\n",
      "139 1.11115933845342\n",
      "140 1.091517289384854\n",
      "141 1.0722237312366085\n",
      "142 1.053262713981012\n",
      "143 1.0346344263908032\n",
      "144 1.0163468333393464\n",
      "145 0.998386046236698\n",
      "146 0.9807519349986763\n",
      "147 0.963437187699824\n",
      "148 0.9464361970607674\n",
      "149 0.9297255377175533\n",
      "150 0.9133181558728874\n",
      "151 0.8972090733631055\n",
      "152 0.8813984585108433\n",
      "153 0.8658764031600091\n",
      "154 0.8506410322412343\n",
      "155 0.8356904876136321\n",
      "156 0.8210166536949348\n",
      "157 0.8066410205902839\n",
      "158 0.7925362537515197\n",
      "159 0.7786994435015357\n",
      "160 0.7651291251035404\n",
      "161 0.7518257945736803\n",
      "162 0.7387860692475585\n",
      "163 0.7260091870607573\n",
      "164 0.7134906104427475\n",
      "165 0.7012180635327534\n",
      "166 0.6892070536370102\n",
      "167 0.6774608239626948\n",
      "168 0.6659974422887813\n",
      "169 0.6548079830014337\n",
      "170 0.6438949188005184\n",
      "171 0.6332616157580714\n",
      "172 0.622908551362628\n",
      "173 0.6128353934047424\n",
      "174 0.6030441653604905\n",
      "175 0.5935369387780757\n",
      "176 0.5843099696763818\n",
      "177 0.5753661722647884\n",
      "178 0.5667036656076733\n",
      "179 0.558321094540344\n",
      "180 0.5502157696200478\n",
      "181 0.5423838130840561\n",
      "182 0.5348260991560039\n",
      "183 0.5275427933018596\n",
      "184 0.5205201327689869\n",
      "185 0.5137519497974232\n",
      "186 0.507229241960632\n",
      "187 0.5009495168982236\n",
      "188 0.49490339493753005\n",
      "189 0.48908067669704475\n",
      "190 0.4834770497307687\n",
      "191 0.47808321823239225\n",
      "192 0.47289247955058694\n",
      "193 0.46789675100159067\n",
      "194 0.46309015398941966\n",
      "195 0.45847084828609547\n",
      "196 0.45402329009126696\n",
      "197 0.44974506927772573\n",
      "198 0.4456341807795607\n",
      "199 0.4416831231486402\n",
      "200 0.4378827075962313\n",
      "200 0.4378827075962313\n",
      "201 0.4342198739738536\n",
      "202 0.430687600434204\n",
      "203 0.42727948355441736\n",
      "204 0.4239893253393308\n",
      "205 0.42081116463995444\n",
      "206 0.417739333277123\n",
      "207 0.4147682481058935\n",
      "208 0.4118992150646622\n",
      "209 0.40912359198566783\n",
      "210 0.40643375482894617\n",
      "211 0.4038253710613529\n",
      "212 0.40129544476110934\n",
      "213 0.39883830110129337\n",
      "214 0.39645008950423244\n",
      "215 0.3941322141077588\n",
      "216 0.39187788281548896\n",
      "217 0.3896819467562237\n",
      "218 0.3875420084635325\n",
      "219 0.385455368259967\n",
      "220 0.38341966487153933\n",
      "221 0.3814322349580323\n",
      "222 0.37949102452398975\n",
      "223 0.3775930592298151\n",
      "224 0.37573697098597947\n",
      "225 0.37392039749383493\n",
      "226 0.37214169300991146\n",
      "227 0.3703994798131315\n",
      "228 0.36869227364221197\n",
      "229 0.36702268366235535\n",
      "230 0.3653866340187001\n",
      "231 0.36377797113713206\n",
      "232 0.3621879720174551\n",
      "233 0.3606253865384005\n",
      "234 0.3590894156475143\n",
      "235 0.3575802854322308\n",
      "236 0.35610138292948096\n",
      "237 0.3546481280559862\n",
      "238 0.35321816994883404\n",
      "239 0.3518152285768779\n",
      "240 0.35043750631508863\n",
      "241 0.3490817827065132\n",
      "242 0.34774745827311876\n",
      "243 0.3464339595082027\n",
      "244 0.34514464388762794\n",
      "245 0.34387743145582644\n",
      "246 0.34262961323170255\n",
      "247 0.34140151423494586\n",
      "248 0.3401941293627768\n",
      "249 0.339002211643083\n",
      "250 0.33782738688688496\n",
      "251 0.33667563711695714\n",
      "252 0.3355416345577758\n",
      "253 0.33442470795253837\n",
      "254 0.33332401309655185\n",
      "255 0.33223942605476026\n",
      "256 0.3311709638336565\n",
      "257 0.3301196432416779\n",
      "258 0.32908364927566536\n",
      "259 0.3280625889265124\n",
      "260 0.327055481650176\n",
      "261 0.32606167714925377\n",
      "262 0.32508063532937587\n",
      "263 0.32411189107288335\n",
      "264 0.3231552558485798\n",
      "265 0.322218429942367\n",
      "266 0.32129761370406373\n",
      "267 0.3203889115091413\n",
      "268 0.3194917648902825\n",
      "269 0.31860594927172403\n",
      "270 0.31773119998903115\n",
      "271 0.31686727092369876\n",
      "272 0.3160139236613556\n",
      "273 0.3151709243553718\n",
      "274 0.3143379340905094\n",
      "275 0.313513754443907\n",
      "276 0.31270290806609663\n",
      "277 0.31190261583669937\n",
      "278 0.3111112058729118\n",
      "279 0.3103285011924342\n",
      "280 0.30955247865724705\n",
      "281 0.30877891405342284\n",
      "282 0.30801204796602316\n",
      "283 0.30725230044929525\n",
      "284 0.3064991716693593\n",
      "285 0.30575258945031625\n",
      "286 0.30501253181388777\n",
      "287 0.3042790285344859\n",
      "288 0.3035519442176508\n",
      "289 0.3028327426033893\n",
      "290 0.3021215202352891\n",
      "291 0.30141623587351124\n",
      "292 0.3007154663278576\n",
      "293 0.30002089044563024\n",
      "294 0.29933079936844453\n",
      "295 0.29864717404794644\n",
      "296 0.2979692111892897\n",
      "297 0.29729676281596984\n",
      "298 0.29662988422983505\n",
      "299 0.2959686394566487\n",
      "300 0.29531286665777134\n",
      "300 0.29531286665777134\n",
      "301 0.2946629345280055\n",
      "302 0.2940188221967188\n",
      "303 0.29338017489123674\n",
      "304 0.292747138600355\n",
      "305 0.29211915542093436\n",
      "306 0.29149617488048307\n",
      "307 0.2908786744562765\n",
      "308 0.29026605067810074\n",
      "309 0.28965886329501067\n",
      "310 0.2890570014323213\n",
      "311 0.2884598700840248\n",
      "312 0.2878674750061745\n",
      "313 0.2872796692269628\n",
      "314 0.2866964005769882\n",
      "315 0.2861184590584654\n",
      "316 0.2855448865661651\n",
      "317 0.2849755595539595\n",
      "318 0.28441166431560533\n",
      "319 0.28385203606503695\n",
      "320 0.28329640750093626\n",
      "321 0.28274467208277276\n",
      "322 0.282196889790491\n",
      "323 0.28165302083841953\n",
      "324 0.2811130434297275\n",
      "325 0.2805769226183731\n",
      "326 0.28004466989950866\n",
      "327 0.2795168609082122\n",
      "328 0.27899315216278253\n",
      "329 0.2784725837777594\n",
      "330 0.27795498556804166\n",
      "331 0.2774407045619275\n",
      "332 0.2769303443858348\n",
      "333 0.2764232928654449\n",
      "334 0.27592029803071705\n",
      "335 0.2754224700409388\n",
      "336 0.27492749466599453\n",
      "337 0.2744351893276346\n",
      "338 0.2739455998763181\n",
      "339 0.2734587642330241\n",
      "340 0.272974713205377\n",
      "341 0.2724941239557862\n",
      "342 0.2720164992115074\n",
      "343 0.27154160091601176\n",
      "344 0.2710711042729313\n",
      "345 0.2706033214167721\n",
      "346 0.27013876135610104\n",
      "347 0.26967804212450364\n",
      "348 0.26922014557063484\n",
      "349 0.2687650612501059\n",
      "350 0.26831289847044903\n",
      "351 0.26786360094209677\n",
      "352 0.26741714530611743\n",
      "353 0.2669736417512326\n",
      "354 0.2665337237325535\n",
      "355 0.26610294811405\n",
      "356 0.26567615469160083\n",
      "357 0.26525206356492764\n",
      "358 0.264830185530416\n",
      "359 0.2644108410052122\n",
      "360 0.26399411718509486\n",
      "361 0.26358019333200816\n",
      "362 0.2631693119957238\n",
      "363 0.262761582244246\n",
      "364 0.2623562777642421\n",
      "365 0.2619533161130751\n",
      "366 0.2615531168878062\n",
      "367 0.2611551480784173\n",
      "368 0.26075953566711324\n",
      "369 0.2603656646532197\n",
      "370 0.259975863987957\n",
      "371 0.2595894908871288\n",
      "372 0.259205259001337\n",
      "373 0.2588230416654267\n",
      "374 0.2584428399145914\n",
      "375 0.2580649660156254\n",
      "376 0.2576894254577469\n",
      "377 0.2573159516702531\n",
      "378 0.2569451013483615\n",
      "379 0.25657684645330653\n",
      "380 0.25621146754836993\n",
      "381 0.25584780502637877\n",
      "382 0.25548587319514865\n",
      "383 0.25512558501956095\n",
      "384 0.2547674584916243\n",
      "385 0.2544114994787729\n",
      "386 0.2540555517800129\n",
      "387 0.2537004475172479\n",
      "388 0.2533470161821228\n",
      "389 0.25299415854352547\n",
      "390 0.2526415763789943\n",
      "391 0.2522899164388198\n",
      "392 0.2519398308475675\n",
      "393 0.25159111776145926\n",
      "394 0.2512432996072174\n",
      "395 0.2508965554728261\n",
      "396 0.2505518455622982\n",
      "397 0.25020868808822544\n",
      "398 0.24986662328183085\n",
      "399 0.24952546526373107\n",
      "400 0.24918539748916932\n",
      "400 0.24918539748916932\n",
      "401 0.24884762430631197\n",
      "402 0.2485113090593271\n",
      "403 0.24817681425397667\n",
      "404 0.24784369477543397\n",
      "405 0.24751175575434864\n",
      "406 0.24718139275559065\n",
      "407 0.2468526443180652\n",
      "408 0.24652534892876052\n",
      "409 0.24619987826034123\n",
      "410 0.2458738053479184\n",
      "411 0.24554763375147798\n",
      "412 0.24522204425856195\n",
      "413 0.2448979381446888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 0.24457475291630051\n",
      "415 0.24425447963350244\n",
      "416 0.24393559081029909\n",
      "417 0.24361821287648452\n",
      "418 0.24330244548234328\n",
      "419 0.24298760353572518\n",
      "420 0.2426731602492914\n",
      "421 0.24235952082585216\n",
      "422 0.2420477560017747\n",
      "423 0.2417376508337221\n",
      "424 0.24142907483832163\n",
      "425 0.24112166179662978\n",
      "426 0.2408157871962168\n",
      "427 0.24051049203084424\n",
      "428 0.2402057270406396\n",
      "429 0.23990169433535788\n",
      "430 0.2395993822158028\n",
      "431 0.23929799342486127\n",
      "432 0.23900275639941101\n",
      "433 0.23870977277779643\n",
      "434 0.23841685292626472\n",
      "435 0.23812577775564056\n",
      "436 0.23783535563764605\n",
      "437 0.23754571878084807\n",
      "438 0.23725624860505595\n",
      "439 0.23696706465829204\n",
      "440 0.23667885961628773\n",
      "441 0.23639243413868463\n",
      "442 0.23610718361968586\n",
      "443 0.23582321741190446\n",
      "444 0.23554080510775746\n",
      "445 0.23526006193264362\n",
      "446 0.23498020317018992\n",
      "447 0.2347011516861958\n",
      "448 0.2344236314004534\n",
      "449 0.23414783586780227\n",
      "450 0.23387285244135056\n",
      "451 0.23359934805352592\n",
      "452 0.23332732130640257\n",
      "453 0.23305628503254033\n",
      "454 0.2327874095561523\n",
      "455 0.2325192970486644\n",
      "456 0.23225196593888547\n",
      "457 0.23198924893033906\n",
      "458 0.23172773676265793\n",
      "459 0.2314675341271117\n",
      "460 0.23120903351371608\n",
      "461 0.2309518362492459\n",
      "462 0.23069558209699983\n",
      "463 0.23044011450418572\n",
      "464 0.23018589356840946\n",
      "465 0.2299334329877077\n",
      "466 0.22968212953215444\n",
      "467 0.22943219995406147\n",
      "468 0.22918355876221283\n",
      "469 0.2289355371460098\n",
      "470 0.22869172214426026\n",
      "471 0.22845031502910318\n",
      "472 0.2282097747052933\n",
      "473 0.2279697574692463\n",
      "474 0.22773150516256935\n",
      "475 0.22749453519221025\n",
      "476 0.2272598762791933\n",
      "477 0.22702621950088517\n",
      "478 0.22679389941221434\n",
      "479 0.22656216512097732\n",
      "480 0.22633108126403612\n",
      "481 0.22610097095029652\n",
      "482 0.22587191617668032\n",
      "483 0.22564439103082537\n",
      "484 0.22541780066174744\n",
      "485 0.2251913022559797\n",
      "486 0.2249649969910625\n",
      "487 0.2247394462204858\n",
      "488 0.22451513558576305\n",
      "489 0.22429182494914998\n",
      "490 0.22406968450342926\n",
      "491 0.2238494901019266\n",
      "492 0.2236308261533528\n",
      "493 0.22341591109353756\n",
      "494 0.22320154446367665\n",
      "495 0.22298778178449669\n",
      "496 0.22277481422225892\n",
      "497 0.2225622645830595\n",
      "498 0.2223503787834495\n",
      "499 0.22214013096432117\n",
      "500 0.22193044333497183\n",
      "500 0.22193044333497183\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = iris_model.train(Xtrain_iris, ytrain_iris, batch_size=100, iterations=500, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xl8HfV57/HPc442S7ZlW5I3eZFlBLYxWLaFMcTsEPYlIQtcaELCxTctJdA0zSXtvU1Lb3KbplmahSQuCTQJSxIgYUkCYQ8QsJH3HYwXsDG2vMqrLOk8/WNGQhbClm2N5izf9+t1XmfOzJzze0aWvzP6nZnfmLsjIiLZLxF3ASIi0jsU+CIiOUKBLyKSIxT4IiI5QoEvIpIjFPgiIjlCgS8ikiMU+CIiOUKBLyKSI/LiLqCj8vJyr6qqirsMEZGMMXfu3C3uXtGdddMq8Kuqqqivr4+7DBGRjGFm67q7rrp0RERyhAJfRCRHKPBFRHKEAl9EJEco8EVEcoQCX0QkRyjwRURyRMYHflNLKz964U1efKMh7lJERNJaxgd+QTLBrD+t5pEF78RdiohIWsv4wDcz6kYP5LW12+IuRUQkrWV84AOcUjWIdVv3srlxf9yliIikrawI/LqqgQDUr9secyUiIukr0sA3s78xs6VmtsTM7jezoijamVhZSlF+Qt06IiKHEFngm1kl8Hmgzt0nAkngmijayk8mmDxyIPVrdYQvIvJBou7SyQP6mFkeUAxEdirNKVUDWfrOTnY3tUTVhIhIRoss8N19A/DvwFvARmCnu/8xqvbqqgaRcpj/lo7yRUS6EmWXzkDgSmAMMBwoMbPru1hvppnVm1l9Q8PRXzw1ZfRAEgavqVtHRKRLUXbpnA+scfcGd28GHgZO77ySu89y9zp3r6uo6NZdurrUtzCPCcP7U68vbkVEuhRl4L8FTDezYjMz4DxgeYTtUTd6EPPf2kFzayrKZkREMlKUffizgQeBecDisK1ZUbUHwQVY+5pbWfpOY5TNiIhkpEjP0nH3r7j7OHef6O5/4e5NUbZ3SngB1mtr1K0jItJZVlxp22Zw/yKqyoqZrcAXEXmfrAp8gOnVZcxes5XWlMddiohIWsm6wD9tbBm79rewTP34IiIHyb7Ary4D4JXVW2KuREQkvWRd4A/uX0R1RQmvvLk17lJERNJK1gU+BEf5r63dTovOxxcRaZeVgT+9uozdTS0sUT++iEi7rA18QN06IiIdZGXgV/QrpGZwX15ZrcAXEWmTlYEPwemZ9Wu3aVwdEZFQ9gZ+dRl7D7SyaP2OuEsREUkLWRv4p6ofX0TkIFkb+INKChg/rD8vrdIFWCIikMWBD3BGTTlz121n7wHd51ZEJKsDf8Zx5TS3ukbPFBEhywN/2phBFOQlePF1deuIiER5E/MTzGxBh0ejmd0WVXtdKcpPMq1qEC+tOvqbo4uIZIsob3G40t1r3b0WmArsBX4TVXsf5Iyacl7ftJtNjft7u2kRkbTSW1065wFvuvu6Xmqv3YyacgBefEPdOiKS23or8K8B7u9qgZnNNLN6M6tvaOj5rpfxQ/tTVlLAS2+oW0dEclvkgW9mBcAVwK+7Wu7us9y9zt3rKioqerz9RMKYUVPOS6u2ktJtD0Ukh/XGEf7FwDx339QLbXVpxnHlbNndxIp3d8VVgohI7Hoj8K/lA7pzessZNcFfDjpbR0RyWaSBb2YlwAXAw1G2czhDS4uoGdxXX9yKSE6LNPDdfY+7l7n7zijb6Y4ZNeXMWbON/c2tcZciIhKLrL7StqMzaypoakkxR8MsiEiOypnAn15dRkFegudXqh9fRHJTzgR+n4Ikp1WX8fzKzXGXIiISi5wJfICzT6hg9ZY9rNu6J+5SRER6XU4F/jknDAZQt46I5KScCvyq8hKqyorVrSMiOSmnAh/g7BMG88rqrTo9U0RyTg4GfgX7m1O8ulo3NxeR3JJzgT+9uoyifJ2eKSK5J+cCvyhfp2eKSG7KucAHOGfcYNZu3cuaLTo9U0RyR04G/tnHt52eqaN8EckdORn4o8qKqa4oUT++iOSUnAx8CI7yX1m9lX0HdHqmiOSGnA38c8ZVcKAlxSurNUa+iOSGqG+AMsDMHjSzFWa23MxOi7K9IzFtzCCKC5I8u0L9+CKSG6I+wv8P4Al3HwdMApZH3F63FeYlOaOmnGeWb8ZdNzcXkewXWeCbWSlwJvATAHc/4O47omrvaJw/fggbd+5n6TuNcZciIhK5KI/wxwANwN1mNt/M7grvcZs2zh03GDN4evmmuEsREYlclIGfB0wBfujuk4E9wO2dVzKzmWZWb2b1DQ29e5pkWd9CpowaqMAXkZwQZeCvB9a7++zw9YMEO4CDuPssd69z97qKiooIy+na+eOHsGRDIxt37uv1tkVEelNkge/u7wJvm9kJ4azzgGVRtXe0LpgQXHX7zHKdrSMi2S3qs3RuAe41s0VALfC1iNs7YmMr+jK6rJhn1K0jIlkuL8oPd/cFQF2UbRwrM+P88UP4+avr2NPUQklhpD8SEZHY5OyVth2dN34wB1pSvPiGrroVkeylwAdOqRpE/6I8deuISFZT4AP5yQTnjBvMsys205rSVbcikp0U+KHzxg9h654DLHg7rS4GFhHpMQr80FnHV5CXMJ5apm4dEclOCvxQaZ98pleX8dSyd+MuRUQkEgr8Di6cOJQ3G/bwxqZdcZciItLjFPgdXDhhCGbwxBId5YtI9lHgdzC4fxFTRg3kiaUKfBHJPgr8Ti46cShL32nk7W174y5FRKRHKfA7ufDEoQA8qaN8EckyCvxORpUVM2FYf/6gfnwRyTIK/C5cNHEoc9dtZ3Pj/rhLERHpMQr8Llw0MezW0UVYIpJFFPhdqBncl+ryEp5Ut46IZBEFfhfMjAsnDuWV1VvZsfdA3OWIiPSISAPfzNaa2WIzW2Bm9VG21dMunjiU1pTrbB0RyRq9cYR/jrvXunta3/mqs5MqSxk1qJjHF22MuxQRkR6hLp0PYGZcPmkYL6/awpbdTXGXIyJyzKIOfAf+aGZzzWxmVyuY2Uwzqzez+oaGhojLOTKXTxpOyuEPi3WULyKZL+rAn+HuU4CLgZvN7MzOK7j7LHevc/e6ioqKiMs5MicM6UfN4L48tlCBLyKZL9LAd/cN4fNm4DfAtCjb62lBt85w5qzdxsad++IuR0TkmEQW+GZWYmb92qaBDwNLomovKpedPAyA3+nLWxHJcFEe4Q8BXjKzhcAc4Hfu/kSE7UWiuqIvEyv785gCX0QyXF5UH+zuq4FJUX1+b7r85OH8/z+s4K2texlVVhx3OSIiR0WnZXbDpWG3zmOL3om5EhGRo6fA74YRA4uZMmoAjy1U4ItI5lLgd9NVkytZ8e4ulr6zM+5SRESOSrcC38zGmllhOH22mX3ezAZEW1p6ufzk4eQnjYfmboi7FBGRo9LdI/yHgFYzOw6YBYwE7ousqjQ0sKSA88YN4ZEFG2huTcVdjojIEetu4KfcvQX4CPA9d/87YFh0ZaWnq6eOYOueA7ywMr2GgBAR6Y7uBn6zmV0LfBp4PJyXH01J6evsEyooKyngoXnr4y5FROSIdTfwPwOcBnzV3deY2Rjg59GVlZ7ykwmurK3k6eWb2L5HN0YRkczSrcB392Xu/nl3v9/MBgL93P3rEdeWlq6eWklzq+ucfBHJON09S+d5M+tvZoOAecB/mtm3oi0tPZ04vJRxQ/vx4Fx164hIZulul06puzcCHwV+5u6nAudHV1Z6+0TdSBat36lz8kUko3Q38PPMbBjwCd770jZnXT1lBIV5Ce6b/VbcpYiIdFt3A/8O4EngTXd/zcyqgTeiKyu9lRbnc9nJw/nt/A3sbmqJuxwRkW7p7pe2v3b3k939L8PXq9396mhLS2/XTR/FngOtPLpAX96KSGbo7pe2I8zsN2a2OXw8ZGYjoi4unU0eOYBxQ/tx7+x1uHvc5YiIHFZ3u3TuBh4FhoePx8J5h2VmSTObb2ZZ1fdvZlw3fTRL32lk4Xp9eSsi6a+7gV/h7ne7e0v4uAfo7h3HbwWWH1V1ae6q2uEUFyS5b/a6uEsRETms7gb+VjO7PjxaT5rZ9cDWw70p7Pa5FLjrWIpMV/2K8rmydjiPLnxHV96KSNrrbuB/luCUzHeBjcDHgBu68b7vAF8CsnZ4yRtOH8P+5hT3zdEpmiKS3rp7ls46d7/C3SvcfbC7XwUc8iwdM7sM2Ozucw+z3kwzqzez+oaGzBuF8oSh/Tijppz/+vNaDrRk7X5NRLLAsdzx6guHWf4h4AozWws8AJxrZr/ovJK7z3L3Onevq6jo7tcC6eWzM8aweVcTv1usUzRFJH0dS+DboRa6+5fdfYS7VwHXAM+6+/XH0F7aOqumgrEVJfzkpTU6RVNE0taxBL6SLZRIGDfOqGbJhkbmrNkWdzkiIl06ZOCb2S4za+zisYvgfPxucffn3f2yY642jX10SiUDi/O566U1cZciItKlQwa+u/dz9/5dPPq5e15vFZkJivKTXD99NE8v38SqzbviLkdE5H2OpUtHOrnh9CqK8pLc+fybcZciIvI+CvweVNa3kP9x6igeWfAOb23dG3c5IiIHUeD3sJlnVpM040d/0lG+iKQXBX4PG9K/iI/XjeDB+vW8u3N/3OWIiLRT4Efgc2eNpdWdWX9aHXcpIiLtFPgRGDmomKtqK7lvzjo2N+ooX0TSgwI/Ip8/7zhaWp3vP7cq7lJERAAFfmRGl5XwyVNGcv+ct3h7m87YEZH4KfAjdMu5NSTM+PbTr8ddioiIAj9KQ0uL+PTpVfxm/gZe36Srb0UkXgr8iH3urLGUFOTxzT+ujLsUEclxCvyIDSop4KYzqnly6Sbq12okTRGJjwK/F9x05hiG9i/ijseXkUppVGkRiYcCvxcUF+TxpYtOYNH6nfx2wYa4yxGRHBVZ4JtZkZnNMbOFZrbUzP45qrYywVW1lUwaUcrXn1jB3gMtcZcjIjkoyiP8JuBcd58E1AIXmdn0CNtLa4mE8X8vm8CmxiZ+/IKGXBCR3hdZ4Htgd/gyP3zkdAd2XdUgLjt5GD/+05u6GEtEel2kffhmljSzBcBm4Cl3nx1le5ng7y8ZT8KMrzy6VDc8F5FeFWngu3uru9cCI4BpZjax8zpmNtPM6s2svqGhIcpy0sLwAX34wgXH8+yKzTy5dFPc5YhIDumVs3TcfQfwHHBRF8tmuXudu9dVVFT0Rjmxu+H0KsYP688/PbqU3U36AldEekeUZ+lUmNmAcLoPcAGwIqr2MkleMsFXPzKRTbv28+2nNM6OiPSOKI/whwHPmdki4DWCPvzHI2wvo0wZNZBrp43i7pfXsGTDzrjLEZEcEOVZOovcfbK7n+zuE939jqjaylT/+8JxDCop4PaHF9Hcmoq7HBHJcrrSNkalxfn8y5UTWbKhkR89r5uei0i0FPgxu/ikYVw+aTjfffYNlm9sjLscEcliCvw0cMcVJ1Lap4C//dVCde2ISGQU+GlgYEkBX/vIRJZtbOQHugeuiEREgZ8mPnziUK6qHc73n13F4vU6a0dEep4CP4380xUnUt63kFsfmM8eXZAlIj1MgZ9GBhQX8O1P1rJm6x7ueGxZ3OWISJZR4KeZ08aWcfPZx/HL+rf53aKNcZcjIllEgZ+Gbj2/htqRA7j94UWs365hlEWkZyjw01B+MsF3r5mMO9z2wAKdqikiPUKBn6ZGlRXz1Y9MpH7ddv7tCY05JyLHToGfxq6sreRTp43mP19cwxNL1J8vIsdGgZ/m/uHS8UwaOYAv/noRqxt2H/4NIiIfQIGf5grzktx53RTyk8Zf3TuPfQda4y5JRDKUAj8DVA7ow3eumczKTbv4h98u1r1wReSoKPAzxFnHV/D5c2t4eN4Gfv7qurjLEZEMFOUtDkea2XNmtszMlprZrVG1lStuPa+G88YN5h8fWcpNP6tX946IHJEoj/BbgL919wnAdOBmM5sQYXtZL5Ewvn1NLVdPGcFTyzbxxV8vJJVS946IdE+Utzjc6O7zwuldwHKgMqr2ckX/ony++YlJ/P0l4/jd4o1852ndBF1EuievNxoxsypgMjC7i2UzgZkAo0aN6o1yssJNZ1TzxqbdfPfZVYwd3Jcra7UvFZFDi/xLWzPrCzwE3Obu77uHn7vPcvc6d6+rqKiIupysYWZ89SMnMW3MIP7uwUXMe2t73CWJSJqLNPDNLJ8g7O9194ejbCsXFeQl+NH1Uxnav4iZP5vLhh374i5JRNJYlGfpGPATYLm7fyuqdnLdoJICfvLpOpqaW/ns3a+xc19z3CWJSJqK8gj/Q8BfAOea2YLwcUmE7eWsmiH9+OH1U1m9ZTczf1bP/madriki7xflWTovubu5+8nuXhs+fh9Ve7luRk05//7xScxes40v/GoBrTpdU0Q66ZWzdKR3XFlbyebGJr76++UM7reMr1w+gaBnTUREgZ91bjqzmk2N+7nrpTUM6V/EX549Nu6SRCRNKPCz0N9fMp7Nu5r4+hMrqOhXyMemjoi7JBFJAwr8LJRIGN/4+Mls33uALz24kD75SS49eVjcZYlIzDRaZpYqzEvy47+YytTRA7n1gfk8tWxT3CWJSMwU+FmsuCCPn95wCidWlnLzvfP40+sNcZckIjFS4Ge5fkX5/Owz0zhucF9m/ryeV1dvjbskEYmJAj8HlBbn8/MbpzFyYDGfufs1/rxqS9wliUgMFPg5oqxvIffdNJ3RZcXccM9rPLdic9wliUgvU+DnkIp+hdx/03SOHxJ07zyx5N24SxKRXqTAzzEDSwq4939O56TKUm6+bx6PLNgQd0ki0ksU+DmotE8+P7/xVE6pGshtv1zA3S+vibskEekFCvwcVVKYxz2fmcaHJwzhnx9bxv97fJnujyuS5RT4OawoP8md103lhtOruOulNdzywHwNrSySxTS0Qo5LJoyvXD6BygF9+Orvl7Nxxz5+dP1UBvcvirs0EelhUd7x6qdmttnMlkTVhvQMM+OmM6u587opLN+4i8u+95LukSuShaLs0rkHuCjCz5cedslJw3j4r06nKD/JNT9+lQfmvBV3SSLSg6K849WfgG1Rfb5EY/yw/jz61x/i1OpB3P7wYr7wqwXsbmqJuywR6QH60lbeZ0BxAfd8Zhq3nlfDb+dv4NLvvsiCt3fEXZaIHKPYA9/MZppZvZnVNzRoNMd0kUwYf3PB8Tww8zSaW1J87Id/5gfPraKlNRV3aSJylGIPfHef5e517l5XUVERdznSybQxg/jDrWdy4cShfOPJlVx158ssfWdn3GWJyFGIPfAl/ZUW5/P9aydz53VTeHdnE1d8/2W+/sQKnbMvkmGiPC3zfuAV4AQzW29mN0bVlkTPzLjkpGE8/YUz+ejkSn74/Juc/60X+MPijbjrCl2RTGDp9J+1rq7O6+vr4y5DuuHPb27hjseWseLdXZw6ZhBfufxEJgzvH3dZIjnHzOa6e1131lWXjhyV08eW8/gtM/iXqyby+qZdXPq9F7ntgfms2bIn7tJE5APoCF+O2c69zdz5wir+689raW51Pjq5klvOrWFUWXHcpYlkvSM5wlfgS49p2NXED59/k1/MXkdLa4qLJw7jxjPGMGXUwLhLE8laCnyJ1abG/dz98lrum72Oxv0tTBk1gOunj+biicPoU5CMuzyRrKLAl7Swp6mFB+eu5+6X17B26176FuZx6UnDuHrqCOpGDySRsLhLFMl4CnxJK+7OnDXb+PXc9fx+8Ub2HmilvG8hF0wYzAUThnD62HKK8nXkL3I0FPiStvY0tfD08k08tWwTz69sYHdTCwXJBLWjBjC9uozpYwZx0ohS+hXlx12qSEZQ4EtGaGpp5dXV2/jzqi28snorSzbspO0ui1VlxZxYWcqJw/tTXV5C5YBiRgzsw4DifMzUFSTS5kgCX3e8ktgU5iU56/gKzjo+GEOpcX8zc9dtZ+mGnSzZ0MjCt3fwu0UbD3pP38I8Rgzsw8hBxYwcWMzIQX0Y0r+Iin6FVPQtpKJfISWF+rUW6Yr+Z0ja6F+UzzknDOacEwa3z9u5t5m3t+9l/fZ9rA+f3962l3Vb9/DSG1vY18V4PiUFSSr6FVLWt5ABffIpLc5nQJ8CBhTnU9onv/05mC6gX1EefQvzKMxL6K8HyWoKfElrpcX5lBaXMrGy9H3L3J1tew6weVcTDeGjfXp3E1t2NbFx535WvLuLnfuaD3sjl2TCKC5I0rcwj5Lw0bcwSXFBXjgvGcwvyKNPfpKi/ASF+UmK8pMU5SWC5/xk+7Ki/CSF4XNRXpL8pGmHIrFS4EvGMjPK+gZH8uOHHX795tYUjfua2bGvmR17m2nc18z2vQfY3dTC7qYW9jS1sKepNXg+0MLucHrr7r3sbmph74FWdje1cKDl6O4JkDDadwqFeQnykwkKOjwXJO2918kE+XkJCpOHWe+g+cF0XtLITxp5iQR5CSOvbV4ieG6fl7D31k8kSIbL8pMJkjplNisp8CVn5CcT7TuIY9HcmmJ/cyv7mltpag6m9zen2N/S+t50czjdkqKp+eD5+5pbOdCSork1xYHWFAdanAOtKZpbUuxvTrFrf7BTCZaF67WkaG719vlRMyPYCSQs3IEkDtoZHLQDCZflJYxkx4cZiXB+23PneUnr9J5O6yQTRsIO/oy297VPJyCZSISfFU4nCN+XIJGAvC7mdWwraUE7ZrS3mQjXb1vW/joRrtfhPZnyl5sCX+QI5YdH0nGdOuruNLd6hx1BiqbwuW1+S8ppaX92mlMpWlo7zEsF67a0BtNtzwfNCz+jucM6za1OaypFc9vntzrNqXBeuENqdSeVclpSTmvbo8O8VPi6NdXp4U4qBS2pVPvZWpkiYYQ7BSPRYWfQ/joRdOe1LTNr25lBImGUlxTyq8+dFnmdCnyRDGNmFOQF3Tolx/bHStpy72InkIJW94OmW1vfv/NIeaedTYd5qfB1S4d57sF0awpS7mHbwXTK23ZQ79WUctrnpzyoo6tlwXza20+F25Ly9y/rV9Q7UazAF5G0YxZ0G70XULoSuydEOh6+mV1kZivNbJWZ3R5lWyIicmhR3uIwCfwAuBiYAFxrZhOiak9ERA4tyiP8acAqd1/t7geAB4ArI2xPREQOIcrArwTe7vB6fTjvIGY208zqzay+oaEhwnJERHJb7Pe0dfdZ7l7n7nUVFRVxlyMikrWiDPwNwMgOr0eE80REJAZRBv5rQI2ZjTGzAuAa4NEI2xMRkUOI7Dx8d28xs78GniQ4ifan7r40qvZEROTQ0uoGKGbWAKw7ireWA1t6uJx0l2vbrO3Nfrm2zT21vaPdvVtfgKZV4B8tM6vv7h1fskWubbO2N/vl2jbHsb2xn6UjIiK9Q4EvIpIjsiXwZ8VdQAxybZu1vdkv17a517c3K/rwRUTk8LLlCF9ERA4j4wM/G4dgNrOfmtlmM1vSYd4gM3vKzN4InweG883Mvhtu/yIzmxJf5UfHzEaa2XNmtszMlprZreH8bN7mIjObY2YLw23+53D+GDObHW7bL8OLFjGzwvD1qnB5VZz1Hy0zS5rZfDN7PHyd7du71swWm9kCM6sP58X2e53RgZ/FQzDfA1zUad7twDPuXgM8E76GYNtrwsdM4Ie9VGNPagH+1t0nANOBm8N/x2ze5ibgXHefBNQCF5nZdODrwLfd/ThgO3BjuP6NwPZw/rfD9TLRrcDyDq+zfXsBznH32g6nYMb3e+3h7bky8QGcBjzZ4fWXgS/HXVcPbVsVsKTD65XAsHB6GLAynP4xcG1X62XqA3gEuCBXthkoBuYBpxJciJMXzm///Sa4Yv20cDovXM/irv0It3MEQcCdCzwOWDZvb1j7WqC807zYfq8z+gifbg7BnCWGuPvGcPpdYEg4nVU/g/BP98nAbLJ8m8PujQXAZuAp4E1gh7u3hKt03K72bQ6X7wTKerfiY/Yd4EtAKnxdRnZvL4ADfzSzuWY2M5wX2++17mmbgdzdzSzrTq8ys77AQ8Bt7t5oZu3LsnGb3b0VqDWzAcBvgHExlxQZM7sM2Ozuc83s7Ljr6UUz3H2DmQ0GnjKzFR0X9vbvdaYf4efSEMybzGwYQPi8OZyfFT8DM8snCPt73f3hcHZWb3Mbd98BPEfQpTHAzNoOxDpuV/s2h8tLga29XOqx+BBwhZmtJbj73bnAf5C92wuAu28InzcT7NSnEePvdaYHfi4Nwfwo8Olw+tME/dxt8z8VfsM/HdjZ4c/FjGDBofxPgOXu/q0Oi7J5myvCI3vMrA/BdxbLCYL/Y+Fqnbe57WfxMeBZDzt6M4G7f9ndR7h7FcH/02fd/TqydHsBzKzEzPq1TQMfBpYQ5+913F9q9MCXIpcArxP0f/5D3PX00DbdD2wEmgn68W4k6L98BngDeBoYFK5rBGcqvQksBurirv8otncGQV/nImBB+Lgky7f5ZGB+uM1LgH8M51cDc4BVwK+BwnB+Ufh6Vbi8Ou5tOIZtPxt4PNu3N9y2heFjaVs+xfl7rSttRURyRKZ36YiISDcp8EVEcoQCX0QkRyjwRURyhAJfRCRHKPAlMmbmZvbNDq+/aGb/1EOffY+Zfezwax5zOx83s+Vm9lyn+VUWjmZqZrVmdkkPtjnAzP6qw+vhZvZgT32+5C4FvkSpCfiomZXHXUhHHa7s7I4bgZvc/ZxDrFNLcN1AT9UwAGgPfHd/x90j37lJ9lPgS5RaCG7j9jedF3Q+Qjez3eHz2Wb2gpk9Ymarzexfzew6C8aOX2xmYzt8zPlmVm9mr4djtbQNSPYNM3stHFP8f3X43BfN7FFgWRf1XBt+/hIz+3o47x8JLgr7iZl9o6sNDK/wvgP4ZDjm+SfDKyx/GtY838yuDNe9wcweNbNngWfMrK+ZPWNm88K2rww/9l+BseHnfaPTXxNFZnZ3uP58Mzunw2c/bGZPWDDO+r91+HncE27XYjN737+F5A4NniZR+wGwqC2AumkSMB7YBqwG7nL3aRbcGOUW4LZwvSqCsUnGAs+Z2XHApwguST/FzAqBl83sj+H6U4CJ7r6mY2NmNpxgvPWpBGOy/9FSjbgeAAACnklEQVTMrnL3O8zsXOCL7l7fVaHufiDcMdS5+1+Hn/c1gqEAPhsOnzDHzJ7uUMPJ7r4tPMr/iAcDxZUDr4Y7pNvDOmvDz6vq0OTNQbN+kpmNC2s9PlxWSzDSaBOw0sy+BwwGKt19YvhZAw7zs5cspiN8iZS7NwI/Az5/BG97zd03unsTwWXmbYG9mCDk2/zK3VPu/gbBjmEcwXgln7Jg2OHZBJex14Trz+kc9qFTgOfdvcGDoXjvBc48gno7+zBwe1jD8wTDBIwKlz3l7tvCaQO+ZmaLCC6xr+S9oXI/yAzgFwDuvgJYB7QF/jPuvtPd9xP8FTOa4OdSbWbfM7OLgMZj2C7JcDrCl97wHYIbfNzdYV4L4QGHmSWAgg7LmjpMpzq8TnHw72zncUGcIERvcfcnOy6wYEjePUdX/hEz4Gp3X9mphlM71XAdUAFMdfdmC0aSLDqGdjv+3FoJbiyy3cwmARcCnwM+AXz2GNqQDKYjfIlceET7K967fR0EdwKaGk5fAeQfxUd/3MwSYb9+NcEdgp4E/tKC4ZYxs+PDkQoPZQ5wlpmVW3DbzGuBF46gjl1Avw6vnwRuMQsG9DezyR/wvlKCMeKbw7740R/weR29SLCjIOzKGUWw3V0Ku4oS7v4Q8H8IupQkRynwpbd8E+h4ts5/EoTsQoJx4I/m6PstgrD+A/C5sCvjLoLujHnhF50/5jB/yXowBO3tBEP1LgTmuvsjh3pPJ88BE9q+tAX+hWAHtsjMloavu3IvUGdmiwm+e1gR1rOV4LuHJV18WXwnkAjf80vghrDr64NUAs+H3Uu/ILgNqOQojZYpIpIjdIQvIpIjFPgiIjlCgS8ikiMU+CIiOUKBLyKSIxT4IiI5QoEvIpIjFPgiIjnivwH+u7rlwxDwbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(Xtrain_iris.shape[1], 32), \n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossFn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fval = []\n",
    "# for epoch in range(1000):\n",
    "#     for i in range(Xtrain_iris.shape[0]//100):\n",
    "#         optimizer.zero_grad()\n",
    "#         yp = model(Xtrain_iris[i:i+100].float())\n",
    "#         loss = lossFn(yp, ytrain_iris[i:i+100].long()[:, 0])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         fval.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotLoss(fval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBreastCancerDataset():\n",
    "    cancer_dataset = pd.read_csv(\"./dataset/breast-cancer/data.csv\")\n",
    "    cancer_dataset.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "    cancer_dataset['diagnosis'] = cancer_dataset['diagnosis'].astype('category').cat.codes\n",
    "    cancer_dataset = torch.tensor(cancer_dataset.values, dtype=torch.double)\n",
    "    return cancer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = loadBreastCancerDataset()\n",
    "cancer_dataset = cancer_dataset[torch.randperm(cancer_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cancer = cancer_dataset[:int(0.8*cancer_dataset.shape[0])]\n",
    "test_cancer = cancer_dataset[int(0.8*cancer_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_cancer = train_cancer[:, 1:]\n",
    "Xtrain_cancer = (Xtrain_cancer-Xtrain_cancer.mean(dim=0))/Xtrain_cancer.std(dim=0)\n",
    "ytrain_cancer = train_cancer[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_cancer = test_cancer[:, 1:]\n",
    "Xtest_cancer = (Xtest_cancer-Xtest_cancer.mean(dim=0))/Xtest_cancer.std(dim=0)\n",
    "ytest_cancer = test_cancer[:, 0].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_model = NeuralNetwork(Xtrain_cancer.shape[1], 1, 32, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.9800841926096813\n",
      "8 2.965088710393259\n",
      "12 2.9281694692046014\n",
      "16 2.8697967177367456\n",
      "20 2.7902036589943986\n",
      "24 2.6871951457033334\n",
      "28 2.581764934297225\n",
      "32 2.4997492609173597\n",
      "36 2.4463006508719993\n",
      "40 2.4123087360719313\n",
      "44 2.3873268362096796\n",
      "48 2.362939056317207\n",
      "52 2.337786958687761\n",
      "56 2.311065723258948\n",
      "60 2.280169064041633\n",
      "64 2.2454073851527405\n",
      "68 2.2051566613899016\n",
      "72 2.155022603590662\n",
      "76 2.0939411762491487\n",
      "80 2.0247842396779783\n",
      "84 1.9564801437689592\n",
      "88 1.9133104359914266\n",
      "92 1.9099931035205793\n",
      "96 1.9273791557952116\n",
      "100 1.9364764129983372\n",
      "100 1.9364764129983372\n",
      "104 1.9132148832464158\n",
      "108 1.8645158443602126\n",
      "112 1.8184098016687058\n",
      "116 1.7947335138587392\n",
      "120 1.7891465235768174\n",
      "124 1.7818907325855537\n",
      "128 1.7642243525423191\n",
      "132 1.7442048056630193\n",
      "136 1.7273057071159446\n",
      "140 1.7103713610920248\n",
      "144 1.6911316729854544\n",
      "148 1.6703910080705295\n",
      "152 1.6526803707965272\n",
      "156 1.6370913572111974\n",
      "160 1.618003627253509\n",
      "164 1.5970979686304767\n",
      "168 1.5772557275769177\n",
      "172 1.5599980464777723\n",
      "176 1.5426365740110224\n",
      "180 1.5246290057980776\n",
      "184 1.5054343100123688\n",
      "188 1.4881700141857677\n",
      "192 1.4721712277465475\n",
      "196 1.4547661121667068\n",
      "200 1.4356996564904265\n",
      "200 1.4356996564904265\n",
      "204 1.4167944292522316\n",
      "208 1.4011839131514598\n",
      "212 1.386881521211456\n",
      "216 1.3752037728644262\n",
      "220 1.364899966836157\n",
      "224 1.3547792775720944\n",
      "228 1.3447964041985154\n",
      "232 1.3346892656567741\n",
      "236 1.323490151180698\n",
      "240 1.314737486302087\n",
      "244 1.3081193081007372\n",
      "248 1.3019714679069914\n",
      "252 1.2959407261315297\n",
      "256 1.2894806436834507\n",
      "260 1.2818622185267854\n",
      "264 1.2744439935752074\n",
      "268 1.2674046167771866\n",
      "272 1.2603822194632712\n",
      "276 1.2542337940745976\n",
      "280 1.2490215075853284\n",
      "284 1.24403772920425\n",
      "288 1.2394667411722036\n",
      "292 1.234790872256861\n",
      "296 1.2302764264146298\n",
      "300 1.22611158959456\n",
      "300 1.22611158959456\n",
      "304 1.2234547502906947\n",
      "308 1.2207803457919773\n",
      "312 1.2170118658152087\n",
      "316 1.2136177455416364\n",
      "320 1.2102378856153373\n",
      "324 1.207010500985147\n",
      "328 1.205448997010782\n",
      "332 1.2046828799098273\n",
      "336 1.201574847485487\n",
      "340 1.1995671285512561\n",
      "344 1.1977117985022392\n",
      "348 1.195856152520617\n",
      "352 1.194450489118423\n",
      "356 1.1923698415831638\n",
      "360 1.1906648241536686\n",
      "364 1.188931201146335\n",
      "368 1.1871180930028817\n",
      "372 1.186041345117743\n",
      "376 1.1841787226658353\n",
      "380 1.1827929966816129\n",
      "384 1.1818764708284417\n",
      "388 1.1812268212620278\n",
      "392 1.1802103299311681\n",
      "396 1.1787228107287375\n",
      "400 1.177918059692224\n",
      "400 1.177918059692224\n",
      "404 1.1775226602753475\n",
      "408 1.1765453613238808\n",
      "412 1.1758889604051812\n",
      "416 1.1753228429936355\n",
      "420 1.174783100985265\n",
      "424 1.1751705459800064\n",
      "428 1.174828389324257\n",
      "432 1.174367385492928\n",
      "436 1.1742708796073682\n",
      "440 1.1734268885251453\n",
      "444 1.172846929252885\n",
      "448 1.1723566941977723\n",
      "452 1.172054143858912\n",
      "456 1.1719834433501681\n",
      "460 1.1711964638471142\n",
      "464 1.1712922505575258\n",
      "468 1.1703882336675513\n",
      "472 1.1700775645696007\n",
      "476 1.1696643313531525\n",
      "480 1.1694260666506515\n",
      "484 1.1690358300216825\n",
      "488 1.1683094523646658\n",
      "492 1.1682882412799955\n",
      "496 1.1677493708980475\n",
      "500 1.167658925232002\n",
      "500 1.167658925232002\n",
      "504 1.1668184759711087\n",
      "508 1.1667236491722233\n",
      "512 1.1661843644050873\n",
      "516 1.1662207001378682\n",
      "520 1.165537424304955\n",
      "524 1.1660313438831988\n",
      "528 1.165217520785016\n",
      "532 1.1657062735072856\n",
      "536 1.1647793273356932\n",
      "540 1.1649499587478012\n",
      "544 1.1645112828515118\n",
      "548 1.1644749565261217\n",
      "552 1.1644235408530101\n",
      "556 1.1639204283236633\n",
      "560 1.1642154757804704\n",
      "564 1.164172513440892\n",
      "568 1.1638298784318146\n",
      "572 1.163942531440173\n",
      "576 1.1631876303415918\n",
      "580 1.163409722863153\n",
      "584 1.1628738497410738\n",
      "588 1.163058395626486\n",
      "592 1.1621497680581327\n",
      "596 1.163011318206452\n",
      "600 1.1619228576604914\n",
      "600 1.1619228576604914\n",
      "604 1.1626388797958231\n",
      "608 1.161998105235224\n",
      "612 1.1623635792998968\n",
      "616 1.1621495233904695\n",
      "620 1.1624872888406275\n",
      "624 1.1618422344553234\n",
      "628 1.162436063220114\n",
      "632 1.1619579357602097\n",
      "636 1.1618844700265252\n",
      "640 1.161759761899301\n",
      "644 1.1620369779001394\n",
      "648 1.1616985695765458\n",
      "652 1.162154601296186\n",
      "656 1.1614819005719732\n",
      "660 1.1623056489028591\n",
      "664 1.1610764351294436\n",
      "668 1.162384288511135\n",
      "672 1.1615993817456738\n",
      "676 1.1617275592632845\n",
      "680 1.1617241901412123\n",
      "684 1.161511058240249\n",
      "688 1.162168139914374\n",
      "692 1.1617359362216946\n",
      "696 1.1617238939900514\n",
      "700 1.1614887926095123\n",
      "700 1.1614887926095123\n",
      "704 1.161764987382961\n",
      "708 1.1620373134873696\n",
      "712 1.1611411431505163\n",
      "716 1.1613800297451424\n",
      "720 1.1613852218175786\n",
      "724 1.1616510296004168\n",
      "728 1.1608608352232974\n",
      "732 1.1616796166900696\n",
      "736 1.1605325154747343\n",
      "740 1.1615859610435728\n",
      "744 1.1609546509716209\n",
      "748 1.1611718366260333\n",
      "752 1.1608639550380948\n",
      "756 1.161192134871876\n",
      "760 1.1609109731353924\n",
      "764 1.1609841460786534\n",
      "768 1.1612377787730952\n",
      "772 1.161146236341509\n",
      "776 1.1610223585344581\n",
      "780 1.1610471621583698\n",
      "784 1.161083044603887\n",
      "788 1.160288905914933\n",
      "792 1.1609426362272077\n",
      "796 1.1606888224016905\n",
      "800 1.161322175531474\n",
      "800 1.161322175531474\n",
      "804 1.1607881702002878\n",
      "808 1.161012505012701\n",
      "812 1.1607544181295992\n",
      "816 1.1605853408373386\n",
      "820 1.1609898951094877\n",
      "824 1.1609177476615529\n",
      "828 1.1607284612262283\n",
      "832 1.1604582554650873\n",
      "836 1.1608367339372534\n",
      "840 1.1605154848862742\n",
      "844 1.1603355980837573\n",
      "848 1.1601771532377385\n",
      "852 1.1604855345409366\n",
      "856 1.1601631891570667\n",
      "860 1.1596113443494047\n",
      "864 1.159909269503102\n",
      "868 1.1597870991461396\n",
      "872 1.1597783414944631\n",
      "876 1.1595325640428271\n",
      "880 1.1598743429270175\n",
      "884 1.1592254438446636\n",
      "888 1.1596731137059246\n",
      "892 1.158840025916418\n",
      "896 1.15975450950412\n",
      "900 1.158866233685113\n",
      "900 1.158866233685113\n",
      "904 1.1591083727442062\n",
      "908 1.158973380314291\n",
      "912 1.158848180430958\n",
      "916 1.1583845550396483\n",
      "920 1.1591456239320583\n",
      "924 1.1592795808124077\n",
      "928 1.1591537794192062\n",
      "932 1.1588024964331753\n",
      "936 1.1589168735502948\n",
      "940 1.1589238870118586\n",
      "944 1.1588385074038956\n",
      "948 1.1581067934652665\n",
      "952 1.1585178199488033\n",
      "956 1.1582596920174468\n",
      "960 1.1579281151429066\n",
      "964 1.1575392647276965\n",
      "968 1.1580847480560996\n",
      "972 1.1572925455656256\n",
      "976 1.1572995965767046\n",
      "980 1.157153575979034\n",
      "984 1.1567425970351193\n",
      "988 1.1565810635398568\n",
      "992 1.1565806261156149\n",
      "996 1.1560140711010543\n",
      "1000 1.156085956805904\n",
      "1000 1.156085956805904\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = cancer_model.train(Xtrain_cancer, ytrain_cancer, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals, filePath=\"breast-cancer/results/RMSProp\", title=\"RMSProp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
