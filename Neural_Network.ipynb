{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of features of the input.\n",
    "            output_size (int): Dimension of output.\n",
    "            hidden_layer_size (int): Number of neurons in the input layer.\n",
    "            activations (list): List of strings giving the activations for each layer.\n",
    "            loss (str): Loss function for the model.\n",
    "            optimizer (str): Optimization algorithm for the model.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X, w1=None, wn=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (tensor): Input for the model. \n",
    "            w1 (tensor): Weights to be used for the first layer. (Optional Argument)\n",
    "            wn (tensor): Weights to be used for the final layer. (Optional Argument)\n",
    "            \n",
    "        Returns:\n",
    "            z (list): List of outputs from linear function at each layer.\n",
    "            a (list): List of activation outputs from each layer.\n",
    "        \"\"\"\n",
    "        if w1 is None:\n",
    "            w1 = self.w1\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a, wn=None):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Input Data\n",
    "            y (Tensor): Output Data\n",
    "            z (list): List of outputs from linear layers.\n",
    "            a (list): List of actiation outputs.\n",
    "            wn (Tensor): Weights from final layer. (Optional Argument)\n",
    "        \"\"\"\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False, decay_rate=0.999, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        ypred = None\n",
    "        if self.optimizer == 'SGD':\n",
    "            if momentum_param != 0:\n",
    "                if nesterov:\n",
    "                    funVals ,ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param, True)\n",
    "                else:\n",
    "                    funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param)\n",
    "            else:\n",
    "                funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'Adagrad':\n",
    "            funVals, ypred = self.Adagrad(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            funVals, ypred = self.RMSProp(X, y, batch_size, iterations, alpha, decay_rate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            funVals, ypred = self.Adam(X, y, batch_size, iterations, alpha, beta1, beta2)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def SGD(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Gradient Descent Algorithm\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        vn = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                if nesterov:\n",
    "                    z, a = self.forward(X[i:i+batch_size], self.w1+momentum_param*v1, self.wn+momentum_param*vn)\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a, self.wn+momentum_param*vn)\n",
    "                else:\n",
    "                    z, a = self.forward(X[i:i+batch_size])\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                v1 = momentum_param * v1 - alpha * dW[0]\n",
    "                vn = momentum_param * vn - alpha * dW[1]\n",
    "                self.w1 = self.w1 + v1\n",
    "                self.wn = self.wn + vn\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adagrad(self, X, y, batch_size=100, iterations=500, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        AdaGrad Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 += dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1)+smoothing_param)) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2)+smoothing_param)) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def RMSProp(self, X, y, batch_size=100, iterations=500, alpha=1e-04, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        RMSProp Optimizer.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 = decay_rate*cache1 + (1 - decay_rate) * dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1+smoothing_param))) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2+smoothing_param))) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adam(self, X, y, batch_size=100, iterations=500, alpha=1e-04, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        m1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        m2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        v2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                m1 = beta1 * m1 + (1-beta1) * dW[0]\n",
    "                v1 = beta2 * v1 + (1-beta2) * dW[0]**2\n",
    "                m2 = beta1 * m2 + (1-beta1) * dW[1]\n",
    "                v2 = beta2 * v2 + (1-beta2) * dW[1]**2\n",
    "                self.w1 += -alpha*(m1/(1-beta1**n_iter))/(torch.sqrt((v1)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                self.wn += -alpha*(m2/(1-beta1**n_iter))/(torch.sqrt((v2)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "        \"\"\"\n",
    "        _, a = self.forward(X)\n",
    "        return a[-1]\n",
    "    \n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset Weights\n",
    "        \"\"\"\n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            def sigmoid(z):\n",
    "                s = torch.exp(z)\n",
    "                return s/(1+s)\n",
    "            return sigmoid\n",
    "#             return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        elif activation == 'tanh':\n",
    "            return lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>=0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            tanh = lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "            return lambda z : 1 - tanh(z)**2\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((ypred - y).T, (ypred - y))/(2*len(y))\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLoss(ypred, y):\n",
    "                ypredy1 = ypred[y==1]\n",
    "                ypredy1[ypredy1==0] = 1\n",
    "                loss = torch.sum(torch.log(ypred[y==1]+1e-05)) + torch.sum(torch.log(1 - ypred[y==0]+1e-05))\n",
    "                return -loss/y.shape[0]\n",
    "            return binaryCrossEntropyLoss\n",
    "#             return lambda ypred, y : (-1/len(y))*(torch.matmul(y.T, torch.log(ypred)) + torch.matmul((1-y).T, torch.log(1-ypred)))\n",
    "            return binaryCrossEntropyLoss\n",
    "        elif self.loss == \"CELoss\":\n",
    "            def crossEntropyLoss(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                prob = self.softmax(ypred)\n",
    "                log_likelihood = -torch.log(prob[range(m), y.long()])\n",
    "                loss = torch.sum(log_likelihood)\n",
    "                return loss/m\n",
    "            return crossEntropyLoss\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y: (ypred - y)/len(y)\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLossGradient(ypred, y):\n",
    "                gradient = 1/(ypred+1e-05) - 1/(1-ypred+1e-05)\n",
    "                return -gradient/y.shape[0]\n",
    "            return binaryCrossEntropyLossGradient\n",
    "#             return lambda ypred, y: (-1/len(y)) * ((y/ypred) - ((1-y)/(1-ypred)))\n",
    "        elif self.loss == 'CELoss':\n",
    "            def crossEntropyLossGradient(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                grad = self.softmax(ypred)\n",
    "                grad[range(m), y.long()] -= 1\n",
    "                return grad/m\n",
    "            return crossEntropyLossGradient\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exps = torch.exp(z - (torch.max(z, dim=1).values.reshape(-1,1)))\n",
    "        return exps/torch.sum(exps, dim=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(312, 20, dtype=torch.double)\n",
    "# # y = torch.randint(0, 2,(1000, 1)).double()\n",
    "# # y = torch.randn(312, 1, dtype=torch.double)\n",
    "# y = torch.randint(0,3,(312, 1)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(X.shape[1], 3, 32, ['tanh', 'linear'], 'CELoss', 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funVals, ypred = model.train(X, y, batch_size=100, iterations=1500, alpha=1e-03, momentum_param=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((torch.sum(ypred.argmax(dim=1).reshape(-1,1) == y.long()).float()*100.0)/(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # def plotLoss(funVals, filePath, title):\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "# plt.xlabel(\"Number of Iterations\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plotLoss(funVals, filePath, title, plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "#     plt.xkcd()\n",
    "    plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"./dataset/\"+filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAutoMPGDataset():\n",
    "    import pandas as pd\n",
    "    auto_mpg_dataset = pd.read_csv(\"./dataset/auto-mpg/auto-mpg.data\", header=-1, comment='\\t', skipinitialspace=True, na_values='?', sep=' ')\n",
    "    auto_mpg_dataset = auto_mpg_dataset.dropna()\n",
    "    origin = auto_mpg_dataset.pop(7)\n",
    "    auto_mpg_dataset[7] = (origin==1)*1.0\n",
    "    auto_mpg_dataset[8] = (origin==2)*1.0\n",
    "    auto_mpg_dataset[9] = (origin==3)*1.0\n",
    "    auto_dataset = torch.tensor(auto_mpg_dataset.values, dtype=torch.double)\n",
    "    return auto_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dataset = loadAutoMPGDataset()\n",
    "auto_dataset = auto_dataset[torch.randperm(auto_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = auto_dataset[:int(0.8 * auto_dataset.shape[0])]\n",
    "test = auto_dataset[int(0.8 * auto_dataset.shape[0]):]\n",
    "\n",
    "Xtrain = train[:, 1:]\n",
    "Xtrain = (Xtrain - Xtrain.mean(dim=0))/Xtrain.std(dim=0)\n",
    "ytrain = train[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest = test[:, 1:]\n",
    "Xtest = (Xtest - Xtest.mean(dim=0))/Xtest.std(dim=0)\n",
    "ytest = test[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model = NeuralNetwork(Xtrain.shape[1], ytrain.shape[1], 64, ['relu', 'relu'], 'MSE', 'RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model.reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 18.377537658182767\n",
      "200 10.65629269932579\n",
      "300 8.330532623642775\n",
      "400 7.26995060091635\n",
      "500 6.663414803793913\n",
      "600 6.28153355940342\n",
      "700 6.021717846632825\n",
      "800 5.796813981890418\n",
      "900 5.632449464598464\n",
      "1000 5.492211671672166\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = auto_mpg_model.train(Xtrain, ytrain, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals, filePath=\"auto-mpg/results/Adam.png\", title=\"Adam\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD sigmoid MSE\n",
      "100 3.3693205517558122\n",
      "200 3.6769616084264563\n",
      "300 4.009872670325126\n",
      "400 4.42380523258435\n",
      "500 4.757679906603839\n",
      "600 4.97175204096323\n",
      "700 5.135054294763755\n",
      "800 5.262538532618515\n",
      "900 5.3619610438973595\n",
      "1000 5.445106310631081\n",
      "SGD sigmoid MSE\n",
      "100 4.454727133814359\n",
      "200 4.163469654682137\n",
      "300 3.9913850449498285\n",
      "400 3.8495178391677074\n",
      "500 3.7396584105366806\n",
      "600 3.666955762615001\n",
      "700 3.6281683716287283\n",
      "800 3.6215895879741096\n",
      "900 3.6377223064506343\n",
      "1000 3.667210044250004\n",
      "SGD sigmoid MSE\n",
      "100 6.746014561364573\n",
      "200 5.57907932293608\n",
      "300 5.061599373783363\n",
      "400 4.787818847710687\n",
      "500 4.618995318961563\n",
      "600 4.50182882209024\n",
      "700 4.413754917692977\n",
      "800 4.34412777445424\n",
      "900 4.286982886112054\n",
      "1000 4.238607141469911\n",
      "SGD sigmoid MSE\n",
      "100 86.43156786523902\n",
      "200 18.47046093289326\n",
      "300 10.158718141603563\n",
      "400 8.095320919262077\n",
      "500 7.227422973524728\n",
      "600 6.751329859172108\n",
      "700 6.455526380657801\n",
      "800 6.255450530972088\n",
      "900 6.109984148957471\n",
      "1000 5.997304843487533\n",
      "SGD sigmoid MSE\n",
      "100 180.6532826263683\n",
      "200 136.19861944042808\n",
      "300 103.37229409874524\n",
      "400 79.20150178088966\n",
      "500 61.363210036642734\n",
      "600 48.166771064579684\n",
      "700 38.37834964639568\n",
      "800 31.096213427606617\n",
      "900 25.6598530302342\n",
      "1000 21.58484069661475\n",
      "SGD relu MSE\n",
      "100 303.79222044728454\n",
      "200 303.79222044728454\n",
      "300 303.79222044728454\n",
      "400 303.79222044728454\n",
      "500 303.79222044728454\n",
      "600 303.79222044728454\n",
      "700 303.79222044728454\n",
      "800 303.79222044728454\n",
      "900 303.79222044728454\n",
      "1000 303.79222044728454\n",
      "SGD relu MSE\n",
      "100 3.584177659576949\n",
      "200 3.750783603546417\n",
      "300 3.9667554940416365\n",
      "400 4.112269190317697\n",
      "500 4.319373336946812\n",
      "600 4.548222272855816\n",
      "700 4.726516108229148\n",
      "800 4.867811666340111\n",
      "900 4.981864598635617\n",
      "1000 5.068648143485831\n",
      "SGD relu MSE\n",
      "100 6.510879564920971\n",
      "200 5.212213303852741\n",
      "300 4.929183873984736\n",
      "400 4.76738364752739\n",
      "500 4.6965539246093\n",
      "600 4.656695191376026\n",
      "700 4.642245181116651\n",
      "800 4.628380505542513\n",
      "900 4.614357561845629\n",
      "1000 4.649692462776218\n",
      "SGD relu MSE\n",
      "100 11.570019525505408\n",
      "200 8.83076073712452\n",
      "300 7.643767407325472\n",
      "400 6.99900887637851\n",
      "500 6.592080698797916\n",
      "600 6.305771262480166\n",
      "700 6.097293065024036\n",
      "800 5.9335516110612145\n",
      "900 5.79321387125509\n",
      "1000 5.673306878471675\n",
      "SGD relu MSE\n",
      "100 256.1912897020016\n",
      "200 244.39005151954342\n",
      "300 213.8530197947753\n",
      "400 138.5094739245286\n",
      "500 57.274613454584525\n",
      "600 34.69322437145341\n",
      "700 25.68004228591004\n",
      "800 20.671836872895057\n",
      "900 17.440132156269673\n",
      "1000 15.192289602877139\n",
      "SGD tanh MSE\n",
      "100 102.26025351036753\n",
      "200 102.44623638161222\n",
      "300 102.61963561168162\n",
      "400 102.69213296711449\n",
      "500 102.7088447059184\n",
      "600 102.71176472552504\n",
      "700 102.70762231316313\n",
      "800 102.6866159583218\n",
      "900 102.61244024007686\n",
      "1000 102.57359781054988\n",
      "SGD tanh MSE\n",
      "100 172.3582815272851\n",
      "200 172.30194916377636\n",
      "300 172.27748328079645\n",
      "400 172.26764734485525\n",
      "500 172.29116932121124\n",
      "600 172.3429914591868\n",
      "700 172.4014298645727\n",
      "800 172.4536336756487\n",
      "900 172.49684189615584\n",
      "1000 172.53186942626914\n",
      "SGD tanh MSE\n",
      "100 70.61693180279003\n",
      "200 67.5180836440976\n",
      "300 65.03489990688053\n",
      "400 63.90645081995731\n",
      "500 63.266505529545896\n",
      "600 62.911849438267836\n",
      "700 62.648921995152435\n",
      "800 62.44211389231443\n",
      "900 62.25068424600891\n",
      "1000 62.08899899696803\n",
      "SGD tanh MSE\n",
      "100 265.9003557970164\n",
      "200 110.51911526742259\n",
      "300 83.08916650158723\n",
      "400 78.41000056849926\n",
      "500 76.58675397156085\n",
      "600 75.73105950179873\n",
      "700 75.2220630295383\n",
      "800 74.75427442335547\n",
      "900 73.77554422403095\n",
      "1000 73.3119748434902\n",
      "SGD tanh MSE\n",
      "100 207.72353440639566\n",
      "200 193.92969894559724\n",
      "300 178.97388314229062\n",
      "400 165.95955521909482\n",
      "500 153.94733863728325\n",
      "600 142.70658735285448\n",
      "700 131.15853512399556\n",
      "800 120.82423496023303\n",
      "900 111.53140761602229\n",
      "1000 102.89456462569443\n",
      "SGDNesterov sigmoid MSE\n",
      "100 303.79222044728454\n",
      "200 303.79222044728454\n",
      "300 303.79222044728454\n",
      "400 303.79222044728454\n",
      "500 303.79222044728454\n",
      "600 303.79222044728454\n",
      "700 303.79222044728454\n",
      "800 303.79222044728454\n",
      "900 303.79222044728454\n",
      "1000 303.79222044728454\n",
      "SGDNesterov sigmoid MSE\n",
      "100 4.724611582965705\n",
      "200 4.043999754374452\n",
      "300 3.865330072177009\n",
      "400 3.755967992420017\n",
      "500 3.6706098756185193\n",
      "600 3.6240254016767985\n",
      "700 3.6127463409625284\n",
      "800 3.6241932199049893\n",
      "900 3.651578022175312\n",
      "1000 3.6900982351767766\n",
      "SGDNesterov sigmoid MSE\n",
      "100 6.784337958486924\n",
      "200 5.832553324863348\n",
      "300 5.290316080846342\n",
      "400 4.965738078396655\n",
      "500 4.757573217853821\n",
      "600 4.614529876462466\n",
      "700 4.510440331682297\n",
      "800 4.430949030304886\n",
      "900 4.3670370208066505\n",
      "1000 4.31273495704988\n",
      "SGDNesterov sigmoid MSE\n",
      "100 33.25796408760649\n",
      "200 11.993537543038277\n",
      "300 8.873222116014356\n",
      "400 7.774953827435361\n",
      "500 7.199411435136304\n",
      "600 6.844908632673312\n",
      "700 6.604097367572844\n",
      "800 6.427114438691462\n",
      "900 6.288184501019347\n",
      "1000 6.173228161379977\n",
      "SGDNesterov sigmoid MSE\n",
      "100 272.2238110510155\n",
      "200 258.5709184463004\n",
      "300 240.95143248667875\n",
      "400 222.41526530224237\n",
      "500 194.57879588765908\n",
      "600 154.8832499645759\n",
      "700 121.1985137409429\n",
      "800 95.82835171574212\n",
      "900 76.6954644676072\n",
      "1000 62.19228141811887\n",
      "SGDNesterov relu MSE\n",
      "100 303.79222044728454\n",
      "200 303.79222044728454\n",
      "300 303.79222044728454\n",
      "400 303.79222044728454\n",
      "500 303.79222044728454\n",
      "600 303.79222044728454\n",
      "700 303.79222044728454\n",
      "800 303.79222044728454\n",
      "900 303.79222044728454\n",
      "1000 303.79222044728454\n",
      "SGDNesterov relu MSE\n",
      "100 99.59696927049745\n",
      "200 99.54711976166072\n",
      "300 99.54857697603136\n",
      "400 99.5179707252841\n",
      "500 99.59103650047554\n",
      "600 99.59408054034631\n",
      "700 99.61800852109347\n",
      "800 99.64483713082367\n",
      "900 99.65648152635437\n",
      "1000 99.63406104707687\n",
      "SGDNesterov relu MSE\n",
      "100 4.1532050214044265\n",
      "200 3.586196623122414\n",
      "300 3.4359898649769565\n",
      "400 3.3499621346107986\n",
      "500 3.3084221462163033\n",
      "600 3.2725897346995145\n",
      "700 3.2377516577890795\n",
      "800 3.212928276133724\n",
      "900 3.196750652182776\n",
      "1000 3.1798058313952784\n",
      "SGDNesterov relu MSE\n",
      "100 32.329993095049524\n",
      "200 11.464298272748303\n",
      "300 8.273345159563524\n",
      "400 6.877308196805294\n",
      "500 6.1167338610267\n",
      "600 5.65397122918084\n",
      "700 5.348799430163503\n",
      "800 5.147578147685058\n",
      "900 5.008602191770541\n",
      "1000 4.896988857359168\n",
      "SGDNesterov relu MSE\n",
      "100 83.57220656379194\n",
      "200 29.97794740883305\n",
      "300 18.41188048104843\n",
      "400 14.575091974315274\n",
      "500 12.851862226845984\n",
      "600 11.89827678333046\n",
      "700 11.275790998566173\n",
      "800 10.819780220489445\n",
      "900 10.459454321947382\n",
      "1000 10.158930444435333\n",
      "SGDNesterov tanh MSE\n",
      "100 73.99668571934502\n",
      "200 46.933450519305836\n",
      "300 47.87761357313701\n",
      "400 47.76423667337717\n",
      "500 47.74464599614688\n",
      "600 47.69158871829606\n",
      "700 47.57690049707377\n",
      "800 47.4981019740139\n",
      "900 47.46359994919957\n",
      "1000 47.46365791080023\n",
      "SGDNesterov tanh MSE\n",
      "100 99.68423145849948\n",
      "200 99.48552634292218\n",
      "300 99.54901751266705\n",
      "400 99.71933043503631\n",
      "500 99.90389836535641\n",
      "600 100.07356382609723\n",
      "700 100.22467433426694\n",
      "800 100.35784123734932\n",
      "900 100.47528475953652\n",
      "1000 100.57624722344107\n",
      "SGDNesterov tanh MSE\n",
      "100 78.91402381506062\n",
      "200 77.78627950392462\n",
      "300 77.57850086622733\n",
      "400 77.47794905000822\n",
      "500 77.41535847188115\n",
      "600 77.37133104880405\n",
      "700 77.3396283772631\n",
      "800 77.318726040937\n",
      "900 77.30681773264683\n",
      "1000 77.30204822119714\n",
      "SGDNesterov tanh MSE\n",
      "100 99.03053446836734\n",
      "200 78.23529073384536\n",
      "300 73.26955865890177\n",
      "400 71.68479128127169\n",
      "500 71.10302617433439\n",
      "600 70.5189777465745\n",
      "700 69.65277889729562\n",
      "800 68.84506023290608\n",
      "900 68.09040109452441\n",
      "1000 67.8757918391383\n",
      "SGDNesterov tanh MSE\n",
      "100 254.09553840798114\n",
      "200 251.53213802271327\n",
      "300 249.8275805554628\n",
      "400 248.61089527984913\n",
      "500 247.53618089653716\n",
      "600 246.53120722508862\n",
      "700 245.59816538633902\n",
      "800 244.70134681498533\n",
      "900 243.81807314627395\n",
      "1000 242.9554371004737\n",
      "Adagrad sigmoid MSE\n",
      "100 5.0402409967044495\n",
      "200 4.606613419386451\n",
      "300 4.384546878597037\n",
      "400 4.2130217683944915\n",
      "500 4.090342021646411\n",
      "600 4.001268158390073\n",
      "700 3.9356099283171893\n",
      "800 3.8902613358235447\n",
      "900 3.8590581721555783\n",
      "1000 3.8395105205007516\n",
      "Adagrad sigmoid MSE\n",
      "100 168.39068811398127\n",
      "200 114.84042027009197\n",
      "300 85.12246313792708\n",
      "400 65.6863636355654\n",
      "500 52.06106495292468\n",
      "600 42.10480874041058\n",
      "700 34.626104135419936\n",
      "800 28.90586202925647\n",
      "900 24.47402883973442\n",
      "1000 21.007572354600647\n",
      "Adagrad sigmoid MSE\n",
      "100 158.84420439464557\n",
      "200 152.82118242301829\n",
      "300 148.29919536430052\n",
      "400 144.5534705040888\n",
      "500 141.305232970919\n",
      "600 138.41066207892268\n",
      "700 135.782903223565\n",
      "800 133.36742469010073\n",
      "900 131.12561240401973\n",
      "1000 129.028179584857\n",
      "Adagrad sigmoid MSE\n",
      "100 303.03186726193275\n",
      "200 302.9949261155259\n",
      "300 302.96664146955743\n",
      "400 302.94284234260346\n",
      "500 302.92190980894276\n",
      "600 302.9030134792537\n",
      "700 302.8856607347974\n",
      "800 302.86953118027884\n",
      "900 302.8544024329799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 302.84011245065295\n",
      "Adagrad sigmoid MSE\n",
      "100 227.10262260716436\n",
      "200 227.0288666162302\n",
      "300 226.9722518495576\n",
      "400 226.92449986188169\n",
      "500 226.8824350408854\n",
      "600 226.84441515968103\n",
      "700 226.80944155400573\n",
      "800 226.7768955288982\n",
      "900 226.74633658455053\n",
      "1000 226.717424884028\n",
      "Adagrad relu MSE\n",
      "100 99.41130393848319\n",
      "200 99.37997542483441\n",
      "300 99.33913887246047\n",
      "400 4.416287383208021\n",
      "500 4.284600892183266\n",
      "600 4.223134897411051\n",
      "700 4.2278886061734395\n",
      "800 4.237683323960286\n",
      "900 4.271214233982407\n",
      "1000 4.308885883600897\n",
      "Adagrad relu MSE\n",
      "100 68.85638119238222\n",
      "200 27.48151590824813\n",
      "300 16.706303333048997\n",
      "400 12.562418608741988\n",
      "500 10.444943582539542\n",
      "600 9.138237844831467\n",
      "700 8.272339716008595\n",
      "800 7.643389178865586\n",
      "900 7.164047676589086\n",
      "1000 6.79477277438481\n",
      "Adagrad relu MSE\n",
      "100 253.90785667143157\n",
      "200 248.42176268016289\n",
      "300 243.8266374018269\n",
      "400 239.18218776527598\n",
      "500 234.21005559761093\n",
      "600 228.8116546829418\n",
      "700 223.01073903568994\n",
      "800 217.26263166420017\n",
      "900 211.89832277231403\n",
      "1000 207.3975600850938\n",
      "Adagrad relu MSE\n",
      "100 221.32325750391178\n",
      "200 220.25833577166895\n",
      "300 219.43952357091376\n",
      "400 218.74546506575393\n",
      "500 218.13433575985673\n",
      "600 217.57826284150673\n",
      "700 217.06585514590975\n",
      "800 216.59112964028304\n",
      "900 216.1474981132907\n",
      "1000 215.72891547240047\n",
      "Adagrad relu MSE\n",
      "100 271.6458918229541\n",
      "200 271.581010340358\n",
      "300 271.53090267260995\n",
      "400 271.48868209480344\n",
      "500 271.4514852734712\n",
      "600 271.41786727922675\n",
      "700 271.3869701299502\n",
      "800 271.3582117313499\n",
      "900 271.33120884954303\n",
      "1000 271.30568281738886\n",
      "Adagrad tanh MSE\n",
      "100 62.54385782756936\n",
      "200 59.76228190829631\n",
      "300 44.22043002563219\n",
      "400 38.95062360756323\n",
      "500 37.407011404592325\n",
      "600 36.608358568152504\n",
      "700 36.15064725263095\n",
      "800 35.828576427370145\n",
      "900 35.614830528992854\n",
      "1000 35.49853566846541\n",
      "Adagrad tanh MSE\n",
      "100 162.30748930897616\n",
      "200 140.31674526040928\n",
      "300 128.33910417050237\n",
      "400 119.88328540049413\n",
      "500 113.90364905271139\n",
      "600 109.3304273164048\n",
      "700 104.69859715927498\n",
      "800 100.66899409846643\n",
      "900 96.54415369999917\n",
      "1000 93.3225787904661\n",
      "Adagrad tanh MSE\n",
      "100 199.40422263971644\n",
      "200 191.9643981198206\n",
      "300 186.46882388827385\n",
      "400 181.89324359043462\n",
      "500 177.97909741164798\n",
      "600 174.51335029625872\n",
      "700 171.41931677527418\n",
      "800 168.6083663947248\n",
      "900 166.0240953532175\n",
      "1000 163.64074703497272\n",
      "Adagrad tanh MSE\n",
      "100 247.05858686908923\n",
      "200 246.40231074547998\n",
      "300 245.89749945041288\n",
      "400 245.46791630607615\n",
      "500 245.09076979919044\n",
      "600 244.75097809204874\n",
      "700 244.43799141399626\n",
      "800 244.1474986749402\n",
      "900 243.87544823803313\n",
      "1000 243.61835377528413\n",
      "Adagrad tanh MSE\n",
      "100 242.13025984761381\n",
      "200 242.05246830862518\n",
      "300 241.9927893732017\n",
      "400 241.94245358912636\n",
      "500 241.8981223550919\n",
      "600 241.8580722430282\n",
      "700 241.82122899246852\n",
      "800 241.78694847614642\n",
      "900 241.75477424850382\n",
      "1000 241.72433161866496\n",
      "RMSProp sigmoid MSE\n",
      "100 4.705302498646049\n",
      "200 4.346760844114686\n",
      "300 4.427628064471549\n",
      "400 4.492330605136493\n",
      "500 4.597502055332028\n",
      "600 4.7029214468826375\n",
      "700 4.79332750462748\n",
      "800 4.878999787216844\n",
      "900 4.889271103647089\n",
      "1000 4.899603576672505\n",
      "RMSProp sigmoid MSE\n",
      "100 303.79222044728454\n",
      "200 303.79222044728454\n",
      "300 303.79222044728454\n",
      "400 303.79222044728454\n",
      "500 303.79222044728454\n",
      "600 303.79222044728454\n",
      "700 303.79222044728454\n",
      "800 303.79222044728454\n",
      "900 303.79222044728454\n",
      "1000 303.79222044728454\n",
      "RMSProp sigmoid MSE\n",
      "100 127.5418700509231\n",
      "200 106.5700623057024\n",
      "300 97.29313043093093\n",
      "400 92.27827485073759\n",
      "500 88.82515943859136\n",
      "600 85.78091640650449\n",
      "700 83.2058806404486\n",
      "800 81.25634952980526\n",
      "900 79.78698182044162\n",
      "1000 78.65864301552809\n",
      "RMSProp sigmoid MSE\n",
      "100 252.7203866367001\n",
      "200 241.31291975748408\n",
      "300 231.54067112834258\n",
      "400 223.94747248922738\n",
      "500 217.5779447907273\n",
      "600 212.0580729588237\n",
      "700 206.96943507933847\n",
      "800 202.27576646303595\n",
      "900 198.0011006708566\n",
      "1000 194.02456012018598\n",
      "RMSProp sigmoid MSE\n",
      "100 250.7961116562336\n",
      "200 250.0003047089371\n",
      "300 249.38553239275444\n",
      "400 248.85454236104377\n",
      "500 248.36685034319012\n",
      "600 247.91373120861735\n",
      "700 247.49146888895044\n",
      "800 247.08895456645857\n",
      "900 246.7018028606447\n",
      "1000 246.32653621065677\n",
      "RMSProp relu MSE\n",
      "100 274.69597669531737\n",
      "200 264.93680210299965\n",
      "300 262.43333284487926\n",
      "400 261.5422231288911\n",
      "500 261.0913303488589\n",
      "600 260.76008843009043\n",
      "700 260.622572713199\n",
      "800 260.47455617791314\n",
      "900 260.3810632090688\n",
      "1000 260.3448350329802\n",
      "RMSProp relu MSE\n",
      "100 5.3221189789226155\n",
      "200 4.4956116298234035\n",
      "300 4.126967944945598\n",
      "400 3.9805607538886174\n",
      "500 3.8878772616811434\n",
      "600 4.0293037914702525\n",
      "700 3.883168377040734\n",
      "800 3.9121414200728224\n",
      "900 3.9789263494083906\n",
      "1000 3.964845232468485\n",
      "RMSProp relu MSE\n",
      "100 15.248128409258795\n",
      "200 10.501153527718227\n",
      "300 8.735597849581183\n",
      "400 7.869056176837441\n",
      "500 7.1522468020745364\n",
      "600 5.536048026113809\n",
      "700 5.045876181135038\n",
      "800 4.864526411550305\n",
      "900 4.781833602805287\n",
      "1000 4.7306331527218\n",
      "RMSProp relu MSE\n",
      "100 99.49980051559052\n",
      "200 85.8541064456373\n",
      "300 77.24764284227794\n",
      "400 71.18680540196344\n",
      "500 66.50468285394354\n",
      "600 62.790450504094224\n",
      "700 59.71602218462633\n",
      "800 57.10962669948731\n",
      "900 54.81280839182646\n",
      "1000 52.73773402671519\n",
      "RMSProp relu MSE\n",
      "100 226.49161850219795\n",
      "200 223.79357575092436\n",
      "300 221.43638851155262\n",
      "400 219.26821365871507\n",
      "500 217.2766674118011\n",
      "600 215.50161004166856\n",
      "700 213.78733932380212\n",
      "800 212.2130776433564\n",
      "900 210.73358633056583\n",
      "1000 209.32716953222825\n",
      "RMSProp tanh MSE\n",
      "100 63.87218621930462\n",
      "200 64.01923372160107\n",
      "300 63.65505159797428\n",
      "400 63.42945798912332\n",
      "500 63.147844763118805\n",
      "600 62.978808680152646\n",
      "700 62.97429049536711\n",
      "800 63.11624169631817\n",
      "900 63.26331654978179\n",
      "1000 63.32197099065967\n",
      "RMSProp tanh MSE\n",
      "100 53.097207366853105\n",
      "200 50.55283318636614\n",
      "300 49.05714463328187\n",
      "400 48.4966598343315\n",
      "500 47.923035891038715\n",
      "600 46.963913252198736\n",
      "700 46.31281461382084\n",
      "800 46.18096087551554\n",
      "900 46.089199515174556\n",
      "1000 46.04728938426368\n",
      "RMSProp tanh MSE\n",
      "100 71.46863065191795\n",
      "200 68.76575315319161\n",
      "300 68.04191569645837\n",
      "400 67.12118432516387\n",
      "500 66.37735263062957\n",
      "600 66.430719288062\n",
      "700 65.90702473653734\n",
      "800 65.48376243279291\n",
      "900 65.0828873878206\n",
      "1000 64.77411438059568\n",
      "RMSProp tanh MSE\n",
      "100 243.0058569910237\n",
      "200 233.39791780909238\n",
      "300 226.53245569809144\n",
      "400 221.04018472490714\n",
      "500 216.55558371604477\n",
      "600 212.76267405502173\n",
      "700 209.48443808058977\n",
      "800 206.61696916553424\n",
      "900 204.07948736423373\n",
      "1000 201.81211809015687\n",
      "RMSProp tanh MSE\n",
      "100 194.0237131972837\n",
      "200 192.34164429011116\n",
      "300 191.01399552170608\n",
      "400 189.86528832218818\n",
      "500 188.84266566578816\n",
      "600 187.90723760887258\n",
      "700 187.03512845538464\n",
      "800 186.2126924310023\n",
      "900 185.43035189809353\n",
      "1000 184.67991119310034\n",
      "Adam sigmoid MSE\n",
      "100 3.9928761936082515\n",
      "200 3.7294518029196766\n",
      "300 3.7057093657994\n",
      "400 3.8469470019949448\n",
      "500 3.95924504226073\n",
      "600 4.038293712074393\n",
      "700 4.185809048517239\n",
      "800 4.377727190193469\n",
      "900 4.544357068134868\n",
      "1000 4.596521879840431\n",
      "Adam sigmoid MSE\n",
      "100 10.109759469279457\n",
      "200 4.918672697499211\n",
      "300 4.280674811260949\n",
      "400 4.076567428369852\n",
      "500 3.960222572006918\n",
      "600 3.853558740658489\n",
      "700 3.7617630727117803\n",
      "800 3.6844214402342756\n",
      "900 3.6199465061617686\n",
      "1000 3.5633455428502323\n",
      "Adam sigmoid MSE\n",
      "100 271.9601686797848\n",
      "200 250.24547899008797\n",
      "300 214.30485741741714\n",
      "400 108.2671614411374\n",
      "500 44.21436583299761\n",
      "600 21.687068417505426\n",
      "700 12.771113560649798\n",
      "800 9.156672480073713\n",
      "900 7.627533952447946\n",
      "1000 6.902967708362461\n",
      "Adam sigmoid MSE\n",
      "100 202.87352375730103\n",
      "200 195.79081227339293\n",
      "300 188.5417037685894\n",
      "400 180.53411077486902\n",
      "500 172.18824062515475\n",
      "600 162.5803001502535\n",
      "700 153.15788932543398\n",
      "800 144.44248600231364\n",
      "900 136.28406600852242\n",
      "1000 128.59670779026746\n",
      "Adam sigmoid MSE\n",
      "100 294.86031170838294\n",
      "200 294.50871882349395\n",
      "300 294.15528233389773\n",
      "400 293.7760044275192\n",
      "500 293.3769810780856\n",
      "600 292.9782296012733\n",
      "700 292.53934450726194\n",
      "800 292.1015742407521\n",
      "900 291.64834552792894\n",
      "1000 291.1761923403061\n",
      "Adam relu MSE\n",
      "100 3.6992470956946617\n",
      "200 3.9580692029662603\n",
      "300 4.210963928289429\n",
      "400 4.971224045921233\n",
      "500 5.001536878095408\n",
      "600 5.308943012156418\n",
      "700 5.2475054269630945\n",
      "800 5.541624908850295\n",
      "900 6.006910155947249\n",
      "1000 5.6948393736816305\n",
      "Adam relu MSE\n",
      "100 5.445027382542402\n",
      "200 4.383689678726512\n",
      "300 4.008618555704713\n",
      "400 3.931376463195478\n",
      "500 3.8670311325912032\n",
      "600 3.863263294642749\n",
      "700 3.913844421722596\n",
      "800 3.9452716035137434\n",
      "900 4.015120063386067\n",
      "1000 4.107316389570593\n",
      "Adam relu MSE\n",
      "100 257.2558168509014\n",
      "200 148.14923384042874\n",
      "300 20.028134547417203\n",
      "400 11.875683690465534\n",
      "500 8.8852676078012\n",
      "600 7.49011500582458\n",
      "700 6.73034775927269\n",
      "800 6.21968324031375\n",
      "900 5.833620095317884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 5.515523327170104\n",
      "Adam relu MSE\n",
      "100 241.2621594744519\n",
      "200 210.20399066403704\n",
      "300 182.30835798500075\n",
      "400 158.18984774011375\n",
      "500 137.98786016369024\n",
      "600 121.04783467204747\n",
      "700 106.8088756926591\n",
      "800 94.77011221079205\n",
      "900 84.56407090560272\n",
      "1000 75.90113051924891\n",
      "Adam relu MSE\n",
      "100 292.01629150134505\n",
      "200 291.4985693892488\n",
      "300 290.98602106053556\n",
      "400 290.46312435541563\n",
      "500 289.94486534857396\n",
      "600 289.42982131229667\n",
      "700 288.89233800125015\n",
      "800 288.34940390658784\n",
      "900 287.7079990986829\n",
      "1000 287.0123907950854\n",
      "Adam tanh MSE\n",
      "100 63.72990750635282\n",
      "200 63.782274526262256\n",
      "300 64.00478684455605\n",
      "400 64.04083732191266\n",
      "500 64.05759454538374\n",
      "600 64.06489549107053\n",
      "700 64.06763159112573\n",
      "800 64.06862371179832\n",
      "900 64.06892804201864\n",
      "1000 64.06900867535293\n",
      "Adam tanh MSE\n",
      "100 67.40848572729405\n",
      "200 62.01606830330289\n",
      "300 58.90470225920762\n",
      "400 40.91995905584878\n",
      "500 36.47432004370848\n",
      "600 35.13122652235826\n",
      "700 34.3888265107496\n",
      "800 33.96801635493816\n",
      "900 33.7445917018635\n",
      "1000 33.710671604514836\n",
      "Adam tanh MSE\n",
      "100 243.34710171636104\n",
      "200 209.25392572306419\n",
      "300 189.9679277183136\n",
      "400 182.44777213695164\n",
      "500 179.12898167366572\n",
      "600 177.41634868123865\n",
      "700 176.44715371045723\n",
      "800 175.8358777719434\n",
      "900 175.4112751602897\n",
      "1000 175.09334154623426\n",
      "Adam tanh MSE\n",
      "100 225.59610911992982\n",
      "200 211.5760354805321\n",
      "300 198.60423094471082\n",
      "400 186.9104471754742\n",
      "500 176.26419322675645\n",
      "600 166.3553835807415\n",
      "700 157.1280406400527\n",
      "800 148.6548007200268\n",
      "900 140.89292446303864\n",
      "1000 133.7055553975811\n",
      "Adam tanh MSE\n",
      "100 245.46854535360566\n",
      "200 244.60163456970284\n",
      "300 243.73390199075993\n",
      "400 242.87527659872924\n",
      "500 242.02483669242181\n",
      "600 241.1769565011591\n",
      "700 240.33349366607476\n",
      "800 239.49566784497432\n",
      "900 238.6531615758983\n",
      "1000 237.81503453522927\n"
     ]
    }
   ],
   "source": [
    "optims=['SGD','SGDNesterov','Adagrad','RMSProp','Adam']\n",
    "# optims=['RMSProp']\n",
    "lrs=[1e-1,1e-2,1e-3,1e-4,1e-5]\n",
    "activs=['sigmoid','relu','tanh']\n",
    "losses=['MSE']#,'BCELoss','CE']\n",
    "fout = open(\"results.csv\",'w')\n",
    "fout.write(\"iter, lr, optim, activ, loss, funVal \\n\")\n",
    "for optim in optims:\n",
    "    nesterov=False\n",
    "    optimfunc=optim\n",
    "    if optim=='SGDNesterov':\n",
    "        nesterov=True\n",
    "        optimfunc='SGD'\n",
    "    for activ in activs:\n",
    "        for loss in losses:\n",
    "            for lr in lrs:\n",
    "                print(optim, activ, loss)\n",
    "                auto_mpg_model = NeuralNetwork(Xtrain.shape[1], ytrain.shape[1], 64, [activ, 'relu'], loss, optimfunc)\n",
    "                funVals, ypred = auto_mpg_model.train(Xtrain, ytrain, batch_size=100, iterations=1000, alpha=lr,momentum_param=0.9, nesterov=nesterov)\n",
    "                auto_mpg_model.reset_weights()\n",
    "                i=0\n",
    "                for val in funVals:\n",
    "                    fout.write(str(i)+','+str(lr)+','+optim+','+activ+','+loss+','+str(val)+'\\n')\n",
    "                    i+=1\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBreastCancerDataset():\n",
    "    cancer_dataset = pd.read_csv(\"./dataset/breast-cancer/data.csv\")\n",
    "    cancer_dataset.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "    cancer_dataset['diagnosis'] = cancer_dataset['diagnosis'].astype('category').cat.codes\n",
    "    cancer_dataset = torch.tensor(cancer_dataset.values, dtype=torch.double)\n",
    "    return cancer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = loadBreastCancerDataset()\n",
    "cancer_dataset = cancer_dataset[torch.randperm(cancer_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cancer = cancer_dataset[:int(0.8*cancer_dataset.shape[0])]\n",
    "test_cancer = cancer_dataset[int(0.8*cancer_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_cancer = train_cancer[:, 1:]\n",
    "Xtrain_cancer = (Xtrain_cancer-Xtrain_cancer.mean(dim=0))/Xtrain_cancer.std(dim=0)\n",
    "ytrain_cancer = train_cancer[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_cancer = test_cancer[:, 1:]\n",
    "Xtest_cancer = (Xtest_cancer-Xtest_cancer.mean(dim=0))/Xtest_cancer.std(dim=0)\n",
    "ytest_cancer = test_cancer[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_model = NeuralNetwork(Xtrain_cancer.shape[1], 1, 16, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.2431538561789015\n",
      "200 2.5915738544670153\n",
      "300 2.091879641153153\n",
      "400 1.6593842198778763\n",
      "500 1.2540401399043681\n",
      "600 0.987342452302461\n",
      "700 0.9122081785957932\n",
      "800 0.8920260193457875\n",
      "900 0.8825577391496079\n",
      "1000 0.8694058036380359\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = cancer_model.train(Xtrain_cancer, ytrain_cancer, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD sigmoid BCELoss\n",
      "100 0.8316656463112739\n",
      "200 0.8332218640686716\n",
      "300 0.8352431358286904\n",
      "400 0.8366064758247443\n",
      "500 0.8374681473470204\n",
      "600 0.8380072992893082\n",
      "700 0.8383447302112648\n",
      "800 0.8385563494753828\n",
      "900 0.8386893564493099\n",
      "1000 0.838773119594002\n",
      "SGD sigmoid BCELoss\n",
      "100 0.8936708321163126\n",
      "200 0.8663111391581861\n",
      "300 0.8598463790978972\n",
      "400 0.856162374971241\n",
      "500 0.8536032645370764\n",
      "600 0.851534774178106\n",
      "700 0.849718242697239\n",
      "800 0.8480882567874928\n",
      "900 0.8466322397690604\n",
      "1000 0.8453447729509672\n",
      "SGD sigmoid BCELoss\n",
      "100 1.1853409145252551\n",
      "200 1.1116661168078024\n",
      "300 1.0632022414944167\n",
      "400 1.0329427498067747\n",
      "500 1.0121945729779722\n",
      "600 0.9967497930932071\n",
      "700 0.9846247179218806\n",
      "800 0.9747533899375584\n",
      "900 0.9665017854421736\n",
      "1000 0.9594732205972353\n",
      "SGD sigmoid BCELoss\n",
      "100 4.304593910980372\n",
      "200 3.50513112845881\n",
      "300 2.731529139105312\n",
      "400 2.132675327750523\n",
      "500 1.718519139662812\n",
      "600 1.4507357098759266\n",
      "700 1.2818630755743181\n",
      "800 1.1747631582055156\n",
      "900 1.1052738945124518\n",
      "1000 1.0587333043314764\n",
      "SGD sigmoid BCELoss\n",
      "100 1.229812725155981\n",
      "200 1.2006631568949064\n",
      "300 1.1740524240367534\n",
      "400 1.1498283099882882\n",
      "500 1.1278409407604284\n",
      "600 1.107943405263643\n",
      "700 1.0899922563496733\n",
      "800 1.0738480196618407\n",
      "900 1.0593757195008549\n",
      "1000 1.0464454037971245\n",
      "SGD relu BCELoss\n",
      "100 1.27307620389698\n",
      "200 1.0470118017708177\n",
      "300 0.6930430138071977\n",
      "400 0.6937622197193746\n",
      "500 0.6936948626881118\n",
      "600 0.6935679543528976\n",
      "700 0.6934424619265633\n",
      "800 0.6933329219755113\n",
      "900 0.6932411647777001\n",
      "1000 0.6931653640737383\n",
      "SGD relu BCELoss\n",
      "100 4.369866249217329\n",
      "200 2.827062780219835\n",
      "300 2.416523099052902\n",
      "400 2.044990011637125\n",
      "500 2.007569886810838\n",
      "600 2.0174117288306923\n",
      "700 2.0477379767632016\n",
      "800 2.0789742654152312\n",
      "900 2.0766359532230627\n",
      "1000 2.0508351842517887\n",
      "SGD relu BCELoss\n",
      "100 3.18464988660136\n",
      "200 3.214281265784985\n",
      "300 3.2536334233004687\n",
      "400 3.1989526416001723\n",
      "500 3.1280950946954778\n",
      "600 2.8416273020352247\n",
      "700 2.6192880751606697\n",
      "800 2.501934180644861\n",
      "900 2.30470269047942\n",
      "1000 2.1825859564280035\n",
      "SGD relu BCELoss\n",
      "100 2.6700671315827504\n",
      "200 2.5944599068841065\n",
      "300 2.5877351510382622\n",
      "400 2.5907201531840527\n",
      "500 2.6054349287947827\n",
      "600 2.6389516931339627\n",
      "700 2.673245066452649\n",
      "800 2.7031052972289977\n",
      "900 2.7333639765813023\n",
      "1000 2.7581172563589753\n",
      "SGD relu BCELoss\n",
      "100 5.932622738728683\n",
      "200 6.013477909017426\n",
      "300 6.132668615594474\n",
      "400 6.277001429959071\n",
      "500 6.40908488597637\n",
      "600 6.504194410440021\n",
      "700 6.5612471331859075\n",
      "800 6.584600678724541\n",
      "900 6.587216632927328\n",
      "1000 6.578601212319643\n",
      "SGD tanh BCELoss\n",
      "100 1.296122246877686\n",
      "200 1.296969832432768\n",
      "300 1.2969867989240424\n",
      "400 1.2969867415234035\n",
      "500 1.2969867416168739\n",
      "600 1.296986741618575\n",
      "700 1.2969867416185659\n",
      "800 1.2969867416185665\n",
      "900 1.2969867416185665\n",
      "1000 1.2969867416185659\n",
      "SGD tanh BCELoss\n",
      "100 1.6136259868971303\n",
      "200 1.5928625797761429\n",
      "300 1.5897947596071325\n",
      "400 1.5888414325689897\n",
      "500 1.5885291897411826\n",
      "600 1.5884494278015973\n",
      "700 1.588446165286049\n",
      "800 1.5884629332168065\n",
      "900 1.588481092351326\n",
      "1000 1.5884956394650713\n",
      "SGD tanh BCELoss\n",
      "100 1.8045266576529289\n",
      "200 1.6416514160632172\n",
      "300 1.558018751308175\n",
      "400 1.496245138492109\n",
      "500 1.457306747156651\n",
      "600 1.433447414813936\n",
      "700 1.4184496015194457\n",
      "800 1.4083756650898482\n",
      "900 1.401420889166461\n",
      "1000 1.3966837819263789\n",
      "SGD tanh BCELoss\n",
      "100 1.4861865642181933\n",
      "200 1.687764840125548\n",
      "300 1.8670700973011012\n",
      "400 1.8998766349748382\n",
      "500 1.8527632995080459\n",
      "600 1.8091745315723382\n",
      "700 1.7795321148407666\n",
      "800 1.7592930726274663\n",
      "900 1.7454866283114574\n",
      "1000 1.7363101740536617\n",
      "SGD tanh BCELoss\n",
      "100 3.3039955016655878\n",
      "200 3.260126147916214\n",
      "300 3.217109321421979\n",
      "400 3.1749792925356743\n",
      "500 3.133770861994878\n",
      "600 3.093520920184627\n",
      "700 3.0542696199461195\n",
      "800 3.016060970279793\n",
      "900 2.9789425910653695\n",
      "1000 2.942964505747747\n",
      "SGDNesterov sigmoid BCELoss\n",
      "100 0.8007500643729799\n",
      "200 0.8023867286991282\n",
      "300 0.8035877545263944\n",
      "400 0.8027943553671961\n",
      "500 0.8002319580803449\n",
      "600 0.7965548456249227\n",
      "700 0.7923245419143321\n",
      "800 0.7878419736369422\n",
      "900 0.7832887192291125\n",
      "1000 0.7788229308674831\n",
      "SGDNesterov sigmoid BCELoss\n",
      "100 0.8428971245055001\n",
      "200 0.8311794893761074\n",
      "300 0.8243864670843972\n",
      "400 0.8205114606869334\n",
      "500 0.8182451125380489\n",
      "600 0.8168268225858928\n",
      "700 0.8158744148371883\n",
      "800 0.8151935094059839\n",
      "900 0.8146813444395821\n",
      "1000 0.8142810282313814\n",
      "SGDNesterov sigmoid BCELoss\n",
      "100 1.123160003579313\n",
      "200 1.037468754955458\n",
      "300 1.0091858124896902\n",
      "400 0.9951388777569059\n",
      "500 0.9872562177387982\n",
      "600 0.9827039391498619\n",
      "700 0.9800789638552859\n",
      "800 0.9786080070188808\n",
      "900 0.9778383171040996\n",
      "1000 0.9774887288673725\n",
      "SGDNesterov sigmoid BCELoss\n",
      "100 2.837377816651631\n",
      "200 2.6028852898419093\n",
      "300 2.3164572331681597\n",
      "400 2.012615323640147\n",
      "500 1.734260499396886\n",
      "600 1.5098355249641644\n",
      "700 1.3451891864542254\n",
      "800 1.2321076782164948\n",
      "900 1.1582249608585133\n",
      "1000 1.111838252796513\n",
      "SGDNesterov sigmoid BCELoss\n",
      "100 1.6772053420992565\n",
      "200 1.6725202995548387\n",
      "300 1.668306854095618\n",
      "400 1.6644747633980228\n",
      "500 1.660946430036534\n",
      "600 1.6576553608826425\n",
      "700 1.6545447636507056\n",
      "800 1.6515663017601423\n",
      "900 1.6486790027660765\n",
      "1000 1.6458483077432697\n",
      "SGDNesterov relu BCELoss\n",
      "100 9.345950327237146\n",
      "200 4.779246656594694\n",
      "300 4.596926359756994\n",
      "400 4.476210808282868\n",
      "500 1.0174912933240543\n",
      "600 4.401288864280021\n",
      "700 nan\n",
      "800 nan\n",
      "900 nan\n",
      "1000 nan\n",
      "SGDNesterov relu BCELoss\n",
      "100 2.360482928057843\n",
      "200 2.1253805409544664\n",
      "300 2.010046453951619\n",
      "400 1.914775694630832\n",
      "500 1.8235646425648335\n",
      "600 1.75129882191398\n",
      "700 1.6825522472479422\n",
      "800 1.6282004769847895\n",
      "900 1.5851689129346571\n",
      "1000 1.5515235142741464\n",
      "SGDNesterov relu BCELoss\n",
      "100 3.679142817552521\n",
      "200 3.1546423225613767\n",
      "300 2.836666849265388\n",
      "400 2.665021692990997\n",
      "500 2.417878738691465\n",
      "600 2.0780391405201253\n",
      "700 1.9190213017936943\n",
      "800 1.8300275532769323\n",
      "900 1.7447424077763338\n",
      "1000 1.7058000519635763\n",
      "SGDNesterov relu BCELoss\n",
      "100 3.5382276484396105\n",
      "200 3.7208165866070764\n",
      "300 3.844994999587426\n",
      "400 3.9225307649903294\n",
      "500 3.9527487000189083\n",
      "600 3.9625136001287884\n",
      "700 3.9556425799426043\n",
      "800 3.9468824803461167\n",
      "900 3.9434256254307902\n",
      "1000 3.942322475888405\n",
      "SGDNesterov relu BCELoss\n",
      "100 9.328124920596393\n",
      "200 9.30560408173837\n",
      "300 9.280223395640588\n",
      "400 9.250529137913256\n",
      "500 9.21406400302145\n",
      "600 9.170238631228804\n",
      "700 9.121871335938241\n",
      "800 9.073487366489948\n",
      "900 9.031878256334554\n",
      "1000 9.001196972112457\n",
      "SGDNesterov tanh BCELoss\n",
      "100 1.1977272865703135\n",
      "200 1.2379663455046521\n",
      "300 1.2564832250679083\n",
      "400 1.2609334707291284\n",
      "500 1.2616973062643495\n",
      "600 1.2617993186885168\n",
      "700 1.261809907518035\n",
      "800 1.2618106140156276\n",
      "900 1.2618105958663721\n",
      "1000 1.2618105794398675\n",
      "SGDNesterov tanh BCELoss\n",
      "100 1.3434238467078166\n",
      "200 1.3090572615262372\n",
      "300 1.302582010150715\n",
      "400 1.2984993431364487\n",
      "500 1.2961608930853548\n",
      "600 1.2948146469474908\n",
      "700 1.29404506695351\n",
      "800 1.2936051349030004\n",
      "900 1.2933522458932438\n",
      "1000 1.2932058744542605\n",
      "SGDNesterov tanh BCELoss\n",
      "100 1.6374298366115891\n",
      "200 1.5954028462039787\n",
      "300 1.5983580404298252\n",
      "400 1.5975459305678064\n",
      "500 1.5912512698873134\n",
      "600 1.5850771574358518\n",
      "700 1.578970091777102\n",
      "800 1.5732621389126042\n",
      "900 1.5674656944500085\n",
      "1000 1.560747062923533\n",
      "SGDNesterov tanh BCELoss\n",
      "100 4.0895169025344\n",
      "200 3.4224703050102305\n",
      "300 3.1143528490402637\n",
      "400 2.91533480957784\n",
      "500 2.753755626075253\n",
      "600 2.6265448657603008\n",
      "700 2.529063482124196\n",
      "800 2.4498286112141665\n",
      "900 2.3824671108715267\n",
      "1000 2.3241471007016745\n",
      "SGDNesterov tanh BCELoss\n",
      "100 2.442912177773489\n",
      "200 2.450433259033744\n",
      "300 2.458030187492686\n",
      "400 2.4657662109313856\n",
      "500 2.4736957980058225\n",
      "600 2.4818627935748037\n",
      "700 2.490300340926991\n",
      "800 2.4990308830019585\n",
      "900 2.5080651529161884\n",
      "1000 2.5173998640586515\n",
      "Adagrad sigmoid BCELoss\n",
      "100 0.7661408768576006\n",
      "200 0.7846526377511249\n",
      "300 0.7967134703414067\n",
      "400 0.7999413155662577\n",
      "500 0.7990572758933009\n",
      "600 0.7977312388051148\n",
      "700 0.7967692780477884\n",
      "800 0.7961165526185956\n",
      "900 0.7956709681371615\n",
      "1000 0.795366795920519\n",
      "Adagrad sigmoid BCELoss\n",
      "100 0.8545916961517216\n",
      "200 0.8423994006082361\n",
      "300 0.8355987495241088\n",
      "400 0.8316166043713608\n",
      "500 0.8289775869555299\n",
      "600 0.8270820988209596\n",
      "700 0.8256485411218472\n",
      "800 0.8245236399313547\n",
      "900 0.8236156149103481\n",
      "1000 0.8228658967073808\n",
      "Adagrad sigmoid BCELoss\n",
      "100 2.024318102086409\n",
      "200 1.748945571972195\n",
      "300 1.5831038288924775\n",
      "400 1.4713953486242557\n",
      "500 1.3911671138203112\n",
      "600 1.3307037201328338\n",
      "700 1.2832897843653688\n",
      "800 1.2448765137536941\n",
      "900 1.2129003051017933\n",
      "1000 1.1856784822987665\n",
      "Adagrad sigmoid BCELoss\n",
      "100 3.111953623863909\n",
      "200 3.062683468296906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 3.024963151582308\n",
      "400 2.993234510169671\n",
      "500 2.9653420318295876\n",
      "600 2.9401790328384783\n",
      "700 2.9170876819348965\n",
      "800 2.895639963831598\n",
      "900 2.8755394834184984\n",
      "1000 2.8565711823884268\n",
      "Adagrad sigmoid BCELoss\n",
      "100 0.6534487911755669\n",
      "200 0.6543754334167918\n",
      "300 0.655087976401915\n",
      "400 0.6556895270961612\n",
      "500 0.6562200994825087\n",
      "600 0.6567002323005803\n",
      "700 0.6571421321651693\n",
      "800 0.6575537546454776\n",
      "900 0.6579406267503228\n",
      "1000 0.6583067732032323\n",
      "Adagrad relu BCELoss\n",
      "100 3.82375883424896\n",
      "200 3.26606992100364\n",
      "300 2.772578790080814\n",
      "400 2.56206390307035\n",
      "500 2.435353323186849\n",
      "600 2.227142376266109\n",
      "700 2.1464524498714432\n",
      "800 2.0394276644832847\n",
      "900 2.0945809041797103\n",
      "1000 1.9781081694086426\n",
      "Adagrad relu BCELoss\n",
      "100 2.105487834783516\n",
      "200 2.0852915017426525\n",
      "300 2.123900105216067\n",
      "400 2.202272675134983\n",
      "500 2.020337123185034\n",
      "600 1.9857799133751382\n",
      "700 1.9364408379068043\n",
      "800 1.913038917760671\n",
      "900 1.917668598085849\n",
      "1000 1.9231975616633632\n",
      "Adagrad relu BCELoss\n",
      "100 5.205865384736074\n",
      "200 5.088897809741125\n",
      "300 5.001679578506415\n",
      "400 4.948195866098029\n",
      "500 4.864464965072305\n",
      "600 4.779194042490259\n",
      "700 4.66776797025148\n",
      "800 4.498158914754362\n",
      "900 4.336777182732399\n",
      "1000 4.212133304324043\n",
      "Adagrad relu BCELoss\n",
      "100 4.061154436894315\n",
      "200 4.166029658643599\n",
      "300 4.238398682414903\n",
      "400 4.288914996706675\n",
      "500 4.325467715059009\n",
      "600 4.352661226180005\n",
      "700 4.3734278675979095\n",
      "800 4.38971292034194\n",
      "900 4.402778099706895\n",
      "1000 4.413447686544\n",
      "Adagrad relu BCELoss\n",
      "100 3.964835736445342\n",
      "200 3.9658713645888497\n",
      "300 3.9665559356678473\n",
      "400 3.9670578325273294\n",
      "500 3.967449413383596\n",
      "600 3.9677658465362446\n",
      "700 3.9680274893780814\n",
      "800 3.9682481382898036\n",
      "900 3.9684369359519245\n",
      "1000 3.968600622555193\n",
      "Adagrad tanh BCELoss\n",
      "100 1.0524320950958008\n",
      "200 1.1672994721219367\n",
      "300 1.1890100067445748\n",
      "400 1.1918112314639737\n",
      "500 1.191630457511552\n",
      "600 1.191497973073029\n",
      "700 1.1914834797812095\n",
      "800 1.1914862839583285\n",
      "900 1.1914873041674212\n",
      "1000 1.1914873573280282\n",
      "Adagrad tanh BCELoss\n",
      "100 1.4013158645276853\n",
      "200 1.3791738980343693\n",
      "300 1.3661494128455822\n",
      "400 1.3613295813523114\n",
      "500 1.3593550205886569\n",
      "600 1.3584646165460532\n",
      "700 1.3580460309757285\n",
      "800 1.357846090388602\n",
      "900 1.3577500013846444\n",
      "1000 1.3577037112776957\n",
      "Adagrad tanh BCELoss\n",
      "100 2.065257414394103\n",
      "200 1.9189190735713306\n",
      "300 1.825823521979006\n",
      "400 1.7641575759235724\n",
      "500 1.7224705752252727\n",
      "600 1.6919242359450921\n",
      "700 1.667455319674165\n",
      "800 1.6468293782580983\n",
      "900 1.6289982528675049\n",
      "1000 1.6133821518028924\n",
      "Adagrad tanh BCELoss\n",
      "100 2.199513470624739\n",
      "200 2.1832377623121846\n",
      "300 2.1707570430745484\n",
      "400 2.1602338370468237\n",
      "500 2.1509522315543332\n",
      "600 2.1425492884870567\n",
      "700 2.134816997102173\n",
      "800 2.1276240375187583\n",
      "900 2.1208806708356156\n",
      "1000 2.1145225888572927\n",
      "Adagrad tanh BCELoss\n",
      "100 5.68689492562977\n",
      "200 5.676533422461474\n",
      "300 5.668551686631734\n",
      "400 5.661803757539251\n",
      "500 5.655844799424228\n",
      "600 5.6504464329322674\n",
      "700 5.645472902121294\n",
      "800 5.640835722717438\n",
      "900 5.636473422076923\n",
      "1000 5.632341237638048\n",
      "RMSProp sigmoid BCELoss\n",
      "100 1.02611948279395\n",
      "200 1.0286970941833729\n",
      "300 1.0273533603213267\n",
      "400 1.0724923634555223\n",
      "500 1.0538870654782668\n",
      "600 1.0482153951562765\n",
      "700 1.0617482792564608\n",
      "800 1.0595739339347732\n",
      "900 1.058002546178071\n",
      "1000 1.0564102727324436\n",
      "RMSProp sigmoid BCELoss\n",
      "100 1.4757579048190923\n",
      "200 1.3127068847466437\n",
      "300 1.2764273644530468\n",
      "400 1.2678835622143068\n",
      "500 1.2650622888968768\n",
      "600 1.263312009861451\n",
      "700 1.2616333407876552\n",
      "800 1.2613716910222181\n",
      "900 1.2148508037867731\n",
      "1000 1.182606262527804\n",
      "RMSProp sigmoid BCELoss\n",
      "100 1.1544784671601487\n",
      "200 1.1381148162654409\n",
      "300 1.1336417571866473\n",
      "400 1.1375264941849386\n",
      "500 1.1393073045114892\n",
      "600 1.1385195025466877\n",
      "700 1.138443802575015\n",
      "800 1.139848256375069\n",
      "900 1.1400328148434142\n",
      "1000 1.1377275882878202\n",
      "RMSProp sigmoid BCELoss\n",
      "100 1.1928275387431908\n",
      "200 1.0770319697764377\n",
      "300 1.0196554491903989\n",
      "400 0.9879164557568111\n",
      "500 0.9686055546252127\n",
      "600 0.9559424551765382\n",
      "700 0.9470540847689444\n",
      "800 0.9403608363774847\n",
      "900 0.93496343578691\n",
      "1000 0.9303477284411169\n",
      "RMSProp sigmoid BCELoss\n",
      "100 3.367845021462998\n",
      "200 3.2924421353008375\n",
      "300 3.2410921539832165\n",
      "400 3.201587932530684\n",
      "500 3.1693384743429815\n",
      "600 3.14202775943514\n",
      "700 3.118299021418803\n",
      "800 3.0972841042506554\n",
      "900 3.078395646936761\n",
      "1000 3.061219885626978\n",
      "RMSProp relu BCELoss\n",
      "100 nan\n",
      "200 nan\n",
      "300 nan\n",
      "400 nan\n",
      "500 nan\n",
      "600 nan\n",
      "700 nan\n",
      "800 nan\n",
      "900 nan\n",
      "1000 nan\n",
      "RMSProp relu BCELoss\n",
      "100 nan\n",
      "200 nan\n",
      "300 nan\n",
      "400 nan\n",
      "500 nan\n",
      "600 nan\n",
      "700 nan\n",
      "800 nan\n",
      "900 nan\n",
      "1000 nan\n",
      "RMSProp relu BCELoss\n",
      "100 2.865298794121358\n",
      "200 2.675461962548168\n",
      "300 2.5575416653632166\n",
      "400 2.4548297827138104\n",
      "500 2.3714854158576735\n",
      "600 2.3261312881962284\n",
      "700 2.3398566088682404\n",
      "800 2.3249763898818214\n",
      "900 2.30595204757198\n",
      "1000 2.302489295375936\n",
      "RMSProp relu BCELoss\n",
      "100 5.562234492321938\n",
      "200 5.169703987516511\n",
      "300 5.028814292921947\n",
      "400 5.018524022033187\n",
      "500 4.994335357437454\n",
      "600 4.952448265872627\n",
      "700 4.891371981636086\n",
      "800 4.816961279860058\n",
      "900 4.743631614612115\n",
      "1000 4.697139996428933\n",
      "RMSProp relu BCELoss\n",
      "100 1.973319185683972\n",
      "200 2.027287037625204\n",
      "300 2.1151705209526788\n",
      "400 2.208211536166971\n",
      "500 2.2893586465829054\n",
      "600 2.360909290857885\n",
      "700 2.428832365097054\n",
      "800 2.4902806817324077\n",
      "900 2.5443903506790377\n",
      "1000 2.592533178937945\n",
      "RMSProp tanh BCELoss\n",
      "100 1.4718897791724312\n",
      "200 1.381786802658801\n",
      "300 1.3497186004424893\n",
      "400 1.3409351334003188\n",
      "500 1.3615831521561172\n",
      "600 1.3812123881168983\n",
      "700 1.3966351786960252\n",
      "800 1.385111193518501\n",
      "900 1.3842064625630894\n",
      "1000 1.3924284443338963\n",
      "RMSProp tanh BCELoss\n",
      "100 1.5353614318524573\n",
      "200 1.4895390296256974\n",
      "300 1.4582219800746512\n",
      "400 1.4123051138680485\n",
      "500 1.2916537086769382\n",
      "600 1.3059549343078145\n",
      "700 1.25468526651115\n",
      "800 1.2891474611634046\n",
      "900 1.2767375532450858\n",
      "1000 1.2940657905338617\n",
      "RMSProp tanh BCELoss\n",
      "100 2.0049896980189854\n",
      "200 1.859710107650798\n",
      "300 1.8791223746773396\n",
      "400 1.8735118982441437\n",
      "500 1.869260598265859\n",
      "600 1.8767566929965722\n",
      "700 1.8773945221034551\n",
      "800 1.8594410757382667\n",
      "900 1.8902188308690435\n",
      "1000 1.886377865373172\n",
      "RMSProp tanh BCELoss\n",
      "100 2.1909782662321744\n",
      "200 2.3920920892282465\n",
      "300 2.356855566802033\n",
      "400 2.2703251079064866\n",
      "500 2.1857329402999\n",
      "600 2.1225244160793357\n",
      "700 2.079429446015196\n",
      "800 2.051049547528058\n",
      "900 2.0337512256678156\n",
      "1000 2.0244408326134646\n",
      "RMSProp tanh BCELoss\n",
      "100 4.39876080617698\n",
      "200 4.072691150670907\n",
      "300 3.8288313350603507\n",
      "400 3.629861433357032\n",
      "500 3.4607119669698627\n",
      "600 3.3134292793679614\n",
      "700 3.183191135025582\n",
      "800 3.0668256197428225\n",
      "900 2.962115816993789\n",
      "1000 2.8674219782255377\n",
      "Adam sigmoid BCELoss\n",
      "100 0.7922637982816263\n",
      "200 0.7954393121216129\n",
      "300 0.795458402048659\n",
      "400 0.7954584647689404\n",
      "500 0.7954584625984294\n",
      "600 0.8044538420421865\n",
      "700 0.7621225007848916\n",
      "800 0.7631115664331761\n",
      "900 0.7631235409368148\n",
      "1000 0.7656929771425033\n",
      "Adam sigmoid BCELoss\n",
      "100 0.8828412733073411\n",
      "200 0.8846318229547917\n",
      "300 0.8841725093977947\n",
      "400 0.8841796137662848\n",
      "500 0.8842190645404724\n",
      "600 0.8842369787322073\n",
      "700 0.8842422885763853\n",
      "800 0.8842434539867976\n",
      "900 0.8842436714221114\n",
      "1000 0.8842436817565603\n",
      "Adam sigmoid BCELoss\n",
      "100 1.1765763517328216\n",
      "200 1.0088492999985075\n",
      "300 0.9448121682048201\n",
      "400 0.9198528480718509\n",
      "500 0.9086004127096647\n",
      "600 0.9032378249479582\n",
      "700 0.9005163676054858\n",
      "800 0.8990417547588522\n",
      "900 0.8982366922535457\n",
      "1000 0.8978333814906672\n",
      "Adam sigmoid BCELoss\n",
      "100 2.74786185023373\n",
      "200 2.4534367298333777\n",
      "300 2.162152560800384\n",
      "400 1.919732584607944\n",
      "500 1.7276669660198243\n",
      "600 1.5930623632068577\n",
      "700 1.4907687747958807\n",
      "800 1.411292815223823\n",
      "900 1.3512408900922246\n",
      "1000 1.3053849291118627\n",
      "Adam sigmoid BCELoss\n",
      "100 3.727780410550066\n",
      "200 3.734421487658755\n",
      "300 3.7414209868554513\n",
      "400 3.748754920871943\n",
      "500 3.7563966487406124\n",
      "600 3.76430355089224\n",
      "700 3.772422314381117\n",
      "800 3.7806934311450826\n",
      "900 3.789049736271568\n",
      "1000 3.7974172793108805\n",
      "Adam relu BCELoss\n",
      "100 nan\n",
      "200 nan\n",
      "300 nan\n",
      "400 nan\n",
      "500 nan\n",
      "600 nan\n",
      "700 nan\n",
      "800 nan\n",
      "900 nan\n",
      "1000 nan\n",
      "Adam relu BCELoss\n",
      "100 2.860719107614036\n",
      "200 2.8784284286765653\n",
      "300 2.870439414687867\n",
      "400 2.580770399559398\n",
      "500 2.5109551782071073\n",
      "600 2.243800556327507\n",
      "700 2.4662420641287714\n",
      "800 2.3588183038602106\n",
      "900 2.299675252448519\n",
      "1000 2.4139196926785034\n",
      "Adam relu BCELoss\n",
      "100 2.103156033733143\n",
      "200 2.057670009117582\n",
      "300 1.9349416331705116\n",
      "400 1.9449658131046277\n",
      "500 1.9580072420072825\n",
      "600 1.9582771090536926\n",
      "700 1.9576503502541232\n",
      "800 1.9571370059871591\n",
      "900 1.9557425820729282\n",
      "1000 1.9548383489016208\n",
      "Adam relu BCELoss\n",
      "100 4.5263458076609\n",
      "200 4.405129395409589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 4.171393587917271\n",
      "400 4.000237961507313\n",
      "500 3.7826169114346198\n",
      "600 3.5802501753599425\n",
      "700 3.3104392327854146\n",
      "800 3.0223530842038233\n",
      "900 3.0796427142421106\n",
      "1000 3.0579693441981557\n",
      "Adam relu BCELoss\n",
      "100 4.112511348669457\n",
      "200 4.111182967512341\n",
      "300 4.111460042273348\n",
      "400 4.110459223383557\n",
      "500 4.113766747401288\n",
      "600 4.127045121707725\n",
      "700 4.148612205480396\n",
      "800 4.173206272666994\n",
      "900 4.194756620849012\n",
      "1000 4.211062206585306\n",
      "Adam tanh BCELoss\n",
      "100 1.1088809291203456\n",
      "200 1.2151486780340004\n",
      "300 1.058564692514952\n",
      "400 0.967497411767572\n",
      "500 1.3017753984718705\n",
      "600 1.0198504779934867\n",
      "700 1.079074746790357\n",
      "800 1.1219927846661468\n",
      "900 1.0414486605532625\n",
      "1000 1.0356426075144347\n",
      "Adam tanh BCELoss\n",
      "100 1.4527721860562541\n",
      "200 1.454075096274189\n",
      "300 1.4541016044514319\n",
      "400 1.4541016109009293\n",
      "500 1.4175991463954996\n",
      "600 1.4570937953613374\n",
      "700 1.4573823305043812\n",
      "800 1.6139011366420821\n",
      "900 1.4609470374959503\n",
      "1000 1.4598104794875273\n",
      "Adam tanh BCELoss\n",
      "100 1.2675594189508024\n",
      "200 1.2633886703057475\n",
      "300 1.2660516507465167\n",
      "400 1.267425550889716\n",
      "500 1.267668166134384\n",
      "600 1.2677203126034238\n",
      "700 1.2677313583988914\n",
      "800 1.267733311435475\n",
      "900 1.2677335815838109\n",
      "1000 1.2677336100072631\n",
      "Adam tanh BCELoss\n",
      "100 4.288277503960535\n",
      "200 3.7051709719470005\n",
      "300 3.3479117929808306\n",
      "400 3.0648597713351875\n",
      "500 2.832316605498105\n",
      "600 2.6430423784755397\n",
      "700 2.479917368246807\n",
      "800 2.3529735353399754\n",
      "900 2.2530753484950856\n",
      "1000 2.1549972267763104\n",
      "Adam tanh BCELoss\n",
      "100 2.1202597422768847\n",
      "200 2.127849570311315\n",
      "300 2.132591970289948\n",
      "400 2.135077702613893\n",
      "500 2.13563194387409\n",
      "600 2.134487669100791\n",
      "700 2.1318023503936128\n",
      "800 2.127650040011647\n",
      "900 2.122029905871445\n",
      "1000 2.1148965483438604\n"
     ]
    }
   ],
   "source": [
    "optims=['SGD','SGDNesterov','Adagrad','RMSProp','Adam']\n",
    "# optims=['RMSProp']\n",
    "lrs=[1e-1,1e-2,1e-3,1e-4,1e-5]\n",
    "activs=['sigmoid','relu','tanh']\n",
    "losses=['BCELoss']\n",
    "fout = open(\"results.csv\",'w')\n",
    "fout.write(\"iter, lr, optim, activ, loss, funVal \\n\")\n",
    "for optim in optims:\n",
    "    nesterov=False\n",
    "    optimfunc=optim\n",
    "    if optim=='SGDNesterov':\n",
    "        nesterov=True\n",
    "        optimfunc='SGD'\n",
    "    for activ in activs:\n",
    "        for loss in losses:\n",
    "            for lr in lrs:\n",
    "                print(optim, activ, loss)\n",
    "                cancer_model = NeuralNetwork(Xtrain_cancer.shape[1], 1, 128, [activ, 'sigmoid'], 'BCELoss', optimfunc)\n",
    "                funVals, ypred = cancer_model.train(Xtrain_cancer, ytrain_cancer, batch_size=100, iterations=1000, alpha=lr,momentum_param=0.9, nesterov=nesterov)\n",
    "                cancer_model.reset_weights()\n",
    "                i=0\n",
    "                for val in funVals:\n",
    "                    fout.write(str(i)+','+str(lr)+','+optim+','+activ+','+loss+','+str(val)+'\\n')\n",
    "                    i+=1\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
