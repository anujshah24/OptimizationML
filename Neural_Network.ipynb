{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of features of the input.\n",
    "            output_size (int): Dimension of output.\n",
    "            hidden_layer_size (int): Number of neurons in the input layer.\n",
    "            activations (list): List of strings giving the activations for each layer.\n",
    "            loss (str): Loss function for the model.\n",
    "            optimizer (str): Optimization algorithm for the model.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X, w1=None, wn=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (tensor): Input for the model. \n",
    "            w1 (tensor): Weights to be used for the first layer. (Optional Argument)\n",
    "            wn (tensor): Weights to be used for the final layer. (Optional Argument)\n",
    "            \n",
    "        Returns:\n",
    "            z (list): List of outputs from linear function at each layer.\n",
    "            a (list): List of activation outputs from each layer.\n",
    "        \"\"\"\n",
    "        if w1 is None:\n",
    "            w1 = self.w1\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a, wn=None):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Input Data\n",
    "            y (Tensor): Output Data\n",
    "            z (list): List of outputs from linear layers.\n",
    "            a (list): List of actiation outputs.\n",
    "            wn (Tensor): Weights from final layer. (Optional Argument)\n",
    "        \"\"\"\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False, decay_rate=0.999, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        ypred = None\n",
    "        if self.optimizer == 'SGD':\n",
    "            if momentum_param != 0:\n",
    "                if nesterov:\n",
    "                    funVals ,ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param, True)\n",
    "                else:\n",
    "                    funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param)\n",
    "            else:\n",
    "                funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'Adagrad':\n",
    "            funVals, ypred = self.Adagrad(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            funVals, ypred = self.RMSProp(X, y, batch_size, iterations, alpha, decay_rate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            funVals, ypred = self.Adam(X, y, batch_size, iterations, alpha, beta1, beta2)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def SGD(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Gradient Descent Algorithm\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        vn = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                if nesterov:\n",
    "                    z, a = self.forward(X[i:i+batch_size], self.w1+momentum_param*v1, self.wn+momentum_param*vn)\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a, self.wn+momentum_param*vn)\n",
    "                else:\n",
    "                    z, a = self.forward(X[i:i+batch_size])\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                v1 = momentum_param * v1 - alpha * dW[0]\n",
    "                vn = momentum_param * vn - alpha * dW[1]\n",
    "                self.w1 = self.w1 + v1\n",
    "                self.wn = self.wn + vn\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adagrad(self, X, y, batch_size=100, iterations=500, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        AdaGrad Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 += dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1)+smoothing_param)) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2)+smoothing_param)) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def RMSProp(self, X, y, batch_size=100, iterations=500, alpha=1e-04, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        RMSProp Optimizer.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 = decay_rate*cache1 + (1 - decay_rate) * dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1+smoothing_param))) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2+smoothing_param))) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#                     print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adam(self, X, y, batch_size=100, iterations=500, alpha=1e-04, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        m1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        m2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        v2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                m1 = beta1 * m1 + (1-beta1) * dW[0]\n",
    "                v1 = beta2 * v1 + (1-beta2) * dW[0]**2\n",
    "                m2 = beta1 * m2 + (1-beta1) * dW[1]\n",
    "                v2 = beta2 * v2 + (1-beta2) * dW[1]**2\n",
    "                self.w1 += -alpha*(m1/(1-beta1**n_iter))/(torch.sqrt((v1)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                self.wn += -alpha*(m2/(1-beta1**n_iter))/(torch.sqrt((v2)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "#             print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "        \"\"\"\n",
    "        _, a = self.forward(X)\n",
    "        return a[-1]\n",
    "    \n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset Weights\n",
    "        \"\"\"\n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        elif activation == 'tanh':\n",
    "            return lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>=0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            tanh = lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "            return lambda z : 1 - tanh(z)**2\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((ypred - y).T, (ypred - y))/(2*len(y))\n",
    "        elif self.loss == 'BCELoss':\n",
    "            return lambda ypred, y : (-1/len(y))*(torch.matmul(y.T, torch.log(ypred)) + torch.matmul((1-y).T, torch.log(1-ypred)))\n",
    "        elif self.loss == \"CELoss\":\n",
    "            def crossEntropyLoss(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                prob = self.softmax(ypred)\n",
    "                log_likelihood = -torch.log(prob[range(m), y.long()])\n",
    "                loss = torch.sum(log_likelihood)\n",
    "                return loss/m\n",
    "            return crossEntropyLoss\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y: (ypred - y)/len(y)\n",
    "        elif self.loss == 'BCELoss':\n",
    "            return lambda ypred, y: (-1/len(y)) * ((y/ypred) - ((1-y)/(1-ypred)))\n",
    "        elif self.loss == 'CELoss':\n",
    "            def crossEntropyLossGradient(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                grad = self.softmax(ypred)\n",
    "                grad[range(m), y.long()] -= 1\n",
    "                return grad/m\n",
    "            return crossEntropyLossGradient\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exps = torch.exp(z - torch.max(z, dim=0).values)\n",
    "        return exps/torch.sum(exps, dim=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(312, 20, dtype=torch.double)\n",
    "# # y = torch.randint(0, 2,(1000, 1)).double()\n",
    "# # y = torch.randn(312, 1, dtype=torch.double)\n",
    "# y = torch.randint(0,3,(312, 1)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(X.shape[1], 3, 32, ['tanh', 'linear'], 'CELoss', 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funVals, ypred = model.train(X, y, batch_size=100, iterations=1500, alpha=1e-03, momentum_param=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((torch.sum(ypred.argmax(dim=1).reshape(-1,1) == y.long()).float()*100.0)/(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # def plotLoss(funVals, filePath, title):\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "# plt.xlabel(\"Number of Iterations\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plotLoss(funVals, filePath, title, plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"./dataset/\"+filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAutoMPGDataset():\n",
    "    import pandas as pd\n",
    "    auto_mpg_dataset = pd.read_csv(\"./dataset/auto-mpg/auto-mpg.data\", header=-1, comment='\\t', skipinitialspace=True, na_values='?', sep=' ')\n",
    "    auto_mpg_dataset = auto_mpg_dataset.dropna()\n",
    "    origin = auto_mpg_dataset.pop(7)\n",
    "    auto_mpg_dataset[7] = (origin==1)*1.0\n",
    "    auto_mpg_dataset[8] = (origin==2)*1.0\n",
    "    auto_mpg_dataset[9] = (origin==3)*1.0\n",
    "    auto_dataset = torch.tensor(auto_mpg_dataset.values, dtype=torch.double)\n",
    "    return auto_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dataset = loadAutoMPGDataset()\n",
    "auto_dataset = auto_dataset[torch.randperm(auto_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = auto_dataset[:int(0.8 * auto_dataset.shape[0])]\n",
    "test = auto_dataset[int(0.8 * auto_dataset.shape[0]):]\n",
    "\n",
    "Xtrain = train[:, 1:]\n",
    "Xtrain = (Xtrain - Xtrain.mean(dim=0))/Xtrain.std(dim=0)\n",
    "ytrain = train[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest = test[:, 1:]\n",
    "Xtest = (Xtest - Xtest.mean(dim=0))/Xtest.std(dim=0)\n",
    "ytest = test[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model = NeuralNetwork(Xtrain.shape[1], ytrain.shape[1], 64, ['relu', 'relu'], 'MSE', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model.reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 92.35154064724551\n",
      "200 60.37551350628857\n",
      "300 16.38864548239426\n",
      "400 11.228666537004942\n",
      "500 8.6939875210588\n",
      "600 7.326550375680231\n",
      "700 6.5283281835459315\n",
      "800 5.983569662423891\n",
      "900 5.56736374660477\n",
      "1000 5.232870761350045\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = auto_mpg_model.train(Xtrain, ytrain, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals, filePath=\"auto-mpg/results/Adam.png\", title=\"Adam\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD sigmoid MSE\n",
      "100 281.3443621324879\n",
      "200 278.9756407987096\n",
      "300 277.67597624476423\n",
      "400 277.1080510218557\n",
      "500 276.879494332755\n",
      "600 276.77671557261857\n",
      "700 276.7230097818229\n",
      "800 276.6912265048343\n",
      "900 276.67060020861123\n",
      "1000 276.6562738637025\n",
      "SGD relu MSE\n",
      "100 12.835968312016515\n",
      "200 8.882095906024718\n",
      "300 7.482811782162537\n",
      "400 6.729946004421936\n",
      "500 6.246782495609003\n",
      "600 5.906065595134786\n",
      "700 5.650682964972915\n",
      "800 5.447919760352731\n",
      "900 5.2756948441564235\n",
      "1000 5.129248564644239\n",
      "SGD tanh MSE\n",
      "100 297.48893951125564\n",
      "200 297.1973323119136\n",
      "300 297.06155320429275\n",
      "400 296.9641230849543\n",
      "500 296.8839280665665\n",
      "600 296.8127433554931\n",
      "700 296.74595609700407\n",
      "800 296.68009055060736\n",
      "900 296.61161613102405\n",
      "1000 296.5357533502525\n",
      "Adagrad sigmoid MSE\n",
      "100 293.0983901925828\n",
      "200 292.0164262976038\n",
      "300 291.21305684868395\n",
      "400 290.54309167870355\n",
      "500 289.9463257941228\n",
      "600 289.3931628037218\n",
      "700 288.86813288009375\n",
      "800 288.3612702607529\n",
      "900 287.8641558418137\n",
      "1000 287.3694500923348\n",
      "Adagrad relu MSE\n",
      "100 247.59373922546288\n",
      "200 225.66798257141954\n",
      "300 210.00539480310564\n",
      "400 198.1769352314387\n",
      "500 188.60651219069595\n",
      "600 180.79277609968855\n",
      "700 174.3149615897746\n",
      "800 168.8157497351583\n",
      "900 164.11336632488883\n",
      "1000 160.05080615887744\n",
      "Adagrad tanh MSE\n",
      "100 294.1523057333296\n",
      "200 293.74591564086876\n",
      "300 293.39807975899635\n",
      "400 292.8576229867652\n",
      "500 292.6583019591863\n",
      "600 292.5699180815108\n",
      "700 292.5111151941631\n",
      "800 292.46464365507575\n",
      "900 292.4271056037556\n",
      "1000 292.3979148247066\n",
      "RMSProp sigmoid MSE\n",
      "RMSProp relu MSE\n",
      "RMSProp tanh MSE\n",
      "Adam sigmoid MSE\n",
      "100 278.44701411299474\n",
      "200 276.77976542228026\n",
      "300 276.68239763577566\n",
      "400 276.6466959226557\n",
      "500 276.6282638254029\n",
      "600 276.617113018083\n",
      "700 276.60970901393574\n",
      "800 276.6044788912768\n",
      "900 276.6006161744008\n",
      "1000 276.597665350491\n",
      "Adam relu MSE\n",
      "100 114.03556273875212\n",
      "200 73.73459176460065\n",
      "300 66.09734617460978\n",
      "400 8.666231116689891\n",
      "500 5.854869075306158\n",
      "600 5.282187668066609\n",
      "700 4.887710530046828\n",
      "800 4.605060889682856\n",
      "900 4.408390611654178\n",
      "1000 4.269402321864464\n",
      "Adam tanh MSE\n",
      "100 290.2165114475734\n",
      "200 283.3600612505455\n",
      "300 278.8796465389599\n",
      "400 277.82974767015344\n",
      "500 277.6117562709954\n",
      "600 277.25456537052384\n",
      "700 276.9486368412397\n",
      "800 276.8655220024044\n",
      "900 276.81301593064416\n",
      "1000 276.7754494184729\n"
     ]
    }
   ],
   "source": [
    "optims=['SGD','Adagrad','RMSProp','Adam']\n",
    "activs=['sigmoid','relu','tanh']\n",
    "losses=['MSE']#,'BCELoss','CE']\n",
    "fout = open(\"results.csv\",'w')\n",
    "fout.write(\"iter, optim, activ, loss, funVal \\n\")\n",
    "for optim in optims:\n",
    "    for activ in activs:\n",
    "        for loss in losses:\n",
    "            print(optim, activ, loss)\n",
    "            auto_mpg_model = NeuralNetwork(Xtrain.shape[1], ytrain.shape[1], 64, [activ, activ], loss, optim)\n",
    "            funVals, ypred = auto_mpg_model.train(Xtrain, ytrain, batch_size=100, iterations=1000, alpha=1e-03)\n",
    "            auto_mpg_model.reset_weights()\n",
    "            i=0\n",
    "            for val in funVals:\n",
    "                fout.write(str(i)+','+optim+','+activ+','+loss+','+str(val)+'\\n')\n",
    "                i+=1\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
