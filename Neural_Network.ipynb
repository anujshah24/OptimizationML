{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of features of the input.\n",
    "            output_size (int): Dimension of output.\n",
    "            hidden_layer_size (int): Number of neurons in the input layer.\n",
    "            activations (list): List of strings giving the activations for each layer.\n",
    "            loss (str): Loss function for the model.\n",
    "            optimizer (str): Optimization algorithm for the model.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X, w1=None, wn=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (tensor): Input for the model. \n",
    "            w1 (tensor): Weights to be used for the first layer. (Optional Argument)\n",
    "            wn (tensor): Weights to be used for the final layer. (Optional Argument)\n",
    "            \n",
    "        Returns:\n",
    "            z (list): List of outputs from linear function at each layer.\n",
    "            a (list): List of activation outputs from each layer.\n",
    "        \"\"\"\n",
    "        if w1 is None:\n",
    "            w1 = self.w1\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a, wn=None):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): Input Data\n",
    "            y (Tensor): Output Data\n",
    "            z (list): List of outputs from linear layers.\n",
    "            a (list): List of actiation outputs.\n",
    "            wn (Tensor): Weights from final layer. (Optional Argument)\n",
    "        \"\"\"\n",
    "        if wn is None:\n",
    "            wn = self.wn\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False, decay_rate=0.999, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        ypred = None\n",
    "        if self.optimizer == 'SGD':\n",
    "            if momentum_param != 0:\n",
    "                if nesterov:\n",
    "                    funVals ,ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param, True)\n",
    "                else:\n",
    "                    funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha, momentum_param)\n",
    "            else:\n",
    "                funVals, ypred = self.SGD(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'Adagrad':\n",
    "            funVals, ypred = self.Adagrad(X, y, batch_size, iterations, alpha)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            funVals, ypred = self.RMSProp(X, y, batch_size, iterations, alpha, decay_rate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            funVals, ypred = self.Adam(X, y, batch_size, iterations, alpha, beta1, beta2)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def SGD(self, X, y, batch_size=100, iterations=500, alpha=1e-05, momentum_param=0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Gradient Descent Algorithm\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        vn = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                if nesterov:\n",
    "                    z, a = self.forward(X[i:i+batch_size], self.w1+momentum_param*v1, self.wn+momentum_param*vn)\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a, self.wn+momentum_param*vn)\n",
    "                else:\n",
    "                    z, a = self.forward(X[i:i+batch_size])\n",
    "                    dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                v1 = momentum_param * v1 - alpha * dW[0]\n",
    "                vn = momentum_param * vn - alpha * dW[1]\n",
    "                self.w1 = self.w1 + v1\n",
    "                self.wn = self.wn + vn\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adagrad(self, X, y, batch_size=100, iterations=500, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        AdaGrad Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 += dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1)+smoothing_param)) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2)+smoothing_param)) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def RMSProp(self, X, y, batch_size=100, iterations=500, alpha=1e-04, decay_rate=0.999):\n",
    "        \"\"\"\n",
    "        RMSProp Optimizer.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        cache1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        cache2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                cache1 = decay_rate*cache1 + (1 - decay_rate) * dW[0]**2\n",
    "                cache2 += dW[1]**2\n",
    "                self.w1 += -(alpha/(torch.sqrt(cache1+smoothing_param))) * dW[0]\n",
    "                self.wn += -(alpha/(torch.sqrt(cache2+smoothing_param))) * dW[1]\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def Adam(self, X, y, batch_size=100, iterations=500, alpha=1e-04, beta1=0.9, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        n_iter = 0\n",
    "        flag = True\n",
    "        smoothing_param = 1e-8\n",
    "        m1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        m2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        v1 = torch.zeros(self.w1.shape, dtype=torch.double)\n",
    "        v2 = torch.zeros(self.wn.shape, dtype=torch.double)\n",
    "        while flag:\n",
    "            for i in range(m//batch_size):\n",
    "                n_iter += 1\n",
    "                z, a = self.forward(X[i:i+batch_size])\n",
    "                dW = self.backward(X[i:i+batch_size], y[i:i+batch_size], z, a)\n",
    "                m1 = beta1 * m1 + (1-beta1) * dW[0]\n",
    "                v1 = beta2 * v1 + (1-beta2) * dW[0]**2\n",
    "                m2 = beta1 * m2 + (1-beta1) * dW[1]\n",
    "                v2 = beta2 * v2 + (1-beta2) * dW[1]**2\n",
    "                self.w1 += -alpha*(m1/(1-beta1**n_iter))/(torch.sqrt((v1)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                self.wn += -alpha*(m2/(1-beta1**n_iter))/(torch.sqrt((v2)/(1-beta2**n_iter)) + smoothing_param)\n",
    "                if n_iter>iterations:\n",
    "                    flag = False\n",
    "                    break\n",
    "                if n_iter%batch_size == 0:\n",
    "                    ypred = self.predict(X)\n",
    "                    funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "                    print(n_iter, funVals[-1])\n",
    "            W = torch.cat([self.w1.flatten().reshape(-1, 1), self.wn.reshape(-1,1)])\n",
    "            optCond = torch.matmul(W.T, W)\n",
    "            if optCond < 1e-2 or n_iter>iterations:\n",
    "                break\n",
    "            ypred = self.predict(X)\n",
    "            funVals.append((self.evaluateLoss()(ypred, y)).item())\n",
    "            print(n_iter, funVals[-1])\n",
    "        ypred = self.predict(X)\n",
    "        if self.loss == 'CELoss':\n",
    "            ypred = self.softmax(ypred)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "        \"\"\"\n",
    "        _, a = self.forward(X)\n",
    "        return a[-1]\n",
    "    \n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset Weights\n",
    "        \"\"\"\n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size, dtype=torch.double)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size, dtype=torch.double)\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            def sigmoid(z):\n",
    "                s = torch.exp(z)\n",
    "                return s/(1+s)\n",
    "            return sigmoid\n",
    "#             return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        elif activation == 'tanh':\n",
    "            return lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>=0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            tanh = lambda z : (2/(1+torch.exp(-2*z))) - 1\n",
    "            return lambda z : 1 - tanh(z)**2\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((ypred - y).T, (ypred - y))/(2*len(y))\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLoss(ypred, y):\n",
    "                ypredy1 = ypred[y==1]\n",
    "                ypredy1[ypredy1==0] = 1\n",
    "                loss = torch.sum(torch.log(ypred[y==1]+1e-05)) + torch.sum(torch.log(1 - ypred[y==0]+1e-05))\n",
    "                return -loss/y.shape[0]\n",
    "            return binaryCrossEntropyLoss\n",
    "#             return lambda ypred, y : (-1/len(y))*(torch.matmul(y.T, torch.log(ypred)) + torch.matmul((1-y).T, torch.log(1-ypred)))\n",
    "            return binaryCrossEntropyLoss\n",
    "        elif self.loss == \"CELoss\":\n",
    "            def crossEntropyLoss(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                prob = self.softmax(ypred)\n",
    "                log_likelihood = -torch.log(prob[range(m), y.long()])\n",
    "                loss = torch.sum(log_likelihood)\n",
    "                return loss/m\n",
    "            return crossEntropyLoss\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y: (ypred - y)/len(y)\n",
    "        elif self.loss == 'BCELoss':\n",
    "            def binaryCrossEntropyLossGradient(ypred, y):\n",
    "                gradient = 1/(ypred+1e-05) - 1/(1-ypred+1e-05)\n",
    "                return -gradient/y.shape[0]\n",
    "            return binaryCrossEntropyLossGradient\n",
    "#             return lambda ypred, y: (-1/len(y)) * ((y/ypred) - ((1-y)/(1-ypred)))\n",
    "        elif self.loss == 'CELoss':\n",
    "            def crossEntropyLossGradient(ypred, y):\n",
    "                m = y.shape[0]\n",
    "                grad = self.softmax(ypred)\n",
    "                grad[range(m), y.long()] -= 1\n",
    "                return grad/m\n",
    "            return crossEntropyLossGradient\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exps = torch.exp(z - (torch.max(z, dim=1).values.reshape(-1,1)))\n",
    "        return exps/torch.sum(exps, dim=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand(312, 20, dtype=torch.double)\n",
    "# y = torch.randint(0, 2,(312, 1)).double()\n",
    "# y = torch.randn(312, 1, dtype=torch.double)\n",
    "# y = torch.randint(0,3,(312, 1)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_temp = NeuralNetwork(X.shape[1], 1, 32, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funVals, ypred = model_temp.train(X, y, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((torch.sum(ypred.argmax(dim=1).reshape(-1,1) == y.long()).float()*100.0)/(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# def plotLoss(funVals, filePath, title):\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "# plt.xlabel(\"Number of Iterations\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plotLoss(funVals, filePath=None, title=\"\", plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot([i for i in range(1, len(funVals)+1)], funVals)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    if not plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"./dataset/\"+filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAutoMPGDataset():\n",
    "    import pandas as pd\n",
    "    auto_mpg_dataset = pd.read_csv(\"./dataset/auto-mpg/auto-mpg.data\", header=-1, comment='\\t', skipinitialspace=True, na_values='?', sep=' ')\n",
    "    auto_mpg_dataset = auto_mpg_dataset.dropna()\n",
    "    origin = auto_mpg_dataset.pop(7)\n",
    "    auto_mpg_dataset[7] = (origin==1)*1.0\n",
    "    auto_mpg_dataset[8] = (origin==2)*1.0\n",
    "    auto_mpg_dataset[9] = (origin==3)*1.0\n",
    "    auto_dataset = torch.tensor(auto_mpg_dataset.values, dtype=torch.double)\n",
    "    return auto_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_dataset = loadAutoMPGDataset()\n",
    "auto_dataset = auto_dataset[torch.randperm(auto_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto = auto_dataset[:int(0.8 * auto_dataset.shape[0])]\n",
    "test_auto = auto_dataset[int(0.8 * auto_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_auto = train_auto[:, 1:]\n",
    "Xtrain_auto = (Xtrain_auto - Xtrain_auto.mean(dim=0))/Xtrain_auto.std(dim=0)\n",
    "ytrain_auto = train_auto[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_auto = test_auto[:, 1:]\n",
    "Xtest_auto = (Xtest_auto - Xtest_auto.mean(dim=0))/Xtest_auto.std(dim=0)\n",
    "ytest_auto = test_auto[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_model = NeuralNetwork(Xtrain_auto.shape[1], ytrain_auto.shape[1], 64, ['tanh', 'relu'], 'MSE', 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_mpg_model.reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 259.2789634158975\n",
      "6 254.28522396070073\n",
      "9 249.19500827673858\n",
      "12 244.1048540521211\n",
      "15 239.05977691392457\n",
      "18 234.2150075268744\n",
      "21 229.54072017303642\n",
      "24 224.98001945427663\n",
      "27 220.64061619234062\n",
      "30 216.5370885605559\n",
      "33 212.66028079897987\n",
      "36 209.00446799579055\n",
      "39 205.55965458078364\n",
      "42 202.31659684319462\n",
      "45 199.26675139228348\n",
      "48 196.40221366744112\n",
      "51 193.71563011180794\n",
      "54 191.20005761940902\n",
      "57 188.8487510095084\n",
      "60 186.63480012669817\n",
      "63 184.54915311029805\n",
      "66 182.58518177203447\n",
      "69 180.7554993313831\n",
      "72 178.84102908303015\n",
      "75 177.0161741794793\n",
      "78 175.28660621518947\n",
      "81 173.64134830173487\n",
      "84 172.05659398124473\n",
      "87 170.5019142997492\n",
      "90 168.71402873707194\n",
      "93 165.86152908202445\n",
      "96 161.46493180410323\n",
      "99 155.6406068116549\n",
      "100 153.68073791669278\n",
      "102 146.21538152569408\n",
      "105 134.94435196398152\n",
      "108 124.12912198456922\n",
      "111 114.7142830096746\n",
      "114 107.67352003636557\n",
      "117 102.28081821265067\n",
      "120 98.02323616432322\n",
      "123 94.584896425215\n",
      "126 91.75316529174192\n",
      "129 89.38487396044471\n",
      "132 87.37929137280514\n",
      "135 85.66315703773732\n",
      "138 84.18183419274483\n",
      "141 82.89372638500699\n",
      "144 81.76658501762553\n",
      "147 80.77499206213564\n",
      "150 79.89860747259158\n",
      "153 79.1209284312752\n",
      "156 78.42839760474598\n",
      "159 77.80975283196356\n",
      "162 77.25554599234084\n",
      "165 76.75778192148293\n",
      "168 76.30964359135177\n",
      "171 75.90528007139696\n",
      "174 75.539640765864\n",
      "177 75.20834419653784\n",
      "180 74.90757289834089\n",
      "183 74.63398829683094\n",
      "186 74.38466105935284\n",
      "189 74.15701356746644\n",
      "192 73.9487719900834\n",
      "195 73.75792604145016\n",
      "198 73.58269495215241\n",
      "200 73.4743565054614\n",
      "201 73.42149851060314\n",
      "204 73.2729322790674\n",
      "207 73.13574627470628\n",
      "210 73.00882654842022\n",
      "213 72.89117920388772\n",
      "216 72.78191648441994\n",
      "219 72.68024462210448\n",
      "222 72.58545319661873\n",
      "225 72.49690579332032\n",
      "228 72.41403178420926\n",
      "231 72.33631908292735\n",
      "234 72.26330774750336\n",
      "237 72.1945843231154\n",
      "240 72.12977683253813\n",
      "243 72.06855033479349\n",
      "246 72.01060298331603\n",
      "249 71.95566252406448\n",
      "252 71.90348318174992\n",
      "255 71.85384288896267\n",
      "258 71.80654081864014\n",
      "261 71.76139518519695\n",
      "264 71.71824128384804\n",
      "267 71.67692974131008\n",
      "270 71.63732495423912\n",
      "273 71.59930369453527\n",
      "276 71.56275386306358\n",
      "279 71.52757337546265\n",
      "282 71.49366916557588\n",
      "285 71.46095629367659\n",
      "288 71.42935714810413\n",
      "291 71.39880073019913\n",
      "294 71.36922201355296\n",
      "297 71.34056136958061\n",
      "300 71.31276405230805\n",
      "300 71.31276405230805\n",
      "303 71.28577973604487\n",
      "306 71.25956210030742\n",
      "309 71.23406845697035\n",
      "312 71.20925941517243\n",
      "315 71.18509857998671\n",
      "318 71.16155228129722\n",
      "321 71.13858932970795\n",
      "324 71.1161807966525\n",
      "327 71.09429981617541\n",
      "330 71.07292140612935\n",
      "333 71.05202230677202\n",
      "336 71.0315808349629\n",
      "339 71.01157675235099\n",
      "342 70.99199114611758\n",
      "345 70.97280632098705\n",
      "348 70.95400570135976\n",
      "351 70.93557374253791\n",
      "354 70.91749585012633\n",
      "357 70.89975830678608\n",
      "360 70.88234820560575\n",
      "363 70.86525338943002\n",
      "366 70.84846239555827\n",
      "369 70.83196440528252\n",
      "372 70.81574919779294\n",
      "375 70.79980710802619\n",
      "378 70.78412898807537\n",
      "381 70.76870617182168\n",
      "384 70.75353044248007\n",
      "387 70.73859400278542\n",
      "390 70.72388944757071\n",
      "393 70.70940973851678\n",
      "396 70.69514818087308\n",
      "399 70.68109840197073\n",
      "400 70.67641799915074\n",
      "402 70.66725433136591\n",
      "405 70.65361018246811\n",
      "408 70.64016043552301\n",
      "411 70.62689982183059\n",
      "414 70.61382330909277\n",
      "417 70.60092608779416\n",
      "420 70.58820355852882\n",
      "423 70.57565132019414\n",
      "426 70.56326515898147\n",
      "429 70.55104103809792\n",
      "432 70.53897508816213\n",
      "435 70.52706359821971\n",
      "438 70.51530300733131\n",
      "441 70.50368989668883\n",
      "444 70.49222098222019\n",
      "447 70.48089310764644\n",
      "450 70.4697032379579\n",
      "453 70.45864845327911\n",
      "456 70.44772594309536\n",
      "459 70.43693300081506\n",
      "462 70.42626701864525\n",
      "465 70.41572548275809\n",
      "468 70.4053059687305\n",
      "471 70.3950061372374\n",
      "474 70.38482372998246\n",
      "477 70.37475656585212\n",
      "480 70.36480253727754\n",
      "483 70.35495960679201\n",
      "486 70.34522580377218\n",
      "489 70.33559922135164\n",
      "492 70.32607801349616\n",
      "495 70.31666039223187\n",
      "498 70.3073446250169\n",
      "500 70.30117767042195\n",
      "501 70.29812903224799\n",
      "504 70.28901198489528\n",
      "507 70.27999190225671\n",
      "510 70.27106724982673\n",
      "513 70.26223653727169\n",
      "516 70.25349831650709\n",
      "519 70.24485117987048\n",
      "522 70.23629375838517\n",
      "525 70.22782472010964\n",
      "528 70.21944276856827\n",
      "531 70.21114664125913\n",
      "534 70.20293510823448\n",
      "537 70.19480697075025\n",
      "540 70.18676105998163\n",
      "543 70.17879623579985\n",
      "546 70.1709113856086\n",
      "549 70.16310542323605\n",
      "552 70.15537728787974\n",
      "555 70.14772594310207\n",
      "558 70.14015037587352\n",
      "561 70.13264959566085\n",
      "564 70.1252226335591\n",
      "567 70.11786854146372\n",
      "570 70.11058639128213\n",
      "573 70.10337527418197\n",
      "576 70.09623429987448\n",
      "579 70.08916259593119\n",
      "582 70.08215930713247\n",
      "585 70.07522359484602\n",
      "588 70.0683546364342\n",
      "591 70.06155162468842\n",
      "594 70.05481376728976\n",
      "597 70.0481402862939\n",
      "600 70.04153041763988\n",
      "600 70.04153041763988\n",
      "603 70.03498341068064\n",
      "606 70.02849852773566\n",
      "609 70.02207504366305\n",
      "612 70.0157122454512\n",
      "615 70.00940943182889\n",
      "618 70.00316591289281\n",
      "621 69.99698100975145\n",
      "624 69.99085405418529\n",
      "627 69.98478438832187\n",
      "630 69.97877136432517\n",
      "633 69.97281434409891\n",
      "636 69.96691269900289\n",
      "639 69.96106580958163\n",
      "642 69.95527306530525\n",
      "645 69.94953386432148\n",
      "648 69.9438476132184\n",
      "651 69.93821372679795\n",
      "654 69.93263162785864\n",
      "657 69.92710074698847\n",
      "660 69.92162052236604\n",
      "663 69.91619039957095\n",
      "666 69.91080983140179\n",
      "669 69.90547827770241\n",
      "672 69.90019520519532\n",
      "675 69.89496008732243\n",
      "678 69.88977240409267\n",
      "681 69.88463164193564\n",
      "684 69.8795372935621\n",
      "687 69.87448885782997\n",
      "690 69.86948583961608\n",
      "693 69.86452774969327\n",
      "696 69.85961410461293\n",
      "699 69.85474442659205\n",
      "700 69.85311129247414\n",
      "702 69.8499182434052\n",
      "705 69.84513508828094\n",
      "708 69.84039449980263\n",
      "711 69.83569602181305\n",
      "714 69.83103920332302\n",
      "717 69.82642359842393\n",
      "720 69.82184876620349\n",
      "723 69.81731427066498\n",
      "726 69.81281968064984\n",
      "729 69.80836456976283\n",
      "732 69.80394851630076\n",
      "735 69.79957110318344\n",
      "738 69.79523191788762\n",
      "741 69.79093055238297\n",
      "744 69.78666660307094\n",
      "747 69.7824396707256\n",
      "750 69.77824936043636\n",
      "753 69.77409528155349\n",
      "756 69.7699770476345\n",
      "759 69.76589427639317\n",
      "762 69.76184658965005\n",
      "765 69.75783361328428\n",
      "768 69.75385497718734\n",
      "771 69.74991031521806\n",
      "774 69.74599926515906\n",
      "777 69.7421214686743\n",
      "780 69.73827657126812\n",
      "783 69.73446422224524\n",
      "786 69.7306840746718\n",
      "789 69.7269357853374\n",
      "792 69.72321901471841\n",
      "795 69.71953342694167\n",
      "798 69.71587868974929\n",
      "800 69.71345705365788\n",
      "801 69.71225447446426\n",
      "804 69.70866045595659\n",
      "807 69.70509631261041\n",
      "810 69.70156172629139\n",
      "813 69.69805638231496\n",
      "816 69.69457996941496\n",
      "819 69.69113217971292\n",
      "822 69.68771270868778\n",
      "825 69.68432125514578\n",
      "828 69.68095752119127\n",
      "831 69.6776212121974\n",
      "834 69.67431203677742\n",
      "837 69.67102970675639\n",
      "840 69.66777393714284\n",
      "843 69.66454444610099\n",
      "846 69.6613409549231\n",
      "849 69.65816318800235\n",
      "852 69.65501087280508\n",
      "855 69.65188373984452\n",
      "858 69.6487815226535\n",
      "861 69.64570395775799\n",
      "864 69.6426507846507\n",
      "867 69.63962174576463\n",
      "870 69.63661658644689\n",
      "873 69.63363505493272\n",
      "876 69.63067690231934\n",
      "879 69.62774188254023\n",
      "882 69.6248297523393\n",
      "885 69.62194027124521\n",
      "888 69.61907320154576\n",
      "891 69.61622830826242\n",
      "894 69.61340535912481\n",
      "897 69.6106041245454\n",
      "900 69.6078243775942\n",
      "900 69.6078243775942\n",
      "903 69.60506589397352\n",
      "906 69.60232845199269\n",
      "909 69.59961183254327\n",
      "912 69.59691581907377\n",
      "915 69.59424019756459\n",
      "918 69.59158475650342\n",
      "921 69.58894928686026\n",
      "924 69.58633358206252\n",
      "927 69.58373743797067\n",
      "930 69.58116065285319\n",
      "933 69.57860302736228\n",
      "936 69.57606436450942\n",
      "939 69.57354446964085\n",
      "942 69.57104315041315\n",
      "945 69.56856021676928\n",
      "948 69.56609548091423\n",
      "951 69.56364875729103\n",
      "954 69.56121986255667\n",
      "957 69.55880861555842\n",
      "960 69.55641483730982\n",
      "963 69.55403835096723\n",
      "966 69.55167898180603\n",
      "969 69.54933655719738\n",
      "972 69.54701090658473\n",
      "975 69.54470186146052\n",
      "978 69.54240925534322\n",
      "981 69.54013292375424\n",
      "984 69.53787270419511\n",
      "987 69.53562843612461\n",
      "990 69.53339996093631\n",
      "993 69.53118712193593\n",
      "996 69.5289897643192\n",
      "999 69.52680773514943\n",
      "1000 69.52608393871222\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = auto_mpg_model.train(Xtrain_auto, ytrain_auto, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIrisDataset():\n",
    "    iris_dataset = pd.read_csv(\"./dataset/iris/iris.data\", header=-1)\n",
    "    iris_dataset[4] = iris_dataset[4].astype('category').cat.codes\n",
    "    iris_dataset = torch.tensor(iris_dataset.values, dtype=torch.double)\n",
    "    return iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = loadIrisDataset()\n",
    "iris_dataset = iris_dataset[torch.randperm(iris_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris = iris_dataset[:int(0.8 * iris_dataset.shape[0])]\n",
    "test_iris = iris_dataset[int(0.8 * iris_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_iris = train_iris[:, :-1]\n",
    "Xtrain_iris = (Xtrain_iris - Xtrain_iris.mean(dim=0))/Xtrain_iris.std(dim=0)\n",
    "ytrain_iris = train_iris[:, -1].reshape(-1, 1)\n",
    "\n",
    "Xtest_iris = test_iris[:, :-1]\n",
    "Xtest_iris = (Xtest_iris - Xtest_iris.mean(dim=0))/Xtest_iris.std(dim=0)\n",
    "ytest_iris = test_iris[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = NeuralNetwork(Xtrain_iris.shape[1], 3, 64, ['relu', 'linear'], 'CELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funVals, ypred = iris_model.train(Xtrain_iris, ytrain_iris, batch_size=100, iterations=500, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(funVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.argmax(ypred, dim=1).reshape(-1,1) == ytrain_iris.long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(Xtrain_iris.shape[1], 32), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval = []\n",
    "for epoch in range(1000):\n",
    "    for i in range(Xtrain_iris.shape[0]//100):\n",
    "        optimizer.zero_grad()\n",
    "        yp = model(Xtrain_iris[i:i+100].float())\n",
    "        loss = lossFn(yp, ytrain_iris[i:i+100].long()[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fval.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLoss(fval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBreastCancerDataset():\n",
    "    cancer_dataset = pd.read_csv(\"./dataset/breast-cancer/data.csv\")\n",
    "    cancer_dataset.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "    cancer_dataset['diagnosis'] = cancer_dataset['diagnosis'].astype('category').cat.codes\n",
    "    cancer_dataset = torch.tensor(cancer_dataset.values, dtype=torch.double)\n",
    "    return cancer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = loadBreastCancerDataset()\n",
    "cancer_dataset = cancer_dataset[torch.randperm(cancer_dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cancer = cancer_dataset[:int(0.8*cancer_dataset.shape[0])]\n",
    "test_cancer = cancer_dataset[int(0.8*cancer_dataset.shape[0]):]\n",
    "\n",
    "Xtrain_cancer = train_cancer[:, 1:]\n",
    "Xtrain_cancer = (Xtrain_cancer-Xtrain_cancer.mean(dim=0))/Xtrain_cancer.std(dim=0)\n",
    "ytrain_cancer = train_cancer[:, 0].reshape(-1, 1)\n",
    "\n",
    "Xtest_cancer = test_cancer[:, 1:]\n",
    "Xtest_cancer = (Xtest_cancer-Xtest_cancer.mean(dim=0))/Xtest_cancer.std(dim=0)\n",
    "ytest_cancer = test_cancer[:, 0].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_model = NeuralNetwork(Xtrain_cancer.shape[1], 1, 16, ['relu', 'sigmoid'], 'BCELoss', 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.791577075670517\n",
      "8 2.775035275740068\n",
      "12 2.760825580608299\n",
      "16 2.7497935279550125\n",
      "20 2.742586395199901\n",
      "24 2.7390428261236983\n",
      "28 2.738721772142172\n",
      "32 2.741286393661416\n",
      "36 2.746156548262233\n",
      "40 2.752339230344166\n",
      "44 2.7595947616664027\n",
      "48 2.767856550820628\n",
      "52 2.7760954619283207\n",
      "56 2.7828810161386865\n",
      "60 2.7873621937349427\n",
      "64 2.7883133505963675\n",
      "68 2.785217659361988\n",
      "72 2.7783899064408546\n",
      "76 2.7687243053398407\n",
      "80 2.756503963843871\n",
      "84 2.7426263213728275\n",
      "88 2.7272074342552814\n",
      "92 2.7090772891311286\n",
      "96 2.6883880099343425\n",
      "100 2.6661238264598404\n",
      "100 2.6661238264598404\n",
      "104 2.642682515880678\n",
      "108 2.6175777219999974\n",
      "112 2.590651432286992\n",
      "116 2.562365746132636\n",
      "120 2.5337180317027026\n",
      "124 2.504721796020999\n",
      "128 2.4753921618412393\n",
      "132 2.4456935897839336\n",
      "136 2.41622804299901\n",
      "140 2.3869651845922624\n",
      "144 2.3581953744920985\n",
      "148 2.3287771156636845\n",
      "152 2.2993522628225866\n",
      "156 2.2695226959647417\n",
      "160 2.239141575620078\n",
      "164 2.2089147530028423\n",
      "168 2.178050639739652\n",
      "172 2.145643711464425\n",
      "176 2.1112310366828715\n",
      "180 2.07439395131306\n",
      "184 2.0348357426873243\n",
      "188 1.9914885276717267\n",
      "192 1.943094792967527\n",
      "196 1.8888692718885658\n",
      "200 1.8297743916517628\n",
      "200 1.8297743916517628\n",
      "204 1.7694587465249985\n",
      "208 1.7083221006343847\n",
      "212 1.644838580452724\n",
      "216 1.5798778724767795\n",
      "220 1.5168526206284163\n",
      "224 1.4589410596726238\n",
      "228 1.408980134371847\n",
      "232 1.3689915728108843\n",
      "236 1.3389561051579537\n",
      "240 1.3160661913447096\n",
      "244 1.3001952414264066\n",
      "248 1.2908674893324066\n",
      "252 1.2864924443860468\n",
      "256 1.2846267915483707\n",
      "260 1.2828699340301573\n",
      "264 1.2799039204439777\n",
      "268 1.276535975303595\n",
      "272 1.2709766878887052\n",
      "276 1.2623712832772578\n",
      "280 1.2509869907235887\n",
      "284 1.2383833451793262\n",
      "288 1.225448657526048\n",
      "292 1.2114988774607052\n",
      "296 1.1954787366561974\n",
      "300 1.1782651030996538\n",
      "300 1.1782651030996538\n",
      "304 1.1614543303515596\n",
      "308 1.1470244405403804\n",
      "312 1.1362930795307344\n",
      "316 1.1276768810572273\n",
      "320 1.1200181233858117\n",
      "324 1.1131074051587555\n",
      "328 1.107230163612086\n",
      "332 1.1013745266173922\n",
      "336 1.0952108217518262\n",
      "340 1.0897528283186515\n",
      "344 1.0866885391029648\n",
      "348 1.0850355431809668\n",
      "352 1.082703246267154\n",
      "356 1.0786306302458293\n",
      "360 1.0727668293365253\n",
      "364 1.0659037365067845\n",
      "368 1.0590686434742036\n",
      "372 1.0526289462499328\n",
      "376 1.046626970689238\n",
      "380 1.0407653468826434\n",
      "384 1.0349744654305555\n",
      "388 1.0296058989614634\n",
      "392 1.0250684711017197\n",
      "396 1.0212383955630042\n",
      "400 1.0176076842295076\n",
      "400 1.0176076842295076\n",
      "404 1.0140190096386388\n",
      "408 1.0099843161637627\n",
      "412 1.0057218581509637\n",
      "416 1.0019414198369763\n",
      "420 0.9989945778775274\n",
      "424 0.996413386597098\n",
      "428 0.9941108804528717\n",
      "432 0.9916331933232091\n",
      "436 0.9877470446604133\n",
      "440 0.9838259580607581\n",
      "444 0.9811308529111609\n",
      "448 0.9783344421062026\n",
      "452 0.9754611681094919\n",
      "456 0.9725474032970728\n",
      "460 0.969570032904232\n",
      "464 0.9671963547166185\n",
      "468 0.9650857072318174\n",
      "472 0.9632032792858836\n",
      "476 0.9609560959311296\n",
      "480 0.9592630261380679\n",
      "484 0.9576918694309807\n",
      "488 0.9565810068986481\n",
      "492 0.9552964067077537\n",
      "496 0.9533163586462726\n",
      "500 0.9517486458024528\n",
      "500 0.9517486458024528\n",
      "504 0.9510506108530323\n",
      "508 0.9503414747321483\n",
      "512 0.9494125151799481\n",
      "516 0.9474601545344628\n",
      "520 0.9450405660255226\n",
      "524 0.9428365866522661\n",
      "528 0.9408304928951601\n",
      "532 0.9390480491071511\n",
      "536 0.9378416431236358\n",
      "540 0.9370175197093044\n",
      "544 0.9368327290024291\n",
      "548 0.9368095687256102\n",
      "552 0.9355261557591091\n",
      "556 0.932810042199758\n",
      "560 0.9292719339952588\n",
      "564 0.92584436825276\n",
      "568 0.9230093975867775\n",
      "572 0.9206882133801705\n",
      "576 0.9192196303606441\n",
      "580 0.9181009659519263\n",
      "584 0.9165525358726612\n",
      "588 0.9153326094535774\n",
      "592 0.9138374924214518\n",
      "596 0.9120618051199721\n",
      "600 0.9109467615753523\n",
      "600 0.9109467615753523\n",
      "604 0.9098163725228507\n",
      "608 0.9088640756497598\n",
      "612 0.9077450259530514\n",
      "616 0.9068984721775099\n",
      "620 0.9062824189591425\n",
      "624 0.9052285344719704\n",
      "628 0.9035540188883926\n",
      "632 0.9022336328439082\n",
      "636 0.9012248246431945\n",
      "640 0.9004046967479211\n",
      "644 0.8994043555631579\n",
      "648 0.8985594894284876\n",
      "652 0.8978190590781027\n",
      "656 0.8970151759948203\n",
      "660 0.8959809656156646\n",
      "664 0.8952732184326746\n",
      "668 0.8943590179539029\n",
      "672 0.8935375209538489\n",
      "676 0.8928849495451092\n",
      "680 0.8916290696746532\n",
      "684 0.8909235076126814\n",
      "688 0.8900179288533281\n",
      "692 0.8886427884738076\n",
      "696 0.8877360278449006\n",
      "700 0.8870307999073622\n",
      "700 0.8870307999073622\n",
      "704 0.8861132827515326\n",
      "708 0.8861923029347702\n",
      "712 0.8858679988948736\n",
      "716 0.8848195414542472\n",
      "720 0.8849013346191679\n",
      "724 0.8846047046078316\n",
      "728 0.8842549154957704\n",
      "732 0.8837543581163487\n",
      "736 0.8827142183874923\n",
      "740 0.8816053047966947\n",
      "744 0.88071267252264\n",
      "748 0.8798746085738443\n",
      "752 0.8785747995028873\n",
      "756 0.8775210578758647\n",
      "760 0.8764667471572201\n",
      "764 0.8757968052156035\n",
      "768 0.8746320453275751\n",
      "772 0.8742519639829057\n",
      "776 0.8733983508875687\n",
      "780 0.8725470314280996\n",
      "784 0.8718011806777279\n",
      "788 0.8712024363126017\n",
      "792 0.8708320124591323\n",
      "796 0.8703478098357549\n",
      "800 0.8696269862743468\n",
      "800 0.8696269862743468\n",
      "804 0.8690457974690549\n",
      "808 0.8683474795279216\n",
      "812 0.8674532107264508\n",
      "816 0.8664063826171188\n",
      "820 0.8659613011868149\n",
      "824 0.8655882931239506\n",
      "828 0.8651868054433275\n",
      "832 0.8646322548368064\n",
      "836 0.8643536943917085\n",
      "840 0.86376700188232\n",
      "844 0.8633514487522334\n",
      "848 0.8624712563106371\n",
      "852 0.8622926603690153\n",
      "856 0.8616354781499491\n",
      "860 0.8610958978191141\n",
      "864 0.860783379482436\n",
      "868 0.8601400506660462\n",
      "872 0.8597860417204588\n",
      "876 0.8591826847198145\n",
      "880 0.8588128908440997\n",
      "884 0.858133733303574\n",
      "888 0.8576816886336649\n",
      "892 0.8571828660126217\n",
      "896 0.8566178059965551\n",
      "900 0.8561332775712288\n",
      "900 0.8561332775712288\n",
      "904 0.8555770772962638\n",
      "908 0.8550572546776923\n",
      "912 0.8542806463963316\n",
      "916 0.8540104823541997\n",
      "920 0.8535377024946672\n",
      "924 0.8529064533872818\n",
      "928 0.85225998248207\n",
      "932 0.8520208256369954\n",
      "936 0.8512905958050161\n",
      "940 0.8509979725239963\n",
      "944 0.8505842172997198\n",
      "948 0.8499279051313834\n",
      "952 0.8497956051012882\n",
      "956 0.8494071574426428\n",
      "960 0.8486226746561976\n",
      "964 0.8484774773445989\n",
      "968 0.8478109020191227\n",
      "972 0.847399731059909\n",
      "976 0.8471197899188637\n",
      "980 0.8466722576160205\n",
      "984 0.8461771997631286\n",
      "988 0.8457415129486899\n",
      "992 0.8453155604947629\n",
      "996 0.8449858796731966\n",
      "1000 0.8446036590287269\n",
      "1000 0.8446036590287269\n"
     ]
    }
   ],
   "source": [
    "funVals, ypred = cancer_model.train(Xtrain_cancer, ytrain_cancer, batch_size=100, iterations=1000, alpha=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(funVals, filePath=\"breast-cancer/results/Adam\", title=\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_model.w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_cancer_pred = cancer_model.predict(Xtest_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_cancer_pred[ytest_cancer_pred<0.5] = 0\n",
    "ytest_cancer_pred[ytest_cancer_pred>=0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5439], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(ytest_cancer_pred == ytest_cancer, dim=0).double()/ytest_cancer.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
