{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network Class for implementing neural networks for different loss and optimization functions.\n",
    "    \n",
    "    Attribuutes:\n",
    "        input_size: An integer indicating number of input features.\n",
    "        output_size: An integer indicating size of output.\n",
    "        hidden_layer_size: An integer indicating size of hidden layer.\n",
    "        \n",
    "        w1: A vector (input_size X hidden_layers_sizes[0]) of floats required for training the neural network.\n",
    "        wn: A vector (hidden_layers_sizes[-1] X output_size) for weights of final layer.\n",
    "        \n",
    "        activations: An array of strings indicating the activation functions for every layer.\n",
    "        loss: A string indicating the loss function for the neural network.\n",
    "        optimizer: A string indicating the optimization algorithm to be used to train the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size, activations, loss, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes Neural Network class attributes.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.w1 = torch.randn(self.input_size, self.hidden_layer_size)\n",
    "        self.wn = torch.randn(self.hidden_layer_size, self.output_size)\n",
    "    \n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        a = []\n",
    "        z.append(torch.matmul(X, self.w1))\n",
    "        a.append(self.evaluateActivation(self.activations[0])(z[-1]))\n",
    "        z.append(torch.matmul(a[-1], self.wn))\n",
    "        a.append(self.evaluateActivation(self.activations[1])(z[-1]))\n",
    "        return z, a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, z, a):\n",
    "        \"\"\"\n",
    "        Backward Pass of the model.\n",
    "        \"\"\"\n",
    "        dW = []\n",
    "        dL_da_n = self.evaluateLossDerivative()(a[-1], y)\n",
    "        da_n_dz_n = self.evaluateActivationDerivative(self.activations[1])(z[-1])\n",
    "        dz_n_dWn = a[0]\n",
    "        dL_dWn = torch.matmul(dz_n_dWn.T, (dL_da_n * da_n_dz_n))\n",
    "        \n",
    "        dz_n_da_1 = self.wn\n",
    "        da_1_dz_1 = self.evaluateActivationDerivative(self.activations[0])(z[0])\n",
    "        dz_1_dW1 = X\n",
    "        dL_dW1 = torch.matmul(dz_1_dW1.T, (torch.matmul(dL_da_n * da_n_dz_n, dz_n_da_1.T)*da_1_dz_1))\n",
    "        dW.append(dL_dW1)\n",
    "        dW.append(dL_dWn)\n",
    "        return dW\n",
    "    \n",
    "    \n",
    "    def train(self, X, y, alpha, iterations):\n",
    "        \"\"\"\n",
    "        Function to train the neural network.\n",
    "        \"\"\"\n",
    "        funVals = []\n",
    "        if self.optimizer == 'GradientDescent':\n",
    "            funVals, ypred = self.GradientDescent(X, y, alpha, iterations)\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def GradientDescent(self, X, y, alpha, iterations):\n",
    "        funVals = []\n",
    "        ypred = []\n",
    "        for _ in range(iterations):\n",
    "            z, a = self.forward(X)\n",
    "            funVals.append((self.evaluateLoss()(a[1], y)).item())\n",
    "            dW = self.backward(X, y, z, a)\n",
    "            self.w1 = self.w1 - alpha * dW[0]\n",
    "            self.wn = self.wn - alpha * dW[1]\n",
    "            ypred = a[-1]\n",
    "        return funVals, ypred\n",
    "    \n",
    "    \n",
    "    def evaluateActivation(self, activation):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid' :\n",
    "            return lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "        if activation == 'softmax':\n",
    "            return lambda z : torch.exp(z - torch.max(z))/torch.sum(torch.exp(z - torch.max(z)), dim=0)\n",
    "        if activation == 'relu':\n",
    "            def relu(z):\n",
    "                z1 = torch.clone(z)\n",
    "                return z1.clamp(min=0)\n",
    "            return relu\n",
    "        if activation == 'tanh':\n",
    "            return lambda z : torch.tanh(z)\n",
    "        if activation == 'logit':\n",
    "            return lambda z : torch.log(z/(1-z))\n",
    "        return lambda z : z\n",
    "    \n",
    "    \n",
    "    def evaluateActivationDerivative(self, activation):\n",
    "        \"\"\"\n",
    "        Derivative of Activation Function\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            sigmoid = lambda z : torch.exp(z)/(1 + torch.exp(z))\n",
    "            return lambda z : sigmoid(z) * (1 - sigmoid(z))\n",
    "        if activation == 'softmax':\n",
    "            pass\n",
    "        if activation == 'relu':\n",
    "            def relu_derivative(z):\n",
    "                z1 = torch.clone(z)\n",
    "                z1[z>0] = 1\n",
    "                z1[z<0] = 0\n",
    "                return z1\n",
    "            return relu_derivative\n",
    "        if activation == 'tanh':\n",
    "            pass\n",
    "        if activation == 'logit':\n",
    "            pass\n",
    "        return lambda z : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLoss(self):\n",
    "        \"\"\"\n",
    "        Loss Function\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : torch.matmul((y - ypred).T, y - ypred)/(2*y.shape[0])\n",
    "        if self.loss == 'Hinge':\n",
    "            def hinge(y, ypred):\n",
    "                yYpred = y * ypred\n",
    "                return torch.sum(torch.max(torch.zeros(yYpred.shape), 1 - yYpred.T))\n",
    "            return hinge\n",
    "        if self.loss == 'CrossEntropyLoss':\n",
    "            pass\n",
    "        return lambda x : 1\n",
    "    \n",
    "    \n",
    "    def evaluateLossDerivative(self):\n",
    "        \"\"\"\n",
    "        Loss function Derivative\n",
    "        \"\"\"\n",
    "        if self.loss == 'MSE':\n",
    "            return lambda ypred, y : (y - ypred)/len(y)\n",
    "        if self.loss == 'Hinge':\n",
    "#             def hingeDerivative(y, ypred):\n",
    "#                 yYpred = y * ypred\n",
    "#                 temp = yYpred.clone()\n",
    "#                 temp[1 > yYpred] = 1\n",
    "#                 temp[1 < yYpred] = 0\n",
    "#                 ymul = -1*y*temp\n",
    "#                 return torch.matmul(ymul.reshape(1, -1), X).reshape(-1, 1)\n",
    "#             return hingeDerivative\n",
    "            pass\n",
    "        if self.loss == 'CrossEntropyLoss':\n",
    "            pass\n",
    "        return lambda x : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000, 10)\n",
    "y = torch.randn(1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X.shape[1], 1, 10, ['relu', 'linear'], 'MSE', 'GradientDescent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "funVals, ypred = model.train(X, y, 1e-05, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.2406]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul((ypred-y).T, (ypred-y))/(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
